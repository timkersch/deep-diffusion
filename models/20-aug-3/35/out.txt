Training network with 56340 training samples and 18780 validation samples
Epoch 1 of 500
  training loss:		1.114112E-02
  validation loss:		1.280785E-04
Epoch took 1.439s

Epoch 2 of 500
  training loss:		1.968259E-04
  validation loss:		3.745177E-04
Epoch took 1.439s

Epoch 3 of 500
  training loss:		6.847304E-05
  validation loss:		2.170704E-05
Epoch took 1.477s

Epoch 4 of 500
  training loss:		2.625964E-04
  validation loss:		8.296690E-05
Epoch took 1.474s

Epoch 5 of 500
  training loss:		6.109420E-05
  validation loss:		2.045715E-04
Epoch took 1.460s

Epoch 6 of 500
  training loss:		2.336092E-04
  validation loss:		9.846864E-06
Epoch took 1.457s

Epoch 7 of 500
  training loss:		1.097759E-04
  validation loss:		8.368146E-05
Epoch took 1.450s

Epoch 8 of 500
  training loss:		1.879010E-04
  validation loss:		4.641383E-04
Epoch took 1.440s

Epoch 9 of 500
  training loss:		1.688255E-04
  validation loss:		7.036118E-06
Epoch took 1.459s

Epoch 10 of 500
  training loss:		1.709267E-05
  validation loss:		8.938399E-06
Epoch took 1.476s

Epoch 11 of 500
  training loss:		1.185747E-04
  validation loss:		1.628753E-04
Epoch took 1.461s

Epoch 12 of 500
  training loss:		8.879319E-05
  validation loss:		1.978437E-05
Epoch took 1.455s

Epoch 13 of 500
  training loss:		5.550605E-05
  validation loss:		9.204699E-05
Epoch took 1.449s

Epoch 14 of 500
  training loss:		1.190841E-04
  validation loss:		6.753070E-06
Epoch took 1.432s

Epoch 15 of 500
  training loss:		3.168228E-05
  validation loss:		2.348700E-04
Epoch took 1.464s

Epoch 16 of 500
  training loss:		6.449450E-05
  validation loss:		6.022353E-05
Epoch took 1.476s

Epoch 17 of 500
  training loss:		4.889287E-05
  validation loss:		2.169150E-05
Epoch took 1.469s

Epoch 18 of 500
  training loss:		1.826753E-05
  validation loss:		1.378036E-04
Epoch took 1.457s

Epoch 19 of 500
  training loss:		4.512233E-05
  validation loss:		1.047390E-05
Epoch took 1.451s

Epoch 20 of 500
  training loss:		5.138262E-03
  validation loss:		5.746697E-05
Epoch took 1.439s

Epoch 21 of 500
  training loss:		4.526524E-05
  validation loss:		2.639698E-05
Epoch took 1.450s

Epoch 22 of 500
  training loss:		2.966399E-05
  validation loss:		2.340107E-05
Epoch took 1.477s

Epoch 23 of 500
  training loss:		3.357033E-05
  validation loss:		3.741705E-05
Epoch took 1.476s

Epoch 24 of 500
  training loss:		6.354468E-05
  validation loss:		8.482780E-05
Epoch took 1.455s

Epoch 25 of 500
  training loss:		3.386008E-05
  validation loss:		5.795520E-06
Epoch took 0.719s

Epoch 26 of 500
  training loss:		6.307705E-05
  validation loss:		6.108036E-05
Epoch took 0.250s

Epoch 27 of 500
  training loss:		5.801091E-05
  validation loss:		7.078344E-06
Epoch took 0.255s

Epoch 28 of 500
  training loss:		4.203608E-05
  validation loss:		1.152982E-05
Epoch took 0.252s

Epoch 29 of 500
  training loss:		5.682568E-05
  validation loss:		6.099392E-06
Epoch took 0.249s

Epoch 30 of 500
  training loss:		1.012503E-05
  validation loss:		2.536861E-06
Epoch took 0.251s

Epoch 31 of 500
  training loss:		4.095818E-05
  validation loss:		2.390289E-06
Epoch took 0.250s

Epoch 32 of 500
  training loss:		7.199145E-06
  validation loss:		3.543386E-05
Epoch took 0.252s

Epoch 33 of 500
  training loss:		2.729788E-05
  validation loss:		2.491870E-06
Epoch took 0.251s

Epoch 34 of 500
  training loss:		1.015602E-05
  validation loss:		3.904928E-06
Epoch took 0.250s

Epoch 35 of 500
  training loss:		1.345499E-05
  validation loss:		2.163180E-05
Epoch took 0.249s

Epoch 36 of 500
  training loss:		2.379168E-06
  validation loss:		7.537965E-07
Epoch took 0.250s

Epoch 37 of 500
  training loss:		1.148893E-05
  validation loss:		1.417779E-06
Epoch took 0.511s

Epoch 38 of 500
  training loss:		6.718048E-06
  validation loss:		6.367924E-06
Epoch took 1.123s

Epoch 39 of 500
  training loss:		4.014932E-06
  validation loss:		2.955198E-06
Epoch took 1.132s

Epoch 40 of 500
  training loss:		2.553581E-06
  validation loss:		2.985806E-06
Epoch took 1.131s

Epoch 41 of 500
  training loss:		4.131458E-06
  validation loss:		2.859691E-06
Epoch took 1.110s

Epoch 42 of 500
  training loss:		2.384332E-03
  validation loss:		1.696216E-05
Epoch took 1.140s

Epoch 43 of 500
  training loss:		1.422917E-05
  validation loss:		5.993496E-06
Epoch took 1.113s

Epoch 44 of 500
  training loss:		1.907884E-05
  validation loss:		1.559168E-05
Epoch took 1.118s

Epoch 45 of 500
  training loss:		3.059151E-05
  validation loss:		3.588750E-06
Epoch took 1.133s

Epoch 46 of 500
  training loss:		2.532979E-05
  validation loss:		5.387812E-05
Epoch took 1.111s

Epoch 47 of 500
  training loss:		1.024466E-05
  validation loss:		2.727621E-06
Epoch took 1.144s

Epoch 48 of 500
  training loss:		6.688005E-06
  validation loss:		2.476737E-06
Epoch took 1.117s

Epoch 49 of 500
  training loss:		1.933670E-05
  validation loss:		1.666213E-06
Epoch took 1.105s

Epoch 50 of 500
  training loss:		2.288098E-06
  validation loss:		1.341413E-06
Epoch took 1.143s

Epoch 51 of 500
  training loss:		7.641702E-06
  validation loss:		3.713431E-06
Epoch took 1.110s

Epoch 52 of 500
  training loss:		1.091889E-05
  validation loss:		2.942513E-06
Epoch took 1.134s

Epoch 53 of 500
  training loss:		3.763608E-06
  validation loss:		5.734049E-07
Epoch took 1.121s

Epoch 54 of 500
  training loss:		6.337363E-06
  validation loss:		3.942698E-07
Epoch took 1.103s

Epoch 55 of 500
  training loss:		1.840145E-06
  validation loss:		9.050205E-07
Epoch took 1.148s

Epoch 56 of 500
  training loss:		3.046237E-06
  validation loss:		3.744618E-06
Epoch took 1.119s

Epoch 57 of 500
  training loss:		3.122956E-04
  validation loss:		1.486773E-06
Epoch took 1.123s

Epoch 58 of 500
  training loss:		1.865473E-06
  validation loss:		7.984517E-07
Epoch took 1.140s

Epoch 59 of 500
  training loss:		1.007156E-06
  validation loss:		5.859567E-07
Epoch took 1.107s

Epoch 60 of 500
  training loss:		2.220813E-06
  validation loss:		3.303098E-07
Epoch took 1.144s

Epoch 61 of 500
  training loss:		1.969864E-06
  validation loss:		6.002937E-07
Epoch took 1.119s

Epoch 62 of 500
  training loss:		3.668266E-06
  validation loss:		1.292441E-06
Epoch took 1.106s

Epoch 63 of 500
  training loss:		9.953398E-07
  validation loss:		1.115916E-06
Epoch took 1.144s

Epoch 64 of 500
  training loss:		1.128081E-06
  validation loss:		1.358799E-06
Epoch took 1.110s

Epoch 65 of 500
  training loss:		2.460273E-06
  validation loss:		8.262577E-06
Epoch took 1.131s

Epoch 66 of 500
  training loss:		2.054609E-06
  validation loss:		6.197415E-08
Epoch took 1.131s

Epoch 67 of 500
  training loss:		8.942195E-08
  validation loss:		8.187934E-08
Epoch took 1.100s

Epoch 68 of 500
  training loss:		5.296546E-08
  validation loss:		3.774858E-08
Epoch took 1.145s

Epoch 69 of 500
  training loss:		6.817960E-07
  validation loss:		4.949820E-08
Epoch took 1.118s

Epoch 70 of 500
  training loss:		4.802075E-08
  validation loss:		4.085679E-08
Epoch took 1.118s

Epoch 71 of 500
  training loss:		9.123723E-08
  validation loss:		3.051113E-08
Epoch took 1.144s

Epoch 72 of 500
  training loss:		2.104591E-07
  validation loss:		1.373137E-07
Epoch took 1.107s

Epoch 73 of 500
  training loss:		1.559598E-07
  validation loss:		7.831397E-09
Epoch took 1.139s

Epoch 74 of 500
  training loss:		1.090591E-07
  validation loss:		7.427943E-08
Epoch took 1.128s

Epoch 75 of 500
  training loss:		6.281988E-08
  validation loss:		5.481162E-09
Epoch took 1.097s

Epoch 76 of 500
  training loss:		5.480689E-08
  validation loss:		8.114509E-09
Epoch took 1.143s

Epoch 77 of 500
  training loss:		4.757621E-08
  validation loss:		4.695226E-09
Epoch took 1.111s

Epoch 78 of 500
  training loss:		6.492528E-05
  validation loss:		4.366842E-07
Epoch took 1.131s

Epoch 79 of 500
  training loss:		1.944310E-07
  validation loss:		1.037495E-07
Epoch took 1.128s

Epoch 80 of 500
  training loss:		6.871319E-08
  validation loss:		4.801866E-08
Epoch took 1.102s

Epoch 81 of 500
  training loss:		3.574872E-08
  validation loss:		3.007962E-08
Epoch took 1.154s

Epoch 82 of 500
  training loss:		2.042971E-08
  validation loss:		1.495922E-08
Epoch took 1.124s

Epoch 83 of 500
  training loss:		1.322446E-08
  validation loss:		8.946153E-09
Epoch took 1.107s

Epoch 84 of 500
  training loss:		1.387587E-08
  validation loss:		8.143955E-09
Epoch took 1.142s

Epoch 85 of 500
  training loss:		7.729999E-09
  validation loss:		3.676401E-09
Epoch took 1.108s

Epoch 86 of 500
  training loss:		3.011229E-09
  validation loss:		2.803778E-09
Epoch took 1.144s

Epoch 87 of 500
  training loss:		6.038262E-08
  validation loss:		1.316223E-07
Epoch took 1.127s

Epoch 88 of 500
  training loss:		7.821004E-09
  validation loss:		2.603174E-09
Epoch took 1.104s

Epoch 89 of 500
  training loss:		5.761698E-08
  validation loss:		3.542665E-07
Epoch took 1.149s

Epoch 90 of 500
  training loss:		6.187034E-08
  validation loss:		4.287617E-09
Epoch took 1.126s

Epoch 91 of 500
  training loss:		4.058274E-09
  validation loss:		2.607583E-09
Epoch took 1.109s

Epoch 92 of 500
  training loss:		6.999755E-09
  validation loss:		4.700972E-08
Epoch took 1.146s

Epoch 93 of 500
  training loss:		1.024654E-08
  validation loss:		4.111141E-09
Epoch took 1.116s

Epoch 94 of 500
  training loss:		1.976683E-08
  validation loss:		1.475592E-08
Epoch took 1.131s

Epoch 95 of 500
  training loss:		1.073941E-08
  validation loss:		9.777736E-10
Epoch took 1.131s

Epoch 96 of 500
  training loss:		1.420135E-06
  validation loss:		1.799447E-13
Epoch took 1.100s

Epoch 97 of 500
  training loss:		6.391691E-13
  validation loss:		9.827065E-14
Epoch took 1.144s

Epoch 98 of 500
  training loss:		6.110998E-13
  validation loss:		2.218179E-12
Epoch took 1.117s

Epoch 99 of 500
  training loss:		8.058178E-13
  validation loss:		1.804385E-13
Epoch took 1.122s

Epoch 100 of 500
  training loss:		1.104723E-13
  validation loss:		1.057913E-13
Epoch took 1.133s

Epoch 101 of 500
  training loss:		1.158078E-13
  validation loss:		9.661493E-14
Epoch took 1.108s

Epoch 102 of 500
  training loss:		1.195102E-13
  validation loss:		1.534107E-13
Epoch took 1.149s

Epoch 103 of 500
  training loss:		1.952980E-13
  validation loss:		1.634144E-13
Epoch took 1.118s

Epoch 104 of 500
  training loss:		7.872482E-10
  validation loss:		1.093565E-08
Epoch took 1.107s

Epoch 105 of 500
  training loss:		3.730070E-09
  validation loss:		1.188534E-10
Epoch took 1.150s

Epoch 106 of 500
  training loss:		7.818595E-09
  validation loss:		9.344777E-14
Epoch took 1.122s

Epoch 107 of 500
  training loss:		1.281864E-09
  validation loss:		2.203281E-10
Epoch took 1.122s

Epoch 108 of 500
  training loss:		3.592771E-09
  validation loss:		1.626952E-12
Epoch took 1.136s

Epoch 109 of 500
  training loss:		3.288956E-09
  validation loss:		6.911801E-10
Epoch took 1.108s

Epoch 110 of 500
  training loss:		3.381440E-09
  validation loss:		6.625080E-13
Epoch took 1.139s

Epoch 111 of 500
  training loss:		3.543451E-09
  validation loss:		2.762114E-12
Epoch took 1.118s

Epoch 112 of 500
  training loss:		4.293010E-09
  validation loss:		3.608627E-10
Epoch took 1.113s

Epoch 113 of 500
  training loss:		4.325942E-09
  validation loss:		2.887704E-09
Epoch took 1.118s

Epoch 114 of 500
  training loss:		3.474175E-09
  validation loss:		1.934514E-11
Epoch took 1.106s

Epoch 115 of 500
  training loss:		2.845657E-09
  validation loss:		8.907548E-12
Epoch took 1.135s

Epoch 116 of 500
  training loss:		4.073783E-09
  validation loss:		5.404455E-11
Epoch took 1.110s

Epoch 117 of 500
  training loss:		4.051216E-09
  validation loss:		7.081509E-12
Epoch took 1.139s

Epoch 118 of 500
  training loss:		3.528118E-09
  validation loss:		2.745441E-13
Epoch took 1.112s

Epoch 119 of 500
  training loss:		2.898899E-09
  validation loss:		3.267543E-10
Epoch took 1.110s

Epoch 120 of 500
  training loss:		4.155724E-09
  validation loss:		5.822328E-13
Epoch took 1.143s

Epoch 121 of 500
  training loss:		4.446784E-09
  validation loss:		3.877943E-09
Epoch took 1.117s

Epoch 122 of 500
  training loss:		3.332289E-09
  validation loss:		4.435601E-10
Epoch took 1.132s

Epoch 123 of 500
  training loss:		3.200884E-09
  validation loss:		6.525231E-08
Epoch took 1.130s

Epoch 124 of 500
  training loss:		3.695783E-09
  validation loss:		1.834047E-12
Epoch took 1.114s

Epoch 125 of 500
  training loss:		3.643949E-09
  validation loss:		7.377277E-12
Epoch took 1.140s

Epoch 126 of 500
  training loss:		2.642408E-09
  validation loss:		9.597371E-09
Epoch took 0.922s

Epoch 127 of 500
  training loss:		4.119845E-09
  validation loss:		2.497784E-11
Epoch took 0.252s

Epoch 128 of 500
  training loss:		3.714078E-09
  validation loss:		6.807935E-10
Epoch took 0.251s

Epoch 129 of 500
  training loss:		2.537307E-09
  validation loss:		3.560357E-11
Epoch took 0.254s

Epoch 130 of 500
  training loss:		3.440401E-09
  validation loss:		1.748768E-08
Epoch took 0.254s

Epoch 131 of 500
  training loss:		4.197454E-09
  validation loss:		1.540037E-10
Epoch took 0.250s

Epoch 132 of 500
  training loss:		2.965913E-09
  validation loss:		6.113056E-13
Epoch took 0.250s

Epoch 133 of 500
  training loss:		3.744365E-09
  validation loss:		7.132074E-09
Epoch took 0.251s

Epoch 134 of 500
  training loss:		2.886359E-09
  validation loss:		8.392106E-12
Epoch took 0.253s

Epoch 135 of 500
  training loss:		2.887616E-09
  validation loss:		4.831743E-12
Epoch took 0.251s

Early stopping, val-loss increased over the last 15 epochs from 3.05480678788e-10 to 6.98062386402e-09
Saving model from epoch 120
Training MSE: 5.82837e-13
Validation MSE: 5.82442e-13
Training MAE: 6.98986e-07
Validation MAE: 6.99349e-07
