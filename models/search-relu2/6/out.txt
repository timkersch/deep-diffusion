Training network with 45280 training samples and 8490 validation samples
Epoch 1 of 100
  training loss:		1.602235E-01
  validation loss:		7.707746E-03

Epoch 2 of 100
  training loss:		1.772087E-02
  validation loss:		4.999124E-03

Epoch 3 of 100
  training loss:		1.103026E-02
  validation loss:		1.784608E-03

Epoch 4 of 100
  training loss:		8.168692E-03
  validation loss:		1.029289E-03

Epoch 5 of 100
  training loss:		6.385433E-03
  validation loss:		1.896314E-03

Epoch 6 of 100
  training loss:		5.705803E-03
  validation loss:		1.560018E-03

Epoch 7 of 100
  training loss:		4.857390E-03
  validation loss:		1.963829E-03

Epoch 8 of 100
  training loss:		4.198521E-03
  validation loss:		8.099368E-04

Epoch 9 of 100
  training loss:		3.825904E-03
  validation loss:		9.812411E-04

Epoch 10 of 100
  training loss:		3.469622E-03
  validation loss:		1.687813E-03

Epoch 11 of 100
  training loss:		3.039876E-03
  validation loss:		9.683694E-04

Epoch 12 of 100
  training loss:		2.790285E-03
  validation loss:		1.702426E-03

Epoch 13 of 100
  training loss:		2.523738E-03
  validation loss:		1.356685E-03

Epoch 14 of 100
  training loss:		2.398954E-03
  validation loss:		6.155622E-04

Epoch 15 of 100
  training loss:		2.099691E-03
  validation loss:		6.722563E-04

Epoch 16 of 100
  training loss:		2.225548E-03
  validation loss:		1.052219E-03

Epoch 17 of 100
  training loss:		1.875686E-03
  validation loss:		1.254660E-03

Epoch 18 of 100
  training loss:		1.760201E-03
  validation loss:		8.542743E-04

Epoch 19 of 100
  training loss:		1.593338E-03
  validation loss:		3.595602E-04

Epoch 20 of 100
  training loss:		1.545495E-03
  validation loss:		3.697007E-04

Epoch 21 of 100
  training loss:		1.469598E-03
  validation loss:		9.440048E-04

Epoch 22 of 100
  training loss:		1.355948E-03
  validation loss:		4.986684E-04

Epoch 23 of 100
  training loss:		1.346204E-03
  validation loss:		8.173756E-04

Epoch 24 of 100
  training loss:		1.225964E-03
  validation loss:		4.091125E-04

Epoch 25 of 100
  training loss:		1.144179E-03
  validation loss:		7.749182E-04

Epoch 26 of 100
  training loss:		1.072278E-03
  validation loss:		7.025924E-04

Epoch 27 of 100
  training loss:		9.479906E-04
  validation loss:		2.841746E-04

Epoch 28 of 100
  training loss:		9.807703E-04
  validation loss:		4.984436E-04

Epoch 29 of 100
  training loss:		9.891379E-04
  validation loss:		4.314432E-04

Epoch 30 of 100
  training loss:		8.644143E-04
  validation loss:		3.064154E-04

Epoch 31 of 100
  training loss:		7.783280E-04
  validation loss:		2.562125E-04

Epoch 32 of 100
  training loss:		7.556246E-04
  validation loss:		3.973030E-04

Epoch 33 of 100
  training loss:		7.320486E-04
  validation loss:		2.784452E-04

Epoch 34 of 100
  training loss:		7.037383E-04
  validation loss:		5.339230E-04

Epoch 35 of 100
  training loss:		6.142928E-04
  validation loss:		4.967959E-04

Epoch 36 of 100
  training loss:		5.986166E-04
  validation loss:		3.084553E-04

Epoch 37 of 100
  training loss:		6.158536E-04
  validation loss:		4.396770E-04

Epoch 38 of 100
  training loss:		5.861232E-04
  validation loss:		2.770175E-04

Epoch 39 of 100
  training loss:		5.460559E-04
  validation loss:		2.027018E-04

Epoch 40 of 100
  training loss:		5.339037E-04
  validation loss:		2.551611E-04

Epoch 41 of 100
  training loss:		5.197090E-04
  validation loss:		3.976902E-04

Epoch 42 of 100
  training loss:		5.097438E-04
  validation loss:		5.749005E-04

Epoch 43 of 100
  training loss:		4.737417E-04
  validation loss:		1.376455E-04

Epoch 44 of 100
  training loss:		4.535229E-04
  validation loss:		2.833754E-04

Epoch 45 of 100
  training loss:		4.334416E-04
  validation loss:		5.403062E-04

Early stopping, val-loss increased over the last 5 epochs from 0.000296602539454 to 0.000386783570089
Training RMSE: 0.0168773251033
Validation RMSE: 0.0168235489438
