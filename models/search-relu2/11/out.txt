Training network with 45280 training samples and 8490 validation samples
Epoch 1 of 100
  training loss:		2.669160E-02
  validation loss:		6.987485E-03

Epoch 2 of 100
  training loss:		6.599407E-03
  validation loss:		1.207470E-02

Epoch 3 of 100
  training loss:		4.091597E-03
  validation loss:		2.450014E-03

Epoch 4 of 100
  training loss:		3.170330E-03
  validation loss:		1.946732E-03

Epoch 5 of 100
  training loss:		2.239921E-03
  validation loss:		1.875823E-03

Epoch 6 of 100
  training loss:		1.952380E-03
  validation loss:		7.109049E-04

Epoch 7 of 100
  training loss:		1.452787E-03
  validation loss:		8.747074E-04

Epoch 8 of 100
  training loss:		1.263591E-03
  validation loss:		1.032893E-03

Epoch 9 of 100
  training loss:		1.154443E-03
  validation loss:		6.668606E-04

Epoch 10 of 100
  training loss:		9.431646E-04
  validation loss:		8.610557E-04

Epoch 11 of 100
  training loss:		8.351800E-04
  validation loss:		3.004412E-04

Epoch 12 of 100
  training loss:		7.274840E-04
  validation loss:		2.754438E-04

Epoch 13 of 100
  training loss:		6.770469E-04
  validation loss:		6.675479E-04

Epoch 14 of 100
  training loss:		5.880391E-04
  validation loss:		7.236535E-04

Epoch 15 of 100
  training loss:		5.907366E-04
  validation loss:		2.708305E-04

Epoch 16 of 100
  training loss:		4.845925E-04
  validation loss:		2.540068E-04

Epoch 17 of 100
  training loss:		4.014372E-04
  validation loss:		9.208061E-05

Epoch 18 of 100
  training loss:		3.995194E-04
  validation loss:		1.383828E-04

Epoch 19 of 100
  training loss:		3.453558E-04
  validation loss:		2.887551E-04

Epoch 20 of 100
  training loss:		3.320785E-04
  validation loss:		1.783735E-04

Epoch 21 of 100
  training loss:		3.333008E-04
  validation loss:		1.800293E-04

Epoch 22 of 100
  training loss:		2.708144E-04
  validation loss:		9.680101E-05

Epoch 23 of 100
  training loss:		3.075396E-04
  validation loss:		7.413862E-04

Epoch 24 of 100
  training loss:		2.292257E-04
  validation loss:		1.953873E-04

Epoch 25 of 100
  training loss:		2.061744E-04
  validation loss:		1.275103E-04

Epoch 26 of 100
  training loss:		1.962686E-04
  validation loss:		1.311631E-04

Epoch 27 of 100
  training loss:		1.920960E-04
  validation loss:		1.025351E-04

Epoch 28 of 100
  training loss:		1.588712E-04
  validation loss:		1.649082E-04

Epoch 29 of 100
  training loss:		1.627151E-04
  validation loss:		8.912455E-05

Epoch 30 of 100
  training loss:		1.362776E-04
  validation loss:		1.481850E-04

Epoch 31 of 100
  training loss:		1.691943E-04
  validation loss:		7.069740E-05

Epoch 32 of 100
  training loss:		1.256822E-04
  validation loss:		8.502040E-05

Epoch 33 of 100
  training loss:		1.127552E-04
  validation loss:		1.461128E-04

Epoch 34 of 100
  training loss:		1.015791E-04
  validation loss:		6.772328E-05

Epoch 35 of 100
  training loss:		1.060952E-04
  validation loss:		1.341269E-04

Epoch 36 of 100
  training loss:		9.701377E-05
  validation loss:		6.149956E-05

Epoch 37 of 100
  training loss:		1.035540E-04
  validation loss:		4.945975E-05

Epoch 38 of 100
  training loss:		1.037526E-04
  validation loss:		3.330270E-05

Epoch 39 of 100
  training loss:		9.838878E-05
  validation loss:		6.037623E-05

Epoch 40 of 100
  training loss:		8.545774E-05
  validation loss:		3.419734E-05

Epoch 41 of 100
  training loss:		8.155926E-05
  validation loss:		9.801349E-05

Epoch 42 of 100
  training loss:		8.373977E-05
  validation loss:		8.233517E-05

Epoch 43 of 100
  training loss:		5.242254E-05
  validation loss:		6.750875E-05

Epoch 44 of 100
  training loss:		6.113581E-05
  validation loss:		1.265471E-04

Epoch 45 of 100
  training loss:		4.871897E-05
  validation loss:		4.186446E-05

Epoch 46 of 100
  training loss:		5.539793E-05
  validation loss:		4.909809E-05

Epoch 47 of 100
  training loss:		6.540585E-05
  validation loss:		5.423966E-05

Epoch 48 of 100
  training loss:		4.837567E-05
  validation loss:		2.335361E-05

Epoch 49 of 100
  training loss:		5.021868E-05
  validation loss:		1.833771E-05

Epoch 50 of 100
  training loss:		4.077676E-05
  validation loss:		2.122677E-05

Epoch 51 of 100
  training loss:		3.722147E-05
  validation loss:		1.279610E-05

Epoch 52 of 100
  training loss:		4.540736E-05
  validation loss:		1.508152E-05

Epoch 53 of 100
  training loss:		4.571506E-05
  validation loss:		3.331064E-05

Epoch 54 of 100
  training loss:		4.740969E-05
  validation loss:		1.687425E-05

Epoch 55 of 100
  training loss:		4.433830E-05
  validation loss:		1.991057E-05

Epoch 56 of 100
  training loss:		2.155025E-05
  validation loss:		1.259440E-05

Epoch 57 of 100
  training loss:		2.857369E-05
  validation loss:		1.794676E-05

Epoch 58 of 100
  training loss:		3.492135E-05
  validation loss:		5.666485E-05

Epoch 59 of 100
  training loss:		3.050323E-05
  validation loss:		1.575977E-05

Epoch 60 of 100
  training loss:		4.077409E-05
  validation loss:		1.021215E-05

Epoch 61 of 100
  training loss:		2.395565E-05
  validation loss:		1.111377E-05

Epoch 62 of 100
  training loss:		2.529529E-05
  validation loss:		4.929306E-05

Epoch 63 of 100
  training loss:		4.692440E-05
  validation loss:		8.850995E-06

Epoch 64 of 100
  training loss:		1.820046E-05
  validation loss:		2.028927E-05

Epoch 65 of 100
  training loss:		2.857974E-05
  validation loss:		1.087830E-05

Epoch 66 of 100
  training loss:		2.751423E-05
  validation loss:		1.220787E-05

Epoch 67 of 100
  training loss:		2.484781E-05
  validation loss:		2.655716E-05

Epoch 68 of 100
  training loss:		2.499129E-05
  validation loss:		1.387903E-05

Epoch 69 of 100
  training loss:		1.652339E-05
  validation loss:		2.012210E-05

Epoch 70 of 100
  training loss:		1.389352E-05
  validation loss:		7.842934E-06

Epoch 71 of 100
  training loss:		2.887316E-05
  validation loss:		2.257157E-05

Epoch 72 of 100
  training loss:		2.364895E-05
  validation loss:		4.595744E-06

Epoch 73 of 100
  training loss:		2.288490E-05
  validation loss:		3.549292E-05

Epoch 74 of 100
  training loss:		1.259019E-05
  validation loss:		9.274171E-06

Epoch 75 of 100
  training loss:		2.729348E-05
  validation loss:		1.176445E-05

Epoch 76 of 100
  training loss:		2.221007E-05
  validation loss:		2.565252E-05

Epoch 77 of 100
  training loss:		1.532595E-05
  validation loss:		5.064144E-06

Epoch 78 of 100
  training loss:		2.436372E-05
  validation loss:		2.122880E-05

Epoch 79 of 100
  training loss:		2.044294E-05
  validation loss:		4.950975E-06

Epoch 80 of 100
  training loss:		1.547119E-05
  validation loss:		1.593486E-05

Epoch 81 of 100
  training loss:		1.475686E-05
  validation loss:		2.244790E-05

Epoch 82 of 100
  training loss:		1.645387E-05
  validation loss:		8.002265E-06

Epoch 83 of 100
  training loss:		1.428303E-05
  validation loss:		1.313048E-05

Epoch 84 of 100
  training loss:		2.019338E-05
  validation loss:		9.207645E-06

Epoch 85 of 100
  training loss:		1.432010E-05
  validation loss:		1.298083E-05

Epoch 86 of 100
  training loss:		1.708101E-05
  validation loss:		6.509159E-06

Epoch 87 of 100
  training loss:		1.723366E-05
  validation loss:		1.019631E-05

Epoch 88 of 100
  training loss:		1.740250E-05
  validation loss:		1.646302E-05

Epoch 89 of 100
  training loss:		1.410667E-05
  validation loss:		1.623045E-05

Epoch 90 of 100
  training loss:		2.355092E-05
  validation loss:		3.032008E-05

Epoch 91 of 100
  training loss:		9.771858E-06
  validation loss:		1.057872E-05

Epoch 92 of 100
  training loss:		8.518225E-06
  validation loss:		4.927844E-05

Epoch 93 of 100
  training loss:		1.472453E-05
  validation loss:		1.842633E-05

Epoch 94 of 100
  training loss:		1.531978E-05
  validation loss:		7.847837E-06

Epoch 95 of 100
  training loss:		2.757331E-05
  validation loss:		1.210731E-05

Epoch 96 of 100
  training loss:		1.225442E-05
  validation loss:		5.251419E-06

Epoch 97 of 100
  training loss:		8.297876E-06
  validation loss:		3.019585E-06

Epoch 98 of 100
  training loss:		8.883445E-06
  validation loss:		4.692888E-06

Epoch 99 of 100
  training loss:		1.064766E-05
  validation loss:		2.266184E-05

Epoch 100 of 100
  training loss:		1.089122E-05
  validation loss:		1.795849E-05

Early stopping, val-loss increased over the last 10 epochs from 1.45488131549e-05 to 1.51822855699e-05
Training RMSE: 0.00470330859855
Validation RMSE: 0.00475755282059
