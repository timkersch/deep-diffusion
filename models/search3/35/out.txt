Training network with 45280 training samples and 8490 validation samples
Epoch 1 of 500
  training loss:		1.493177E-02
  validation loss:		1.426320E-03

Epoch 2 of 500
  training loss:		1.168413E-03
  validation loss:		1.015513E-03

Epoch 3 of 500
  training loss:		1.003758E-03
  validation loss:		9.231015E-04

Epoch 4 of 500
  training loss:		9.157497E-04
  validation loss:		6.397534E-04

Epoch 5 of 500
  training loss:		3.490497E-04
  validation loss:		1.797707E-04

Epoch 6 of 500
  training loss:		1.182969E-04
  validation loss:		7.719558E-05

Epoch 7 of 500
  training loss:		7.168822E-05
  validation loss:		6.043203E-05

Epoch 8 of 500
  training loss:		5.630061E-05
  validation loss:		6.319501E-05

Epoch 9 of 500
  training loss:		4.515031E-05
  validation loss:		4.721519E-05

Epoch 10 of 500
  training loss:		3.810016E-05
  validation loss:		2.653933E-05

Epoch 11 of 500
  training loss:		3.669825E-05
  validation loss:		1.484555E-04

Epoch 12 of 500
  training loss:		3.365821E-05
  validation loss:		2.458494E-05

Epoch 13 of 500
  training loss:		3.072849E-05
  validation loss:		1.887532E-05

Epoch 14 of 500
  training loss:		3.470539E-05
  validation loss:		2.365473E-05

Epoch 15 of 500
  training loss:		3.251332E-05
  validation loss:		1.517256E-05

Epoch 16 of 500
  training loss:		3.304617E-05
  validation loss:		1.749270E-05

Epoch 17 of 500
  training loss:		3.122034E-05
  validation loss:		2.324463E-05

Epoch 18 of 500
  training loss:		2.735464E-05
  validation loss:		1.775466E-05

Epoch 19 of 500
  training loss:		2.780719E-05
  validation loss:		3.217695E-05

Epoch 20 of 500
  training loss:		2.604542E-05
  validation loss:		1.879481E-05

Epoch 21 of 500
  training loss:		2.988132E-05
  validation loss:		1.811012E-05

Epoch 22 of 500
  training loss:		2.606274E-05
  validation loss:		3.731605E-05

Epoch 23 of 500
  training loss:		2.316100E-05
  validation loss:		1.704460E-05

Epoch 24 of 500
  training loss:		2.567471E-05
  validation loss:		9.602661E-06

Epoch 25 of 500
  training loss:		2.250355E-05
  validation loss:		9.117868E-05

Epoch 26 of 500
  training loss:		2.533559E-05
  validation loss:		3.531880E-05

Epoch 27 of 500
  training loss:		2.598482E-05
  validation loss:		6.107840E-05

Epoch 28 of 500
  training loss:		2.268323E-05
  validation loss:		5.513130E-05

Epoch 29 of 500
  training loss:		1.977737E-05
  validation loss:		8.370337E-06

Epoch 30 of 500
  training loss:		2.358747E-05
  validation loss:		2.109117E-05

Epoch 31 of 500
  training loss:		2.078869E-05
  validation loss:		5.886767E-06

Epoch 32 of 500
  training loss:		2.267087E-05
  validation loss:		1.321923E-05

Epoch 33 of 500
  training loss:		1.892579E-05
  validation loss:		4.933172E-05

Epoch 34 of 500
  training loss:		2.398921E-05
  validation loss:		7.822277E-06

Epoch 35 of 500
  training loss:		2.096786E-05
  validation loss:		1.527922E-05

Epoch 36 of 500
  training loss:		2.226595E-05
  validation loss:		1.664391E-05

Epoch 37 of 500
  training loss:		1.873429E-05
  validation loss:		1.091975E-05

Epoch 38 of 500
  training loss:		2.069375E-05
  validation loss:		6.907100E-06

Epoch 39 of 500
  training loss:		2.165305E-05
  validation loss:		4.857328E-06

Epoch 40 of 500
  training loss:		1.767197E-05
  validation loss:		6.516088E-06

Epoch 41 of 500
  training loss:		2.062199E-05
  validation loss:		1.744723E-05

Epoch 42 of 500
  training loss:		1.862179E-05
  validation loss:		5.965050E-05

Epoch 43 of 500
  training loss:		2.117633E-05
  validation loss:		9.185135E-06

Epoch 44 of 500
  training loss:		1.742075E-05
  validation loss:		1.405783E-04

Epoch 45 of 500
  training loss:		2.004269E-05
  validation loss:		4.300447E-06

Epoch 46 of 500
  training loss:		1.793662E-05
  validation loss:		1.254190E-05

Epoch 47 of 500
  training loss:		2.012381E-05
  validation loss:		3.990172E-06

Epoch 48 of 500
  training loss:		1.717472E-05
  validation loss:		7.038322E-06

Epoch 49 of 500
  training loss:		1.692274E-05
  validation loss:		8.209040E-06

Epoch 50 of 500
  training loss:		1.800294E-05
  validation loss:		1.884529E-05

Epoch 51 of 500
  training loss:		1.884863E-05
  validation loss:		3.186093E-05

Epoch 52 of 500
  training loss:		1.818268E-05
  validation loss:		5.836654E-06

Epoch 53 of 500
  training loss:		1.867232E-05
  validation loss:		9.361550E-06

Epoch 54 of 500
  training loss:		1.868306E-05
  validation loss:		3.585879E-06

Epoch 55 of 500
  training loss:		1.752871E-05
  validation loss:		3.136826E-05

Epoch 56 of 500
  training loss:		1.654731E-05
  validation loss:		2.675490E-06

Epoch 57 of 500
  training loss:		1.772045E-05
  validation loss:		8.387362E-06

Epoch 58 of 500
  training loss:		1.750468E-05
  validation loss:		3.782816E-05

Epoch 59 of 500
  training loss:		1.912687E-05
  validation loss:		4.736638E-06

Epoch 60 of 500
  training loss:		1.638375E-05
  validation loss:		1.535342E-05

Epoch 61 of 500
  training loss:		1.808389E-05
  validation loss:		2.453147E-06

Epoch 62 of 500
  training loss:		1.650262E-05
  validation loss:		2.714076E-06

Epoch 63 of 500
  training loss:		1.740691E-05
  validation loss:		2.491593E-05

Epoch 64 of 500
  training loss:		1.806999E-05
  validation loss:		4.466700E-06

Epoch 65 of 500
  training loss:		1.744145E-05
  validation loss:		9.548483E-06

Epoch 66 of 500
  training loss:		1.640114E-05
  validation loss:		6.220294E-06

Epoch 67 of 500
  training loss:		1.753038E-05
  validation loss:		5.030055E-05

Epoch 68 of 500
  training loss:		1.583122E-05
  validation loss:		7.370976E-06

Epoch 69 of 500
  training loss:		1.678413E-05
  validation loss:		6.301664E-05

Epoch 70 of 500
  training loss:		1.610306E-05
  validation loss:		2.778121E-06

Epoch 71 of 500
  training loss:		1.633490E-05
  validation loss:		2.902527E-06

Epoch 72 of 500
  training loss:		1.667029E-05
  validation loss:		2.305475E-06

Epoch 73 of 500
  training loss:		1.700973E-05
  validation loss:		2.120346E-05

Epoch 74 of 500
  training loss:		1.672699E-05
  validation loss:		9.370349E-05

Epoch 75 of 500
  training loss:		1.637989E-05
  validation loss:		3.716546E-05

Early stopping, val-loss increased over the last 15 epochs from 0.000887123911641 to 0.00145668744469
Training RMSE: 9.49528726803e-09
Validation RMSE: 9.48538197899e-09
