Training network with 45280 training samples and 8490 validation samples
Epoch 1 of 100
  training loss:		1.146570E-01
  validation loss:		8.384858E-03
Epoch took 13.697s

Epoch 2 of 100
  training loss:		3.094635E-02
  validation loss:		5.653839E-03
Epoch took 13.699s

Epoch 3 of 100
  training loss:		2.192202E-02
  validation loss:		3.805871E-03
Epoch took 13.697s

Epoch 4 of 100
  training loss:		1.739572E-02
  validation loss:		4.144216E-03
Epoch took 13.659s

Epoch 5 of 100
  training loss:		1.445991E-02
  validation loss:		2.510865E-03
Epoch took 13.677s

Epoch 6 of 100
  training loss:		1.346840E-02
  validation loss:		2.989420E-03
Epoch took 13.716s

Epoch 7 of 100
  training loss:		1.166645E-02
  validation loss:		2.524561E-03
Epoch took 13.673s

Epoch 8 of 100
  training loss:		9.949401E-03
  validation loss:		2.364579E-03
Epoch took 13.709s

Epoch 9 of 100
  training loss:		8.953989E-03
  validation loss:		1.582344E-03
Epoch took 13.700s

Epoch 10 of 100
  training loss:		8.308708E-03
  validation loss:		1.313973E-03
Epoch took 13.717s

Epoch 11 of 100
  training loss:		7.444359E-03
  validation loss:		1.342316E-03
Epoch took 13.703s

Epoch 12 of 100
  training loss:		7.037798E-03
  validation loss:		1.788406E-03
Epoch took 13.715s

Epoch 13 of 100
  training loss:		6.246050E-03
  validation loss:		1.605582E-03
Epoch took 13.695s

Epoch 14 of 100
  training loss:		5.844800E-03
  validation loss:		1.105341E-03
Epoch took 13.741s

Epoch 15 of 100
  training loss:		5.535243E-03
  validation loss:		1.722477E-03
Epoch took 13.706s

Epoch 16 of 100
  training loss:		5.087729E-03
  validation loss:		9.066756E-04
Epoch took 13.705s

Epoch 17 of 100
  training loss:		4.911694E-03
  validation loss:		1.156321E-03
Epoch took 13.700s

Epoch 18 of 100
  training loss:		4.435639E-03
  validation loss:		9.600575E-04
Epoch took 13.689s

Epoch 19 of 100
  training loss:		4.089035E-03
  validation loss:		1.006824E-03
Epoch took 13.681s

Epoch 20 of 100
  training loss:		3.876851E-03
  validation loss:		8.796223E-04
Epoch took 13.709s

Epoch 21 of 100
  training loss:		3.775936E-03
  validation loss:		6.671319E-04
Epoch took 13.690s

Epoch 22 of 100
  training loss:		3.432553E-03
  validation loss:		6.347606E-04
Epoch took 13.682s

Epoch 23 of 100
  training loss:		3.296063E-03
  validation loss:		1.265522E-03
Epoch took 13.669s

Epoch 24 of 100
  training loss:		3.037745E-03
  validation loss:		5.015789E-04
Epoch took 13.673s

Epoch 25 of 100
  training loss:		2.967016E-03
  validation loss:		5.299524E-04
Epoch took 13.687s

Epoch 26 of 100
  training loss:		2.806413E-03
  validation loss:		6.069591E-04
Epoch took 13.694s

Epoch 27 of 100
  training loss:		2.822365E-03
  validation loss:		5.470737E-04
Epoch took 13.694s

Epoch 28 of 100
  training loss:		2.569317E-03
  validation loss:		4.496849E-04
Epoch took 13.711s

Epoch 29 of 100
  training loss:		2.488567E-03
  validation loss:		8.106901E-04
Epoch took 13.673s

Epoch 30 of 100
  training loss:		2.388001E-03
  validation loss:		5.929722E-04
Epoch took 13.711s

Epoch 31 of 100
  training loss:		2.230436E-03
  validation loss:		5.875062E-04
Epoch took 13.677s

Epoch 32 of 100
  training loss:		2.205568E-03
  validation loss:		4.445291E-04
Epoch took 13.701s

Epoch 33 of 100
  training loss:		2.132176E-03
  validation loss:		3.925498E-04
Epoch took 13.644s

Epoch 34 of 100
  training loss:		2.012552E-03
  validation loss:		3.725892E-04
Epoch took 13.699s

Epoch 35 of 100
  training loss:		1.941400E-03
  validation loss:		4.197981E-04
Epoch took 13.667s

Epoch 36 of 100
  training loss:		1.779872E-03
  validation loss:		5.082661E-04
Epoch took 13.690s

Epoch 37 of 100
  training loss:		1.733626E-03
  validation loss:		4.570969E-04
Epoch took 13.699s

Epoch 38 of 100
  training loss:		1.685990E-03
  validation loss:		5.241324E-04
Epoch took 13.703s

Epoch 39 of 100
  training loss:		1.657568E-03
  validation loss:		4.147472E-04
Epoch took 13.644s

Epoch 40 of 100
  training loss:		1.492362E-03
  validation loss:		2.920120E-04
Epoch took 13.652s

Epoch 41 of 100
  training loss:		1.536396E-03
  validation loss:		4.141697E-04
Epoch took 13.682s

Epoch 42 of 100
  training loss:		1.507103E-03
  validation loss:		3.393857E-04
Epoch took 13.683s

Epoch 43 of 100
  training loss:		1.410832E-03
  validation loss:		3.314814E-04
Epoch took 13.711s

Epoch 44 of 100
  training loss:		1.346603E-03
  validation loss:		3.810310E-04
Epoch took 13.693s

Epoch 45 of 100
  training loss:		1.370358E-03
  validation loss:		3.965676E-04
Epoch took 13.674s

Epoch 46 of 100
  training loss:		1.296426E-03
  validation loss:		4.330139E-04
Epoch took 13.722s

Epoch 47 of 100
  training loss:		1.280232E-03
  validation loss:		2.175658E-04
Epoch took 13.666s

Epoch 48 of 100
  training loss:		1.197267E-03
  validation loss:		2.034345E-04
Epoch took 13.697s

Epoch 49 of 100
  training loss:		1.147076E-03
  validation loss:		1.641408E-04
Epoch took 13.668s

Epoch 50 of 100
  training loss:		1.125832E-03
  validation loss:		3.393982E-04
Epoch took 13.716s

Epoch 51 of 100
  training loss:		1.215808E-03
  validation loss:		2.505400E-04
Epoch took 13.692s

Epoch 52 of 100
  training loss:		1.050060E-03
  validation loss:		1.861189E-04
Epoch took 13.712s

Epoch 53 of 100
  training loss:		1.081041E-03
  validation loss:		2.180297E-04
Epoch took 13.678s

Epoch 54 of 100
  training loss:		1.013240E-03
  validation loss:		2.146955E-04
Epoch took 13.718s

Epoch 55 of 100
  training loss:		1.041641E-03
  validation loss:		2.578166E-04
Epoch took 13.677s

Epoch 56 of 100
  training loss:		9.835095E-04
  validation loss:		2.752120E-04
Epoch took 13.690s

Epoch 57 of 100
  training loss:		9.809932E-04
  validation loss:		2.315310E-04
Epoch took 13.697s

Epoch 58 of 100
  training loss:		9.178832E-04
  validation loss:		2.279389E-04
Epoch took 13.717s

Epoch 59 of 100
  training loss:		9.209316E-04
  validation loss:		3.392576E-04
Epoch took 13.724s

Epoch 60 of 100
  training loss:		9.182505E-04
  validation loss:		1.496162E-04
Epoch took 13.719s

Early stopping, val-loss increased over the last 5 epochs from 0.000225440146932 to 0.000244711143259
Training RMSE: 1.79420034817e-08
Validation RMSE: 1.80586889762e-08
