Training network with 45280 training samples and 8490 validation samples
Epoch 1 of 100
  training loss:		6.196620E-02
  validation loss:		1.337203E-02
Epoch took 13.698s

Epoch 2 of 100
  training loss:		1.142768E-02
  validation loss:		2.688061E-03
Epoch took 13.728s

Epoch 3 of 100
  training loss:		7.896272E-03
  validation loss:		2.106289E-03
Epoch took 13.685s

Epoch 4 of 100
  training loss:		5.815983E-03
  validation loss:		1.304092E-03
Epoch took 13.858s

Epoch 5 of 100
  training loss:		4.860649E-03
  validation loss:		1.289129E-03
Epoch took 13.652s

Epoch 6 of 100
  training loss:		3.743949E-03
  validation loss:		1.083691E-03
Epoch took 13.756s

Epoch 7 of 100
  training loss:		3.060837E-03
  validation loss:		1.096503E-03
Epoch took 13.679s

Epoch 8 of 100
  training loss:		2.712463E-03
  validation loss:		8.620690E-04
Epoch took 13.728s

Epoch 9 of 100
  training loss:		2.335069E-03
  validation loss:		8.596786E-04
Epoch took 13.686s

Epoch 10 of 100
  training loss:		2.132250E-03
  validation loss:		1.004221E-03
Epoch took 13.798s

Epoch 11 of 100
  training loss:		1.876903E-03
  validation loss:		6.002820E-04
Epoch took 13.676s

Epoch 12 of 100
  training loss:		1.673188E-03
  validation loss:		7.555800E-04
Epoch took 13.716s

Epoch 13 of 100
  training loss:		1.566462E-03
  validation loss:		5.017103E-04
Epoch took 13.684s

Epoch 14 of 100
  training loss:		1.392808E-03
  validation loss:		6.702695E-04
Epoch took 13.736s

Epoch 15 of 100
  training loss:		1.285263E-03
  validation loss:		4.651604E-04
Epoch took 13.652s

Epoch 16 of 100
  training loss:		1.111563E-03
  validation loss:		3.477567E-04
Epoch took 13.700s

Epoch 17 of 100
  training loss:		1.064940E-03
  validation loss:		3.997958E-04
Epoch took 13.685s

Epoch 18 of 100
  training loss:		9.199655E-04
  validation loss:		4.951644E-04
Epoch took 13.761s

Epoch 19 of 100
  training loss:		9.118812E-04
  validation loss:		4.351590E-04
Epoch took 13.686s

Epoch 20 of 100
  training loss:		8.486851E-04
  validation loss:		4.357710E-04
Epoch took 13.677s

Epoch 21 of 100
  training loss:		7.692334E-04
  validation loss:		5.857852E-04
Epoch took 13.675s

Epoch 22 of 100
  training loss:		7.189126E-04
  validation loss:		2.622895E-04
Epoch took 13.808s

Epoch 23 of 100
  training loss:		6.306426E-04
  validation loss:		3.148712E-04
Epoch took 13.664s

Epoch 24 of 100
  training loss:		6.332375E-04
  validation loss:		2.982424E-04
Epoch took 13.677s

Epoch 25 of 100
  training loss:		5.620501E-04
  validation loss:		2.499342E-04
Epoch took 13.702s

Epoch 26 of 100
  training loss:		5.013624E-04
  validation loss:		2.202831E-04
Epoch took 13.768s

Epoch 27 of 100
  training loss:		4.910515E-04
  validation loss:		2.691936E-04
Epoch took 13.684s

Epoch 28 of 100
  training loss:		4.531293E-04
  validation loss:		2.038788E-04
Epoch took 13.752s

Epoch 29 of 100
  training loss:		4.537567E-04
  validation loss:		1.983178E-04
Epoch took 13.690s

Epoch 30 of 100
  training loss:		4.407800E-04
  validation loss:		2.322913E-04
Epoch took 13.704s

Epoch 31 of 100
  training loss:		3.883139E-04
  validation loss:		2.855080E-04
Epoch took 13.661s

Epoch 32 of 100
  training loss:		3.561801E-04
  validation loss:		2.012930E-04
Epoch took 13.693s

Epoch 33 of 100
  training loss:		3.354686E-04
  validation loss:		2.875128E-04
Epoch took 13.694s

Epoch 34 of 100
  training loss:		3.142807E-04
  validation loss:		2.431212E-04
Epoch took 13.830s

Epoch 35 of 100
  training loss:		3.197895E-04
  validation loss:		1.130966E-04
Epoch took 13.703s

Early stopping, val-loss increased over the last 5 epochs from 0.000224792924159 to 0.00022610633024
Training RMSE: 1.5256796666e-08
Validation RMSE: 1.52898280554e-08
