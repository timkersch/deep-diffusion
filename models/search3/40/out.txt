Training network with 45280 training samples and 8490 validation samples
Epoch 1 of 500
  training loss:		7.677832E-02
  validation loss:		2.666226E-03

Epoch 2 of 500
  training loss:		2.318558E-03
  validation loss:		2.092640E-03

Epoch 3 of 500
  training loss:		1.953880E-03
  validation loss:		1.859440E-03

Epoch 4 of 500
  training loss:		1.777824E-03
  validation loss:		1.682805E-03

Epoch 5 of 500
  training loss:		1.137845E-03
  validation loss:		5.729508E-04

Epoch 6 of 500
  training loss:		4.007566E-04
  validation loss:		2.728494E-04

Epoch 7 of 500
  training loss:		2.259403E-04
  validation loss:		1.724063E-04

Epoch 8 of 500
  training loss:		1.549909E-04
  validation loss:		1.293410E-04

Epoch 9 of 500
  training loss:		1.214603E-04
  validation loss:		1.093200E-04

Epoch 10 of 500
  training loss:		1.037769E-04
  validation loss:		9.416305E-05

Epoch 11 of 500
  training loss:		9.047996E-05
  validation loss:		8.475647E-05

Epoch 12 of 500
  training loss:		8.166421E-05
  validation loss:		7.642390E-05

Epoch 13 of 500
  training loss:		7.312416E-05
  validation loss:		6.934327E-05

Epoch 14 of 500
  training loss:		6.685904E-05
  validation loss:		6.281362E-05

Epoch 15 of 500
  training loss:		6.014920E-05
  validation loss:		5.886403E-05

Epoch 16 of 500
  training loss:		5.476395E-05
  validation loss:		5.195400E-05

Epoch 17 of 500
  training loss:		5.074139E-05
  validation loss:		4.838169E-05

Epoch 18 of 500
  training loss:		4.438533E-05
  validation loss:		4.227480E-05

Epoch 19 of 500
  training loss:		4.054595E-05
  validation loss:		3.728459E-05

Epoch 20 of 500
  training loss:		3.754302E-05
  validation loss:		3.995208E-05

Epoch 21 of 500
  training loss:		3.312714E-05
  validation loss:		3.284852E-05

Epoch 22 of 500
  training loss:		2.978962E-05
  validation loss:		2.902438E-05

Epoch 23 of 500
  training loss:		2.759975E-05
  validation loss:		2.426016E-05

Epoch 24 of 500
  training loss:		2.402202E-05
  validation loss:		2.252182E-05

Epoch 25 of 500
  training loss:		2.188842E-05
  validation loss:		2.007324E-05

Epoch 26 of 500
  training loss:		2.017387E-05
  validation loss:		1.812774E-05

Epoch 27 of 500
  training loss:		1.829830E-05
  validation loss:		1.649314E-05

Epoch 28 of 500
  training loss:		1.693572E-05
  validation loss:		1.539009E-05

Epoch 29 of 500
  training loss:		1.661362E-05
  validation loss:		1.466695E-05

Epoch 30 of 500
  training loss:		1.667831E-05
  validation loss:		1.546693E-05

Epoch 31 of 500
  training loss:		1.487460E-05
  validation loss:		1.286942E-05

Epoch 32 of 500
  training loss:		1.260847E-05
  validation loss:		1.264256E-05

Epoch 33 of 500
  training loss:		1.450137E-05
  validation loss:		1.063705E-05

Epoch 34 of 500
  training loss:		1.423045E-05
  validation loss:		2.120410E-05

Epoch 35 of 500
  training loss:		1.618117E-05
  validation loss:		9.939663E-06

Epoch 36 of 500
  training loss:		1.792736E-05
  validation loss:		4.098933E-05

Epoch 37 of 500
  training loss:		2.324013E-05
  validation loss:		8.574003E-06

Epoch 38 of 500
  training loss:		1.701871E-05
  validation loss:		3.727466E-05

Epoch 39 of 500
  training loss:		2.048036E-05
  validation loss:		1.438425E-05

Epoch 40 of 500
  training loss:		2.156298E-05
  validation loss:		7.706860E-06

Epoch 41 of 500
  training loss:		2.148273E-05
  validation loss:		4.814740E-05

Epoch 42 of 500
  training loss:		2.905549E-05
  validation loss:		1.618275E-05

Epoch 43 of 500
  training loss:		1.421475E-05
  validation loss:		7.029052E-06

Epoch 44 of 500
  training loss:		2.982853E-05
  validation loss:		1.268938E-05

Epoch 45 of 500
  training loss:		1.469613E-05
  validation loss:		5.366336E-05

Epoch 46 of 500
  training loss:		2.095244E-05
  validation loss:		3.234636E-05

Epoch 47 of 500
  training loss:		2.166785E-05
  validation loss:		6.357494E-06

Epoch 48 of 500
  training loss:		2.103994E-05
  validation loss:		7.537189E-06

Epoch 49 of 500
  training loss:		4.245433E-05
  validation loss:		8.708078E-06

Epoch 50 of 500
  training loss:		9.780284E-06
  validation loss:		7.066596E-06

Epoch 51 of 500
  training loss:		1.601744E-05
  validation loss:		4.014439E-05

Epoch 52 of 500
  training loss:		1.967966E-05
  validation loss:		8.057929E-06

Epoch 53 of 500
  training loss:		2.537128E-05
  validation loss:		1.521148E-04

Epoch 54 of 500
  training loss:		2.123862E-05
  validation loss:		5.790693E-05

Epoch 55 of 500
  training loss:		1.496010E-05
  validation loss:		3.623141E-05

Epoch 56 of 500
  training loss:		3.960518E-05
  validation loss:		4.047884E-05

Epoch 57 of 500
  training loss:		1.160901E-05
  validation loss:		1.130573E-05

Epoch 58 of 500
  training loss:		1.518180E-05
  validation loss:		5.869160E-06

Epoch 59 of 500
  training loss:		2.505944E-05
  validation loss:		1.917085E-05

Epoch 60 of 500
  training loss:		1.350663E-05
  validation loss:		6.701907E-05

Early stopping, val-loss increased over the last 15 epochs from 0.000690654418248 to 0.00110069270734
Training RMSE: 4.25835138882e-09
Validation RMSE: 4.28877781334e-09
