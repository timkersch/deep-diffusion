Training network with 45280 training samples and 8490 validation samples
Epoch 1 of 500
  training loss:		4.132905E-02
  validation loss:		9.285056E-03

Epoch 2 of 500
  training loss:		7.459774E-03
  validation loss:		6.224749E-03

Epoch 3 of 500
  training loss:		4.692666E-03
  validation loss:		3.355357E-03

Epoch 4 of 500
  training loss:		2.698839E-03
  validation loss:		1.847052E-03

Epoch 5 of 500
  training loss:		1.250352E-03
  validation loss:		7.289535E-04

Epoch 6 of 500
  training loss:		5.999978E-04
  validation loss:		3.925339E-04

Epoch 7 of 500
  training loss:		3.500800E-04
  validation loss:		2.408156E-04

Epoch 8 of 500
  training loss:		2.265377E-04
  validation loss:		1.638825E-04

Epoch 9 of 500
  training loss:		1.634084E-04
  validation loss:		1.600754E-04

Epoch 10 of 500
  training loss:		1.246893E-04
  validation loss:		8.807654E-05

Epoch 11 of 500
  training loss:		9.798025E-05
  validation loss:		7.128167E-05

Epoch 12 of 500
  training loss:		7.984633E-05
  validation loss:		1.291284E-04

Epoch 13 of 500
  training loss:		7.070253E-05
  validation loss:		4.902612E-05

Epoch 14 of 500
  training loss:		6.003404E-05
  validation loss:		4.242639E-05

Epoch 15 of 500
  training loss:		5.307528E-05
  validation loss:		1.152542E-04

Epoch 16 of 500
  training loss:		4.588443E-05
  validation loss:		4.173966E-05

Epoch 17 of 500
  training loss:		4.085469E-05
  validation loss:		2.758081E-05

Epoch 18 of 500
  training loss:		3.956722E-05
  validation loss:		3.323343E-05

Epoch 19 of 500
  training loss:		3.613792E-05
  validation loss:		3.779256E-05

Epoch 20 of 500
  training loss:		3.151391E-05
  validation loss:		2.053175E-05

Epoch 21 of 500
  training loss:		2.903323E-05
  validation loss:		3.710235E-05

Epoch 22 of 500
  training loss:		2.719750E-05
  validation loss:		3.764704E-05

Epoch 23 of 500
  training loss:		2.554868E-05
  validation loss:		3.191915E-05

Epoch 24 of 500
  training loss:		2.472643E-05
  validation loss:		2.208080E-05

Epoch 25 of 500
  training loss:		2.337531E-05
  validation loss:		1.548532E-05

Epoch 26 of 500
  training loss:		2.230606E-05
  validation loss:		1.297171E-05

Epoch 27 of 500
  training loss:		1.922923E-05
  validation loss:		1.222356E-05

Epoch 28 of 500
  training loss:		1.847200E-05
  validation loss:		1.105048E-05

Epoch 29 of 500
  training loss:		1.861831E-05
  validation loss:		1.251904E-05

Epoch 30 of 500
  training loss:		1.647588E-05
  validation loss:		1.672523E-05

Epoch 31 of 500
  training loss:		1.588109E-05
  validation loss:		1.009908E-05

Epoch 32 of 500
  training loss:		1.561949E-05
  validation loss:		1.037973E-05

Epoch 33 of 500
  training loss:		1.437570E-05
  validation loss:		9.286378E-06

Epoch 34 of 500
  training loss:		1.426705E-05
  validation loss:		1.431586E-05

Epoch 35 of 500
  training loss:		1.323675E-05
  validation loss:		2.194829E-05

Epoch 36 of 500
  training loss:		1.232231E-05
  validation loss:		2.500266E-05

Epoch 37 of 500
  training loss:		1.302313E-05
  validation loss:		6.101727E-06

Epoch 38 of 500
  training loss:		1.172575E-05
  validation loss:		2.190416E-05

Epoch 39 of 500
  training loss:		1.122181E-05
  validation loss:		6.469760E-06

Epoch 40 of 500
  training loss:		1.055991E-05
  validation loss:		3.427248E-05

Epoch 41 of 500
  training loss:		1.062846E-05
  validation loss:		5.094846E-06

Epoch 42 of 500
  training loss:		1.032326E-05
  validation loss:		7.557208E-06

Epoch 43 of 500
  training loss:		8.998194E-06
  validation loss:		4.405640E-06

Epoch 44 of 500
  training loss:		9.144605E-06
  validation loss:		4.035853E-06

Epoch 45 of 500
  training loss:		9.159053E-06
  validation loss:		5.027747E-06

Epoch 46 of 500
  training loss:		8.194742E-06
  validation loss:		8.511565E-06

Epoch 47 of 500
  training loss:		7.895907E-06
  validation loss:		3.904228E-06

Epoch 48 of 500
  training loss:		7.992631E-06
  validation loss:		3.364118E-06

Epoch 49 of 500
  training loss:		7.877933E-06
  validation loss:		5.965903E-06

Epoch 50 of 500
  training loss:		7.773892E-06
  validation loss:		1.617079E-05

Epoch 51 of 500
  training loss:		7.009071E-06
  validation loss:		2.827916E-06

Epoch 52 of 500
  training loss:		6.920144E-06
  validation loss:		9.860882E-06

Epoch 53 of 500
  training loss:		6.529267E-06
  validation loss:		3.779840E-06

Epoch 54 of 500
  training loss:		6.521674E-06
  validation loss:		2.489095E-06

Epoch 55 of 500
  training loss:		6.542211E-06
  validation loss:		2.343593E-06

Epoch 56 of 500
  training loss:		6.056585E-06
  validation loss:		4.052487E-06

Epoch 57 of 500
  training loss:		6.610841E-06
  validation loss:		6.256934E-06

Epoch 58 of 500
  training loss:		5.621114E-06
  validation loss:		1.610788E-05

Epoch 59 of 500
  training loss:		6.391058E-06
  validation loss:		1.920897E-06

Epoch 60 of 500
  training loss:		5.532633E-06
  validation loss:		6.544667E-06

Epoch 61 of 500
  training loss:		5.390487E-06
  validation loss:		5.480824E-06

Epoch 62 of 500
  training loss:		5.901590E-06
  validation loss:		1.852367E-06

Epoch 63 of 500
  training loss:		5.474147E-06
  validation loss:		4.239216E-06

Epoch 64 of 500
  training loss:		5.182707E-06
  validation loss:		1.121333E-05

Epoch 65 of 500
  training loss:		5.245721E-06
  validation loss:		6.368587E-06

Epoch 66 of 500
  training loss:		5.047297E-06
  validation loss:		1.791882E-06

Epoch 67 of 500
  training loss:		4.624192E-06
  validation loss:		1.744013E-06

Epoch 68 of 500
  training loss:		4.666012E-06
  validation loss:		1.899483E-06

Epoch 69 of 500
  training loss:		5.043284E-06
  validation loss:		2.382664E-06

Epoch 70 of 500
  training loss:		4.550629E-06
  validation loss:		4.852070E-06

Epoch 71 of 500
  training loss:		4.757794E-06
  validation loss:		2.546049E-06

Epoch 72 of 500
  training loss:		4.761639E-06
  validation loss:		1.263636E-06

Epoch 73 of 500
  training loss:		4.021705E-06
  validation loss:		3.806991E-06

Epoch 74 of 500
  training loss:		4.523297E-06
  validation loss:		5.772775E-06

Epoch 75 of 500
  training loss:		4.456281E-06
  validation loss:		1.161857E-05

Epoch 76 of 500
  training loss:		4.566432E-06
  validation loss:		1.876096E-05

Epoch 77 of 500
  training loss:		4.287407E-06
  validation loss:		1.206246E-06

Epoch 78 of 500
  training loss:		4.172911E-06
  validation loss:		3.920576E-06

Epoch 79 of 500
  training loss:		4.260543E-06
  validation loss:		6.087879E-06

Epoch 80 of 500
  training loss:		4.284745E-06
  validation loss:		4.947494E-06

Epoch 81 of 500
  training loss:		3.982135E-06
  validation loss:		2.420779E-05

Epoch 82 of 500
  training loss:		4.379855E-06
  validation loss:		8.753194E-07

Epoch 83 of 500
  training loss:		3.891448E-06
  validation loss:		8.281020E-07

Epoch 84 of 500
  training loss:		4.248583E-06
  validation loss:		1.296031E-06

Epoch 85 of 500
  training loss:		4.048912E-06
  validation loss:		9.798960E-06

Epoch 86 of 500
  training loss:		3.778801E-06
  validation loss:		9.045323E-07

Epoch 87 of 500
  training loss:		3.712691E-06
  validation loss:		7.363084E-07

Epoch 88 of 500
  training loss:		4.082471E-06
  validation loss:		2.639227E-06

Epoch 89 of 500
  training loss:		3.584567E-06
  validation loss:		2.197820E-06

Epoch 90 of 500
  training loss:		3.583023E-06
  validation loss:		7.687414E-07

Early stopping, val-loss increased over the last 15 epochs from 0.00118070681907 to 0.00139877577181
Training RMSE: 1.45657852573e-09
Validation RMSE: 1.45232611298e-09
