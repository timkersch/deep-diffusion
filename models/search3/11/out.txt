Training network with 45280 training samples and 8490 validation samples
Epoch 1 of 100
  training loss:		5.459506E-02
  validation loss:		5.595565E-03
Epoch took 13.892s

Epoch 2 of 100
  training loss:		1.166749E-02
  validation loss:		2.826640E-03
Epoch took 13.861s

Epoch 3 of 100
  training loss:		7.736877E-03
  validation loss:		2.253431E-03
Epoch took 13.888s

Epoch 4 of 100
  training loss:		5.631064E-03
  validation loss:		1.201282E-03
Epoch took 13.855s

Epoch 5 of 100
  training loss:		4.617112E-03
  validation loss:		2.003291E-03
Epoch took 13.852s

Epoch 6 of 100
  training loss:		3.894189E-03
  validation loss:		2.071004E-03
Epoch took 13.851s

Epoch 7 of 100
  training loss:		3.392607E-03
  validation loss:		1.348811E-03
Epoch took 13.873s

Epoch 8 of 100
  training loss:		2.948265E-03
  validation loss:		9.560232E-04
Epoch took 13.869s

Epoch 9 of 100
  training loss:		2.465092E-03
  validation loss:		8.267950E-04
Epoch took 13.907s

Epoch 10 of 100
  training loss:		2.151785E-03
  validation loss:		7.541414E-04
Epoch took 13.871s

Epoch 11 of 100
  training loss:		1.851332E-03
  validation loss:		6.704372E-04
Epoch took 13.901s

Epoch 12 of 100
  training loss:		1.607054E-03
  validation loss:		6.471245E-04
Epoch took 13.859s

Epoch 13 of 100
  training loss:		1.517166E-03
  validation loss:		5.764380E-04
Epoch took 13.891s

Epoch 14 of 100
  training loss:		1.359292E-03
  validation loss:		4.433121E-04
Epoch took 13.881s

Epoch 15 of 100
  training loss:		1.246657E-03
  validation loss:		5.684858E-04
Epoch took 13.914s

Epoch 16 of 100
  training loss:		1.131121E-03
  validation loss:		4.947824E-04
Epoch took 13.862s

Epoch 17 of 100
  training loss:		1.024924E-03
  validation loss:		5.333902E-04
Epoch took 13.876s

Epoch 18 of 100
  training loss:		9.785729E-04
  validation loss:		3.654554E-04
Epoch took 13.856s

Epoch 19 of 100
  training loss:		8.648859E-04
  validation loss:		2.925561E-04
Epoch took 13.898s

Epoch 20 of 100
  training loss:		8.083605E-04
  validation loss:		2.345512E-04
Epoch took 13.851s

Epoch 21 of 100
  training loss:		7.187797E-04
  validation loss:		6.410482E-04
Epoch took 13.891s

Epoch 22 of 100
  training loss:		6.487246E-04
  validation loss:		3.767716E-04
Epoch took 13.859s

Epoch 23 of 100
  training loss:		6.088655E-04
  validation loss:		3.659635E-04
Epoch took 13.900s

Epoch 24 of 100
  training loss:		5.723937E-04
  validation loss:		4.560939E-04
Epoch took 13.851s

Epoch 25 of 100
  training loss:		5.243807E-04
  validation loss:		2.428090E-04
Epoch took 13.902s

Epoch 26 of 100
  training loss:		5.129335E-04
  validation loss:		1.741557E-04
Epoch took 13.862s

Epoch 27 of 100
  training loss:		4.878110E-04
  validation loss:		5.290825E-04
Epoch took 13.895s

Epoch 28 of 100
  training loss:		4.134831E-04
  validation loss:		1.383090E-04
Epoch took 13.907s

Epoch 29 of 100
  training loss:		3.844278E-04
  validation loss:		1.969522E-04
Epoch took 13.881s

Epoch 30 of 100
  training loss:		3.794953E-04
  validation loss:		2.193359E-04
Epoch took 13.876s

Epoch 31 of 100
  training loss:		3.868582E-04
  validation loss:		1.548972E-04
Epoch took 13.890s

Epoch 32 of 100
  training loss:		3.736124E-04
  validation loss:		3.496029E-04
Epoch took 13.885s

Epoch 33 of 100
  training loss:		2.964871E-04
  validation loss:		1.217753E-04
Epoch took 13.907s

Epoch 34 of 100
  training loss:		3.012932E-04
  validation loss:		1.578813E-04
Epoch took 13.870s

Epoch 35 of 100
  training loss:		2.754232E-04
  validation loss:		1.138877E-04
Epoch took 13.902s

Epoch 36 of 100
  training loss:		2.441922E-04
  validation loss:		8.920268E-05
Epoch took 13.887s

Epoch 37 of 100
  training loss:		2.603566E-04
  validation loss:		1.924484E-04
Epoch took 13.919s

Epoch 38 of 100
  training loss:		2.588087E-04
  validation loss:		1.194376E-04
Epoch took 13.882s

Epoch 39 of 100
  training loss:		2.372410E-04
  validation loss:		1.395657E-04
Epoch took 13.905s

Epoch 40 of 100
  training loss:		2.162049E-04
  validation loss:		6.465995E-05
Epoch took 13.877s

Epoch 41 of 100
  training loss:		2.201443E-04
  validation loss:		2.164222E-04
Epoch took 13.915s

Epoch 42 of 100
  training loss:		2.077667E-04
  validation loss:		4.581382E-05
Epoch took 13.855s

Epoch 43 of 100
  training loss:		1.910766E-04
  validation loss:		6.451851E-05
Epoch took 13.915s

Epoch 44 of 100
  training loss:		1.959953E-04
  validation loss:		7.548384E-05
Epoch took 13.853s

Epoch 45 of 100
  training loss:		1.838097E-04
  validation loss:		6.864899E-05
Epoch took 13.891s

Epoch 46 of 100
  training loss:		1.728638E-04
  validation loss:		6.830209E-05
Epoch took 13.888s

Epoch 47 of 100
  training loss:		1.878192E-04
  validation loss:		6.217391E-05
Epoch took 13.883s

Epoch 48 of 100
  training loss:		1.639867E-04
  validation loss:		2.778757E-04
Epoch took 13.867s

Epoch 49 of 100
  training loss:		1.714661E-04
  validation loss:		5.514104E-05
Epoch took 13.890s

Epoch 50 of 100
  training loss:		1.496518E-04
  validation loss:		8.067457E-05
Epoch took 13.909s

Epoch 51 of 100
  training loss:		1.384278E-04
  validation loss:		3.877133E-05
Epoch took 13.908s

Epoch 52 of 100
  training loss:		1.366808E-04
  validation loss:		7.829818E-05
Epoch took 13.877s

Epoch 53 of 100
  training loss:		1.390573E-04
  validation loss:		3.904205E-05
Epoch took 13.891s

Epoch 54 of 100
  training loss:		1.354484E-04
  validation loss:		1.173441E-04
Epoch took 13.876s

Epoch 55 of 100
  training loss:		1.330544E-04
  validation loss:		6.074884E-05
Epoch took 13.897s

Epoch 56 of 100
  training loss:		1.158519E-04
  validation loss:		4.270355E-05
Epoch took 13.885s

Epoch 57 of 100
  training loss:		1.081504E-04
  validation loss:		9.508646E-05
Epoch took 13.922s

Epoch 58 of 100
  training loss:		1.136020E-04
  validation loss:		4.764382E-05
Epoch took 13.858s

Epoch 59 of 100
  training loss:		1.122576E-04
  validation loss:		1.247714E-04
Epoch took 13.874s

Epoch 60 of 100
  training loss:		1.055621E-04
  validation loss:		4.246197E-05
Epoch took 13.840s

Epoch 61 of 100
  training loss:		1.058215E-04
  validation loss:		5.069197E-05
Epoch took 13.901s

Epoch 62 of 100
  training loss:		9.951061E-05
  validation loss:		5.910107E-05
Epoch took 13.856s

Epoch 63 of 100
  training loss:		9.460477E-05
  validation loss:		7.623957E-05
Epoch took 13.895s

Epoch 64 of 100
  training loss:		1.058754E-04
  validation loss:		4.176451E-05
Epoch took 13.867s

Epoch 65 of 100
  training loss:		9.244187E-05
  validation loss:		5.458378E-05
Epoch took 13.896s

Epoch 66 of 100
  training loss:		9.459624E-05
  validation loss:		8.480914E-05
Epoch took 13.944s

Epoch 67 of 100
  training loss:		8.783461E-05
  validation loss:		4.795297E-05
Epoch took 14.079s

Epoch 68 of 100
  training loss:		8.070602E-05
  validation loss:		3.601450E-05
Epoch took 14.178s

Epoch 69 of 100
  training loss:		8.455583E-05
  validation loss:		4.996519E-05
Epoch took 14.170s

Epoch 70 of 100
  training loss:		7.612354E-05
  validation loss:		3.214631E-05
Epoch took 14.185s

Epoch 71 of 100
  training loss:		7.901359E-05
  validation loss:		2.997963E-05
Epoch took 14.235s

Epoch 72 of 100
  training loss:		8.851808E-05
  validation loss:		3.672844E-05
Epoch took 14.274s

Epoch 73 of 100
  training loss:		7.558794E-05
  validation loss:		1.263693E-04
Epoch took 14.252s

Epoch 74 of 100
  training loss:		8.770775E-05
  validation loss:		5.640320E-05
Epoch took 14.261s

Epoch 75 of 100
  training loss:		7.976380E-05
  validation loss:		2.417782E-05
Epoch took 14.235s

Epoch 76 of 100
  training loss:		8.180147E-05
  validation loss:		4.883030E-05
Epoch took 14.388s

Epoch 77 of 100
  training loss:		7.230666E-05
  validation loss:		1.565743E-04
Epoch took 14.290s

Epoch 78 of 100
  training loss:		7.024250E-05
  validation loss:		6.882663E-05
Epoch took 14.242s

Epoch 79 of 100
  training loss:		6.192357E-05
  validation loss:		8.921803E-05
Epoch took 14.286s

Epoch 80 of 100
  training loss:		6.178554E-05
  validation loss:		2.248093E-05
Epoch took 14.208s

Early stopping, val-loss increased over the last 10 epochs from 5.33269012725e-05 to 6.59588547584e-05
Training RMSE: 9.15979653733e-09
Validation RMSE: 9.24330435942e-09
