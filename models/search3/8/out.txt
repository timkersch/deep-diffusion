Training network with 45280 training samples and 8490 validation samples
Epoch 1 of 100
  training loss:		2.537630E-02
  validation loss:		5.718318E-03
Epoch took 13.671s

Epoch 2 of 100
  training loss:		6.634207E-03
  validation loss:		1.180314E-03
Epoch took 13.776s

Epoch 3 of 100
  training loss:		4.342347E-03
  validation loss:		1.093819E-03
Epoch took 13.690s

Epoch 4 of 100
  training loss:		2.875113E-03
  validation loss:		8.442878E-04
Epoch took 13.843s

Epoch 5 of 100
  training loss:		2.264842E-03
  validation loss:		1.826833E-03
Epoch took 13.707s

Epoch 6 of 100
  training loss:		1.812086E-03
  validation loss:		9.407447E-04
Epoch took 13.783s

Epoch 7 of 100
  training loss:		1.516646E-03
  validation loss:		6.455550E-04
Epoch took 13.695s

Epoch 8 of 100
  training loss:		1.297331E-03
  validation loss:		6.861375E-04
Epoch took 13.869s

Epoch 9 of 100
  training loss:		1.088469E-03
  validation loss:		6.073460E-04
Epoch took 13.710s

Epoch 10 of 100
  training loss:		1.004903E-03
  validation loss:		9.126972E-04
Epoch took 13.869s

Epoch 11 of 100
  training loss:		8.605578E-04
  validation loss:		2.558125E-04
Epoch took 13.720s

Epoch 12 of 100
  training loss:		7.620132E-04
  validation loss:		2.489449E-04
Epoch took 13.848s

Epoch 13 of 100
  training loss:		6.319110E-04
  validation loss:		2.598794E-04
Epoch took 13.682s

Epoch 14 of 100
  training loss:		5.938307E-04
  validation loss:		3.978610E-04
Epoch took 13.802s

Epoch 15 of 100
  training loss:		5.490473E-04
  validation loss:		2.626592E-04
Epoch took 13.676s

Epoch 16 of 100
  training loss:		5.031452E-04
  validation loss:		1.988593E-04
Epoch took 13.767s

Epoch 17 of 100
  training loss:		4.564593E-04
  validation loss:		2.518721E-04
Epoch took 13.703s

Epoch 18 of 100
  training loss:		4.324867E-04
  validation loss:		1.388312E-04
Epoch took 13.799s

Epoch 19 of 100
  training loss:		3.751554E-04
  validation loss:		1.152034E-04
Epoch took 13.694s

Epoch 20 of 100
  training loss:		3.625776E-04
  validation loss:		1.254510E-04
Epoch took 13.796s

Epoch 21 of 100
  training loss:		3.129422E-04
  validation loss:		1.516127E-04
Epoch took 13.686s

Epoch 22 of 100
  training loss:		2.893709E-04
  validation loss:		1.582594E-04
Epoch took 13.804s

Epoch 23 of 100
  training loss:		2.500699E-04
  validation loss:		1.536809E-04
Epoch took 13.682s

Epoch 24 of 100
  training loss:		2.650600E-04
  validation loss:		1.440799E-04
Epoch took 13.831s

Epoch 25 of 100
  training loss:		2.557065E-04
  validation loss:		8.125291E-05
Epoch took 13.687s

Epoch 26 of 100
  training loss:		2.213735E-04
  validation loss:		9.446893E-05
Epoch took 13.799s

Epoch 27 of 100
  training loss:		2.122940E-04
  validation loss:		1.438364E-04
Epoch took 13.702s

Epoch 28 of 100
  training loss:		2.306956E-04
  validation loss:		3.416849E-04
Epoch took 13.840s

Epoch 29 of 100
  training loss:		2.120837E-04
  validation loss:		9.766886E-05
Epoch took 13.715s

Epoch 30 of 100
  training loss:		1.671692E-04
  validation loss:		1.016590E-04
Epoch took 13.826s

Early stopping, val-loss increased over the last 5 epochs from 0.000137777161315 to 0.000155863613186
Training RMSE: 9.67950416294e-09
Validation RMSE: 9.68425491708e-09
