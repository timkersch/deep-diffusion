Training network with 45280 training samples and 8490 validation samples
Epoch 1 of 100
  training loss:		5.773139E-01
  validation loss:		1.876276E-01
Epoch took 13.855s

Epoch 2 of 100
  training loss:		1.492759E-01
  validation loss:		3.081776E-02
Epoch took 13.708s

Epoch 3 of 100
  training loss:		6.116434E-02
  validation loss:		9.086447E-03
Epoch took 13.731s

Epoch 4 of 100
  training loss:		3.540767E-02
  validation loss:		8.733672E-03
Epoch took 13.713s

Epoch 5 of 100
  training loss:		2.802594E-02
  validation loss:		8.056482E-03
Epoch took 13.862s

Epoch 6 of 100
  training loss:		2.393598E-02
  validation loss:		6.579561E-03
Epoch took 13.706s

Epoch 7 of 100
  training loss:		2.173205E-02
  validation loss:		5.943725E-03
Epoch took 13.827s

Epoch 8 of 100
  training loss:		1.894078E-02
  validation loss:		6.303509E-03
Epoch took 13.710s

Epoch 9 of 100
  training loss:		1.652351E-02
  validation loss:		4.932715E-03
Epoch took 13.852s

Epoch 10 of 100
  training loss:		1.596059E-02
  validation loss:		6.345653E-03
Epoch took 13.714s

Epoch 11 of 100
  training loss:		1.405335E-02
  validation loss:		3.416708E-03
Epoch took 13.856s

Epoch 12 of 100
  training loss:		1.290463E-02
  validation loss:		3.070761E-03
Epoch took 13.699s

Epoch 13 of 100
  training loss:		1.188362E-02
  validation loss:		2.966032E-03
Epoch took 13.805s

Epoch 14 of 100
  training loss:		1.178882E-02
  validation loss:		2.155772E-03
Epoch took 13.707s

Epoch 15 of 100
  training loss:		1.057766E-02
  validation loss:		2.891671E-03
Epoch took 13.681s

Epoch 16 of 100
  training loss:		9.837259E-03
  validation loss:		2.643639E-03
Epoch took 13.717s

Epoch 17 of 100
  training loss:		9.424948E-03
  validation loss:		1.839290E-03
Epoch took 13.834s

Epoch 18 of 100
  training loss:		9.236609E-03
  validation loss:		1.597148E-03
Epoch took 13.713s

Epoch 19 of 100
  training loss:		8.240594E-03
  validation loss:		8.764899E-04
Epoch took 13.858s

Epoch 20 of 100
  training loss:		7.798179E-03
  validation loss:		1.217241E-03
Epoch took 13.724s

Epoch 21 of 100
  training loss:		7.314053E-03
  validation loss:		1.454244E-03
Epoch took 13.849s

Epoch 22 of 100
  training loss:		7.057652E-03
  validation loss:		9.214073E-04
Epoch took 13.698s

Epoch 23 of 100
  training loss:		6.688072E-03
  validation loss:		1.284277E-03
Epoch took 13.736s

Epoch 24 of 100
  training loss:		6.625695E-03
  validation loss:		9.252485E-04
Epoch took 13.720s

Epoch 25 of 100
  training loss:		6.291284E-03
  validation loss:		1.348643E-03
Epoch took 13.841s

Epoch 26 of 100
  training loss:		5.996450E-03
  validation loss:		8.667741E-04
Epoch took 13.720s

Epoch 27 of 100
  training loss:		5.554202E-03
  validation loss:		1.013528E-03
Epoch took 13.840s

Epoch 28 of 100
  training loss:		5.279743E-03
  validation loss:		9.866166E-04
Epoch took 13.702s

Epoch 29 of 100
  training loss:		5.231958E-03
  validation loss:		8.311822E-04
Epoch took 13.835s

Epoch 30 of 100
  training loss:		4.981114E-03
  validation loss:		9.547606E-04
Epoch took 13.718s

Epoch 31 of 100
  training loss:		4.878129E-03
  validation loss:		9.329262E-04
Epoch took 13.719s

Epoch 32 of 100
  training loss:		4.593986E-03
  validation loss:		7.472967E-04
Epoch took 13.729s

Epoch 33 of 100
  training loss:		4.593823E-03
  validation loss:		7.572202E-04
Epoch took 13.851s

Epoch 34 of 100
  training loss:		4.492817E-03
  validation loss:		6.236297E-04
Epoch took 13.715s

Epoch 35 of 100
  training loss:		4.210472E-03
  validation loss:		6.557500E-04
Epoch took 13.743s

Epoch 36 of 100
  training loss:		3.994057E-03
  validation loss:		8.635659E-04
Epoch took 13.700s

Epoch 37 of 100
  training loss:		3.880399E-03
  validation loss:		6.600882E-04
Epoch took 13.855s

Epoch 38 of 100
  training loss:		3.781304E-03
  validation loss:		6.892664E-04
Epoch took 13.690s

Epoch 39 of 100
  training loss:		3.573091E-03
  validation loss:		6.999840E-04
Epoch took 13.697s

Epoch 40 of 100
  training loss:		3.391621E-03
  validation loss:		5.758736E-04
Epoch took 13.714s

Epoch 41 of 100
  training loss:		3.567431E-03
  validation loss:		6.223014E-04
Epoch took 13.834s

Epoch 42 of 100
  training loss:		3.272334E-03
  validation loss:		8.707311E-04
Epoch took 13.733s

Epoch 43 of 100
  training loss:		3.170825E-03
  validation loss:		5.908257E-04
Epoch took 13.724s

Epoch 44 of 100
  training loss:		3.228301E-03
  validation loss:		5.428095E-04
Epoch took 13.696s

Epoch 45 of 100
  training loss:		3.190136E-03
  validation loss:		5.970474E-04
Epoch took 13.746s

Epoch 46 of 100
  training loss:		3.040244E-03
  validation loss:		6.116461E-04
Epoch took 13.712s

Epoch 47 of 100
  training loss:		2.821088E-03
  validation loss:		5.188566E-04
Epoch took 13.686s

Epoch 48 of 100
  training loss:		2.800432E-03
  validation loss:		4.315293E-04
Epoch took 13.732s

Epoch 49 of 100
  training loss:		2.837547E-03
  validation loss:		3.938742E-04
Epoch took 13.818s

Epoch 50 of 100
  training loss:		2.735633E-03
  validation loss:		5.207389E-04
Epoch took 13.710s

Epoch 51 of 100
  training loss:		2.677022E-03
  validation loss:		4.832968E-04
Epoch took 13.702s

Epoch 52 of 100
  training loss:		2.528590E-03
  validation loss:		3.784343E-04
Epoch took 13.722s

Epoch 53 of 100
  training loss:		2.498377E-03
  validation loss:		5.203468E-04
Epoch took 13.827s

Epoch 54 of 100
  training loss:		2.534082E-03
  validation loss:		6.849236E-04
Epoch took 13.718s

Epoch 55 of 100
  training loss:		2.431195E-03
  validation loss:		4.712901E-04
Epoch took 13.693s

Early stopping, val-loss increased over the last 5 epochs from 0.000495329029581 to 0.00050765829548
Training RMSE: 2.57512622161e-08
Validation RMSE: 2.56178470389e-08
