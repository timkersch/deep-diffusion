Training network with 56340 training samples and 18780 validation samples
Epoch 1 of 200
  training loss:		9.081851E-02
  validation loss:		4.846265E-02
Epoch took 1.115s

Epoch 2 of 200
  training loss:		4.604654E-02
  validation loss:		4.118719E-02
Epoch took 1.054s

Epoch 3 of 200
  training loss:		3.842022E-02
  validation loss:		3.763500E-02
Epoch took 1.053s

Epoch 4 of 200
  training loss:		3.480131E-02
  validation loss:		3.729920E-02
Epoch took 1.052s

Epoch 5 of 200
  training loss:		3.122426E-02
  validation loss:		3.202792E-02
Epoch took 1.053s

Epoch 6 of 200
  training loss:		3.051328E-02
  validation loss:		3.039530E-02
Epoch took 1.055s

Epoch 7 of 200
  training loss:		3.013160E-02
  validation loss:		2.889809E-02
Epoch took 1.054s

Epoch 8 of 200
  training loss:		2.863685E-02
  validation loss:		2.924809E-02
Epoch took 1.054s

Epoch 9 of 200
  training loss:		2.933089E-02
  validation loss:		2.924413E-02
Epoch took 1.053s

Epoch 10 of 200
  training loss:		2.898874E-02
  validation loss:		2.790975E-02
Epoch took 1.053s

Epoch 11 of 200
  training loss:		2.780764E-02
  validation loss:		2.880276E-02
Epoch took 1.054s

Epoch 12 of 200
  training loss:		2.835459E-02
  validation loss:		2.972959E-02
Epoch took 1.053s

Epoch 13 of 200
  training loss:		2.754614E-02
  validation loss:		2.966896E-02
Epoch took 1.053s

Epoch 14 of 200
  training loss:		2.732001E-02
  validation loss:		2.753316E-02
Epoch took 1.053s

Epoch 15 of 200
  training loss:		2.799014E-02
  validation loss:		3.025826E-02
Epoch took 1.052s

Epoch 16 of 200
  training loss:		2.792753E-02
  validation loss:		2.785112E-02
Epoch took 1.052s

Epoch 17 of 200
  training loss:		2.742871E-02
  validation loss:		2.799504E-02
Epoch took 1.053s

Epoch 18 of 200
  training loss:		2.719974E-02
  validation loss:		2.767209E-02
Epoch took 1.052s

Epoch 19 of 200
  training loss:		2.760188E-02
  validation loss:		2.761190E-02
Epoch took 1.052s

Epoch 20 of 200
  training loss:		2.698252E-02
  validation loss:		2.760071E-02
Epoch took 1.052s

Epoch 21 of 200
  training loss:		2.693243E-02
  validation loss:		2.850411E-02
Epoch took 1.052s

Epoch 22 of 200
  training loss:		2.750143E-02
  validation loss:		2.712611E-02
Epoch took 1.051s

Epoch 23 of 200
  training loss:		2.675457E-02
  validation loss:		2.689101E-02
Epoch took 1.053s

Epoch 24 of 200
  training loss:		2.628901E-02
  validation loss:		2.706663E-02
Epoch took 1.052s

Epoch 25 of 200
  training loss:		2.698408E-02
  validation loss:		2.697534E-02
Epoch took 1.052s

Epoch 26 of 200
  training loss:		2.665982E-02
  validation loss:		2.730213E-02
Epoch took 1.052s

Epoch 27 of 200
  training loss:		2.611029E-02
  validation loss:		2.702891E-02
Epoch took 1.052s

Epoch 28 of 200
  training loss:		2.756602E-02
  validation loss:		2.826286E-02
Epoch took 1.051s

Epoch 29 of 200
  training loss:		2.604100E-02
  validation loss:		2.650757E-02
Epoch took 1.051s

Epoch 30 of 200
  training loss:		2.589268E-02
  validation loss:		2.710041E-02
Epoch took 1.054s

Epoch 31 of 200
  training loss:		2.800071E-02
  validation loss:		2.834111E-02
Epoch took 1.050s

Epoch 32 of 200
  training loss:		2.603727E-02
  validation loss:		2.694878E-02
Epoch took 1.052s

Epoch 33 of 200
  training loss:		2.574762E-02
  validation loss:		2.651704E-02
Epoch took 1.054s

Epoch 34 of 200
  training loss:		2.581931E-02
  validation loss:		2.655729E-02
Epoch took 1.051s

Epoch 35 of 200
  training loss:		2.592721E-02
  validation loss:		2.648558E-02
Epoch took 1.051s

Epoch 36 of 200
  training loss:		2.652751E-02
  validation loss:		2.641543E-02
Epoch took 1.051s

Epoch 37 of 200
  training loss:		2.609595E-02
  validation loss:		2.642340E-02
Epoch took 1.052s

Epoch 38 of 200
  training loss:		2.567265E-02
  validation loss:		2.640479E-02
Epoch took 1.053s

Epoch 39 of 200
  training loss:		2.566002E-02
  validation loss:		2.651376E-02
Epoch took 1.052s

Epoch 40 of 200
  training loss:		2.738656E-02
  validation loss:		2.667144E-02
Epoch took 1.051s

Epoch 41 of 200
  training loss:		2.561092E-02
  validation loss:		2.644769E-02
Epoch took 1.050s

Epoch 42 of 200
  training loss:		2.555150E-02
  validation loss:		2.654643E-02
Epoch took 1.054s

Epoch 43 of 200
  training loss:		2.554624E-02
  validation loss:		2.630566E-02
Epoch took 1.050s

Epoch 44 of 200
  training loss:		2.680655E-02
  validation loss:		2.649408E-02
Epoch took 1.050s

Epoch 45 of 200
  training loss:		2.591017E-02
  validation loss:		2.647771E-02
Epoch took 1.052s

Epoch 46 of 200
  training loss:		2.553498E-02
  validation loss:		2.649376E-02
Epoch took 1.053s

Epoch 47 of 200
  training loss:		2.554489E-02
  validation loss:		2.618149E-02
Epoch took 1.052s

Epoch 48 of 200
  training loss:		2.546871E-02
  validation loss:		2.627614E-02
Epoch took 1.051s

Epoch 49 of 200
  training loss:		2.681814E-02
  validation loss:		2.969187E-02
Epoch took 1.051s

Epoch 50 of 200
  training loss:		2.602768E-02
  validation loss:		2.636111E-02
Epoch took 1.049s

Epoch 51 of 200
  training loss:		2.548355E-02
  validation loss:		2.624938E-02
Epoch took 1.056s

Epoch 52 of 200
  training loss:		2.548874E-02
  validation loss:		2.632040E-02
Epoch took 1.051s

Epoch 53 of 200
  training loss:		2.550933E-02
  validation loss:		2.652909E-02
Epoch took 1.051s

Epoch 54 of 200
  training loss:		2.549335E-02
  validation loss:		2.634533E-02
Epoch took 1.051s

Epoch 55 of 200
  training loss:		2.883778E-02
  validation loss:		2.789965E-02
Epoch took 1.049s

Epoch 56 of 200
  training loss:		2.556234E-02
  validation loss:		2.617666E-02
Epoch took 1.052s

Epoch 57 of 200
  training loss:		2.543948E-02
  validation loss:		2.613532E-02
Epoch took 1.056s

Epoch 58 of 200
  training loss:		2.551143E-02
  validation loss:		2.628714E-02
Epoch took 1.050s

Epoch 59 of 200
  training loss:		2.620119E-02
  validation loss:		2.635482E-02
Epoch took 1.049s

Epoch 60 of 200
  training loss:		2.540192E-02
  validation loss:		2.627984E-02
Epoch took 1.055s

Epoch 61 of 200
  training loss:		2.538782E-02
  validation loss:		2.625286E-02
Epoch took 1.053s

Epoch 62 of 200
  training loss:		2.536827E-02
  validation loss:		2.653523E-02
Epoch took 1.051s

Epoch 63 of 200
  training loss:		2.702007E-02
  validation loss:		2.626600E-02
Epoch took 1.050s

Epoch 64 of 200
  training loss:		2.541668E-02
  validation loss:		2.626166E-02
Epoch took 1.051s

Epoch 65 of 200
  training loss:		2.539280E-02
  validation loss:		2.616906E-02
Epoch took 1.055s

Epoch 66 of 200
  training loss:		2.608823E-02
  validation loss:		3.394623E-02
Epoch took 1.050s

Epoch 67 of 200
  training loss:		2.635278E-02
  validation loss:		2.611755E-02
Epoch took 1.049s

Epoch 68 of 200
  training loss:		2.538345E-02
  validation loss:		2.623287E-02
Epoch took 1.058s

Epoch 69 of 200
  training loss:		2.613166E-02
  validation loss:		2.644785E-02
Epoch took 1.052s

Epoch 70 of 200
  training loss:		2.549366E-02
  validation loss:		2.620283E-02
Epoch took 1.051s

Epoch 71 of 200
  training loss:		2.537392E-02
  validation loss:		2.626533E-02
Epoch took 1.058s

Epoch 72 of 200
  training loss:		2.535512E-02
  validation loss:		2.621778E-02
Epoch took 1.052s

Epoch 73 of 200
  training loss:		2.540915E-02
  validation loss:		2.616064E-02
Epoch took 1.052s

Epoch 74 of 200
  training loss:		2.538252E-02
  validation loss:		2.616446E-02
Epoch took 1.052s

Epoch 75 of 200
  training loss:		2.681613E-02
  validation loss:		2.762674E-02
Epoch took 1.052s

Epoch 76 of 200
  training loss:		2.545530E-02
  validation loss:		2.628032E-02
Epoch took 1.052s

Epoch 77 of 200
  training loss:		2.688650E-02
  validation loss:		2.703686E-02
Epoch took 1.056s

Epoch 78 of 200
  training loss:		2.548210E-02
  validation loss:		2.627790E-02
Epoch took 1.052s

Epoch 79 of 200
  training loss:		2.534709E-02
  validation loss:		2.620888E-02
Epoch took 1.059s

Epoch 80 of 200
  training loss:		2.827173E-02
  validation loss:		2.850381E-02
Epoch took 1.051s

Early stopping, val-loss increased over the last 20 epochs from 0.0265926787553 to 0.0268587422379
Saving model from epoch 60
Training MSE: 2.43106e-14
Validation MSE: 2.5166e-14
Training R2: 0.741673375034
Validation R2: 0.731964845482
