Training network with 75120 training samples and 14085 validation samples
Epoch 1 of 200
  training loss:		1.109519E-01
  validation loss:		5.827689E-02
Epoch took 9.132s

Epoch 2 of 200
  training loss:		5.070985E-02
  validation loss:		4.562929E-02
Epoch took 8.658s

Epoch 3 of 200
  training loss:		4.197086E-02
  validation loss:		3.985743E-02
Epoch took 8.579s

Epoch 4 of 200
  training loss:		3.791900E-02
  validation loss:		3.674761E-02
Epoch took 9.286s

Epoch 5 of 200
  training loss:		3.561044E-02
  validation loss:		3.495788E-02
Epoch took 8.823s

Epoch 6 of 200
  training loss:		3.438379E-02
  validation loss:		3.366044E-02
Epoch took 9.247s

Epoch 7 of 200
  training loss:		3.328684E-02
  validation loss:		3.229206E-02
Epoch took 8.166s

Epoch 8 of 200
  training loss:		3.236001E-02
  validation loss:		3.272450E-02
Epoch took 7.531s

Epoch 9 of 200
  training loss:		3.154522E-02
  validation loss:		3.231261E-02
Epoch took 9.024s

Epoch 10 of 200
  training loss:		3.146083E-02
  validation loss:		3.212095E-02
Epoch took 8.612s

Epoch 11 of 200
  training loss:		3.080465E-02
  validation loss:		3.794001E-02
Epoch took 9.250s

Epoch 12 of 200
  training loss:		3.026479E-02
  validation loss:		2.936853E-02
Epoch took 7.876s

Epoch 13 of 200
  training loss:		3.005776E-02
  validation loss:		3.078995E-02
Epoch took 9.233s

Epoch 14 of 200
  training loss:		3.005265E-02
  validation loss:		2.958826E-02
Epoch took 9.319s

Epoch 15 of 200
  training loss:		2.971299E-02
  validation loss:		3.015325E-02
Epoch took 8.922s

Epoch 16 of 200
  training loss:		2.902194E-02
  validation loss:		3.135990E-02
Epoch took 9.280s

Epoch 17 of 200
  training loss:		2.927349E-02
  validation loss:		2.910945E-02
Epoch took 8.250s

Epoch 18 of 200
  training loss:		2.893545E-02
  validation loss:		2.851985E-02
Epoch took 8.622s

Epoch 19 of 200
  training loss:		2.883085E-02
  validation loss:		2.841646E-02
Epoch took 9.255s

Epoch 20 of 200
  training loss:		2.860871E-02
  validation loss:		2.793705E-02
Epoch took 8.944s

Epoch 21 of 200
  training loss:		2.807983E-02
  validation loss:		2.766269E-02
Epoch took 8.767s

Epoch 22 of 200
  training loss:		2.826128E-02
  validation loss:		2.911124E-02
Epoch took 8.598s

Epoch 23 of 200
  training loss:		2.845418E-02
  validation loss:		2.867708E-02
Epoch took 9.410s

Epoch 24 of 200
  training loss:		2.826981E-02
  validation loss:		2.730988E-02
Epoch took 9.068s

Epoch 25 of 200
  training loss:		2.794039E-02
  validation loss:		2.875634E-02
Epoch took 8.062s

Epoch 26 of 200
  training loss:		2.803278E-02
  validation loss:		2.964654E-02
Epoch took 8.614s

Epoch 27 of 200
  training loss:		2.768425E-02
  validation loss:		2.814587E-02
Epoch took 8.658s

Epoch 28 of 200
  training loss:		2.771087E-02
  validation loss:		2.778490E-02
Epoch took 8.918s

Epoch 29 of 200
  training loss:		2.755060E-02
  validation loss:		2.757311E-02
Epoch took 9.804s

Epoch 30 of 200
  training loss:		2.766457E-02
  validation loss:		2.921735E-02
Epoch took 8.679s

Epoch 31 of 200
  training loss:		2.771227E-02
  validation loss:		2.758604E-02
Epoch took 7.987s

Epoch 32 of 200
  training loss:		2.739357E-02
  validation loss:		2.662941E-02
Epoch took 9.230s

Epoch 33 of 200
  training loss:		2.734429E-02
  validation loss:		2.735210E-02
Epoch took 9.480s

Epoch 34 of 200
  training loss:		2.734291E-02
  validation loss:		2.710862E-02
Epoch took 8.503s

Epoch 35 of 200
  training loss:		2.715814E-02
  validation loss:		2.807010E-02
Epoch took 8.459s

Epoch 36 of 200
  training loss:		2.729135E-02
  validation loss:		2.830286E-02
Epoch took 9.243s

Epoch 37 of 200
  training loss:		2.726175E-02
  validation loss:		2.692793E-02
Epoch took 8.802s

Epoch 38 of 200
  training loss:		2.737674E-02
  validation loss:		2.707539E-02
Epoch took 8.869s

Epoch 39 of 200
  training loss:		2.714854E-02
  validation loss:		2.701113E-02
Epoch took 8.658s

Epoch 40 of 200
  training loss:		2.701328E-02
  validation loss:		2.689870E-02
Epoch took 8.422s

Epoch 41 of 200
  training loss:		2.760197E-02
  validation loss:		2.707880E-02
Epoch took 7.978s

Epoch 42 of 200
  training loss:		2.697940E-02
  validation loss:		2.792809E-02
Epoch took 8.211s

Epoch 43 of 200
  training loss:		2.723720E-02
  validation loss:		2.698903E-02
Epoch took 8.429s

Epoch 44 of 200
  training loss:		2.723812E-02
  validation loss:		2.808860E-02
Epoch took 8.290s

Epoch 45 of 200
  training loss:		2.739275E-02
  validation loss:		2.771358E-02
Epoch took 8.510s

Epoch 46 of 200
  training loss:		2.733610E-02
  validation loss:		2.767434E-02
Epoch took 9.380s

Epoch 47 of 200
  training loss:		2.703475E-02
  validation loss:		2.699569E-02
Epoch took 8.366s

Epoch 48 of 200
  training loss:		2.679931E-02
  validation loss:		2.696985E-02
Epoch took 8.752s

Epoch 49 of 200
  training loss:		2.720285E-02
  validation loss:		2.781162E-02
Epoch took 9.180s

Epoch 50 of 200
  training loss:		2.697266E-02
  validation loss:		2.649236E-02
Epoch took 9.579s

Early stopping, val-loss increased over the last 10 epochs from 0.0272962283308 to 0.0273741955361
Training RMSE: 1.59906852502e-07
Validation RMSE: 1.60705408688e-07
Test RMSE: 1.61396725082e-07
Test MSE: 2.60489028673e-14
Test MAE: 9.53163699005e-08
Test R2: 0.727245126871 

