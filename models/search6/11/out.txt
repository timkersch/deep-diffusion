Training network with 75120 training samples and 14085 validation samples
Epoch 1 of 200
  training loss:		1.783548E-01
  validation loss:		8.978884E-02
Epoch took 8.042s

Epoch 2 of 200
  training loss:		8.096842E-02
  validation loss:		7.300448E-02
Epoch took 7.925s

Epoch 3 of 200
  training loss:		6.874012E-02
  validation loss:		6.420254E-02
Epoch took 8.666s

Epoch 4 of 200
  training loss:		6.145048E-02
  validation loss:		5.824100E-02
Epoch took 7.209s

Epoch 5 of 200
  training loss:		5.626170E-02
  validation loss:		5.427906E-02
Epoch took 6.981s

Epoch 6 of 200
  training loss:		5.232147E-02
  validation loss:		5.061794E-02
Epoch took 7.250s

Epoch 7 of 200
  training loss:		4.910198E-02
  validation loss:		4.927389E-02
Epoch took 7.462s

Epoch 8 of 200
  training loss:		4.657671E-02
  validation loss:		4.721619E-02
Epoch took 8.727s

Epoch 9 of 200
  training loss:		4.426081E-02
  validation loss:		4.329166E-02
Epoch took 7.448s

Epoch 10 of 200
  training loss:		4.231568E-02
  validation loss:		4.231243E-02
Epoch took 7.830s

Epoch 11 of 200
  training loss:		4.073944E-02
  validation loss:		4.006962E-02
Epoch took 8.340s

Epoch 12 of 200
  training loss:		3.920451E-02
  validation loss:		3.908869E-02
Epoch took 8.988s

Epoch 13 of 200
  training loss:		3.823369E-02
  validation loss:		3.788491E-02
Epoch took 7.779s

Epoch 14 of 200
  training loss:		3.733247E-02
  validation loss:		3.678600E-02
Epoch took 8.408s

Epoch 15 of 200
  training loss:		3.619510E-02
  validation loss:		3.586986E-02
Epoch took 8.838s

Epoch 16 of 200
  training loss:		3.546139E-02
  validation loss:		3.535363E-02
Epoch took 9.282s

Epoch 17 of 200
  training loss:		3.470035E-02
  validation loss:		3.506117E-02
Epoch took 7.829s

Epoch 18 of 200
  training loss:		3.417505E-02
  validation loss:		3.432988E-02
Epoch took 9.619s

Epoch 19 of 200
  training loss:		3.367459E-02
  validation loss:		3.392011E-02
Epoch took 9.090s

Epoch 20 of 200
  training loss:		3.306993E-02
  validation loss:		3.406518E-02
Epoch took 8.560s

Epoch 21 of 200
  training loss:		3.270566E-02
  validation loss:		3.256902E-02
Epoch took 8.198s

Epoch 22 of 200
  training loss:		3.238266E-02
  validation loss:		3.251623E-02
Epoch took 8.593s

Epoch 23 of 200
  training loss:		3.185436E-02
  validation loss:		3.171672E-02
Epoch took 9.233s

Epoch 24 of 200
  training loss:		3.160710E-02
  validation loss:		3.121818E-02
Epoch took 8.362s

Epoch 25 of 200
  training loss:		3.123684E-02
  validation loss:		3.137223E-02
Epoch took 8.892s

Epoch 26 of 200
  training loss:		3.122893E-02
  validation loss:		3.111847E-02
Epoch took 9.968s

Epoch 27 of 200
  training loss:		3.074836E-02
  validation loss:		3.221333E-02
Epoch took 9.317s

Epoch 28 of 200
  training loss:		3.054331E-02
  validation loss:		3.043687E-02
Epoch took 9.065s

Epoch 29 of 200
  training loss:		3.021338E-02
  validation loss:		3.047626E-02
Epoch took 8.723s

Epoch 30 of 200
  training loss:		3.007476E-02
  validation loss:		3.027818E-02
Epoch took 8.933s

Epoch 31 of 200
  training loss:		2.977979E-02
  validation loss:		2.968984E-02
Epoch took 9.009s

Epoch 32 of 200
  training loss:		2.962558E-02
  validation loss:		2.956589E-02
Epoch took 7.876s

Epoch 33 of 200
  training loss:		2.941853E-02
  validation loss:		2.969760E-02
Epoch took 7.695s

Epoch 34 of 200
  training loss:		2.953145E-02
  validation loss:		2.971530E-02
Epoch took 8.370s

Epoch 35 of 200
  training loss:		2.901884E-02
  validation loss:		2.938259E-02
Epoch took 9.646s

Epoch 36 of 200
  training loss:		2.901062E-02
  validation loss:		2.889669E-02
Epoch took 9.016s

Epoch 37 of 200
  training loss:		2.878110E-02
  validation loss:		2.955088E-02
Epoch took 8.744s

Epoch 38 of 200
  training loss:		2.863583E-02
  validation loss:		2.909200E-02
Epoch took 8.575s

Epoch 39 of 200
  training loss:		2.861170E-02
  validation loss:		2.896368E-02
Epoch took 7.667s

Epoch 40 of 200
  training loss:		2.847959E-02
  validation loss:		2.826847E-02
Epoch took 7.368s

Epoch 41 of 200
  training loss:		2.833635E-02
  validation loss:		2.909326E-02
Epoch took 8.292s

Epoch 42 of 200
  training loss:		2.828397E-02
  validation loss:		2.797023E-02
Epoch took 8.297s

Epoch 43 of 200
  training loss:		2.807525E-02
  validation loss:		2.967645E-02
Epoch took 7.886s

Epoch 44 of 200
  training loss:		2.796642E-02
  validation loss:		2.828148E-02
Epoch took 8.595s

Epoch 45 of 200
  training loss:		2.790972E-02
  validation loss:		2.792616E-02
Epoch took 8.409s

Epoch 46 of 200
  training loss:		2.799891E-02
  validation loss:		2.768191E-02
Epoch took 9.224s

Epoch 47 of 200
  training loss:		2.769712E-02
  validation loss:		2.770677E-02
Epoch took 7.495s

Epoch 48 of 200
  training loss:		2.777071E-02
  validation loss:		2.754323E-02
Epoch took 6.825s

Epoch 49 of 200
  training loss:		2.767750E-02
  validation loss:		2.731646E-02
Epoch took 6.938s

Epoch 50 of 200
  training loss:		2.773661E-02
  validation loss:		2.769565E-02
Epoch took 6.839s

Epoch 51 of 200
  training loss:		2.746392E-02
  validation loss:		2.793486E-02
Epoch took 6.836s

Epoch 52 of 200
  training loss:		2.752861E-02
  validation loss:		2.762513E-02
Epoch took 6.840s

Epoch 53 of 200
  training loss:		2.726814E-02
  validation loss:		2.736540E-02
Epoch took 6.833s

Epoch 54 of 200
  training loss:		2.734731E-02
  validation loss:		2.767372E-02
Epoch took 6.846s

Epoch 55 of 200
  training loss:		2.735652E-02
  validation loss:		2.770430E-02
Epoch took 6.898s

Epoch 56 of 200
  training loss:		2.714577E-02
  validation loss:		2.725530E-02
Epoch took 6.897s

Epoch 57 of 200
  training loss:		2.719879E-02
  validation loss:		2.683883E-02
Epoch took 6.850s

Epoch 58 of 200
  training loss:		2.704755E-02
  validation loss:		2.702190E-02
Epoch took 6.858s

Epoch 59 of 200
  training loss:		2.718432E-02
  validation loss:		2.699334E-02
Epoch took 9.233s

Epoch 60 of 200
  training loss:		2.710290E-02
  validation loss:		2.671343E-02
Epoch took 8.512s

Epoch 61 of 200
  training loss:		2.699360E-02
  validation loss:		2.756048E-02
Epoch took 8.847s

Epoch 62 of 200
  training loss:		2.693758E-02
  validation loss:		2.718945E-02
Epoch took 8.089s

Epoch 63 of 200
  training loss:		2.687929E-02
  validation loss:		2.661576E-02
Epoch took 8.451s

Epoch 64 of 200
  training loss:		2.698274E-02
  validation loss:		2.665886E-02
Epoch took 8.740s

Epoch 65 of 200
  training loss:		2.686623E-02
  validation loss:		2.662276E-02
Epoch took 8.650s

Epoch 66 of 200
  training loss:		2.686390E-02
  validation loss:		2.687009E-02
Epoch took 8.159s

Epoch 67 of 200
  training loss:		2.691364E-02
  validation loss:		2.705521E-02
Epoch took 8.712s

Epoch 68 of 200
  training loss:		2.674672E-02
  validation loss:		2.686696E-02
Epoch took 7.546s

Epoch 69 of 200
  training loss:		2.690962E-02
  validation loss:		2.720505E-02
Epoch took 8.436s

Epoch 70 of 200
  training loss:		2.674106E-02
  validation loss:		2.679076E-02
Epoch took 8.144s

Epoch 71 of 200
  training loss:		2.673305E-02
  validation loss:		2.665325E-02
Epoch took 9.596s

Epoch 72 of 200
  training loss:		2.662941E-02
  validation loss:		2.658304E-02
Epoch took 8.268s

Epoch 73 of 200
  training loss:		2.669961E-02
  validation loss:		2.637543E-02
Epoch took 10.229s

Epoch 74 of 200
  training loss:		2.677387E-02
  validation loss:		2.661887E-02
Epoch took 8.370s

Epoch 75 of 200
  training loss:		2.659030E-02
  validation loss:		2.678488E-02
Epoch took 9.811s

Epoch 76 of 200
  training loss:		2.660465E-02
  validation loss:		2.700516E-02
Epoch took 8.516s

Epoch 77 of 200
  training loss:		2.658233E-02
  validation loss:		2.633250E-02
Epoch took 9.235s

Epoch 78 of 200
  training loss:		2.659897E-02
  validation loss:		2.637031E-02
Epoch took 8.741s

Epoch 79 of 200
  training loss:		2.661878E-02
  validation loss:		2.855034E-02
Epoch took 7.796s

Epoch 80 of 200
  training loss:		2.655719E-02
  validation loss:		2.646090E-02
Epoch took 8.375s

Epoch 81 of 200
  training loss:		2.652597E-02
  validation loss:		2.727636E-02
Epoch took 8.291s

Epoch 82 of 200
  training loss:		2.641995E-02
  validation loss:		2.673756E-02
Epoch took 9.274s

Epoch 83 of 200
  training loss:		2.654969E-02
  validation loss:		2.618048E-02
Epoch took 8.414s

Epoch 84 of 200
  training loss:		2.657960E-02
  validation loss:		2.762003E-02
Epoch took 9.116s

Epoch 85 of 200
  training loss:		2.649552E-02
  validation loss:		2.634272E-02
Epoch took 8.666s

Epoch 86 of 200
  training loss:		2.645304E-02
  validation loss:		2.615872E-02
Epoch took 7.439s

Epoch 87 of 200
  training loss:		2.656310E-02
  validation loss:		2.651470E-02
Epoch took 6.887s

Epoch 88 of 200
  training loss:		2.652945E-02
  validation loss:		2.643840E-02
Epoch took 6.845s

Epoch 89 of 200
  training loss:		2.639931E-02
  validation loss:		2.636911E-02
Epoch took 7.490s

Epoch 90 of 200
  training loss:		2.643994E-02
  validation loss:		2.626359E-02
Epoch took 9.284s

Epoch 91 of 200
  training loss:		2.633621E-02
  validation loss:		2.635876E-02
Epoch took 8.575s

Epoch 92 of 200
  training loss:		2.644473E-02
  validation loss:		2.646360E-02
Epoch took 8.955s

Epoch 93 of 200
  training loss:		2.635385E-02
  validation loss:		2.611478E-02
Epoch took 9.157s

Epoch 94 of 200
  training loss:		2.646972E-02
  validation loss:		2.651113E-02
Epoch took 7.972s

Epoch 95 of 200
  training loss:		2.636652E-02
  validation loss:		2.635002E-02
Epoch took 9.461s

Epoch 96 of 200
  training loss:		2.628191E-02
  validation loss:		2.628238E-02
Epoch took 8.901s

Epoch 97 of 200
  training loss:		2.644975E-02
  validation loss:		2.606240E-02
Epoch took 8.782s

Epoch 98 of 200
  training loss:		2.638913E-02
  validation loss:		2.618060E-02
Epoch took 9.067s

Epoch 99 of 200
  training loss:		2.637874E-02
  validation loss:		2.682040E-02
Epoch took 8.125s

Epoch 100 of 200
  training loss:		2.634282E-02
  validation loss:		2.666556E-02
Epoch took 9.100s

Epoch 101 of 200
  training loss:		2.636726E-02
  validation loss:		2.639540E-02
Epoch took 8.590s

Epoch 102 of 200
  training loss:		2.635372E-02
  validation loss:		2.628218E-02
Epoch took 9.023s

Epoch 103 of 200
  training loss:		2.623752E-02
  validation loss:		2.693018E-02
Epoch took 8.279s

Epoch 104 of 200
  training loss:		2.632281E-02
  validation loss:		2.608953E-02
Epoch took 8.226s

Epoch 105 of 200
  training loss:		2.627193E-02
  validation loss:		2.643420E-02
Epoch took 9.834s

Epoch 106 of 200
  training loss:		2.632028E-02
  validation loss:		2.603971E-02
Epoch took 8.500s

Epoch 107 of 200
  training loss:		2.623654E-02
  validation loss:		2.698069E-02
Epoch took 8.520s

Epoch 108 of 200
  training loss:		2.653151E-02
  validation loss:		2.640870E-02
Epoch took 8.751s

Epoch 109 of 200
  training loss:		2.624610E-02
  validation loss:		2.647944E-02
Epoch took 8.255s

Epoch 110 of 200
  training loss:		2.633236E-02
  validation loss:		2.610293E-02
Epoch took 8.705s

Early stopping, val-loss increased over the last 10 epochs from 0.026380963487 to 0.0264142955212
Training RMSE: 1.59878246565e-07
Validation RMSE: 1.60038961637e-07
Test RMSE: 1.60396856663e-07
Test MSE: 2.57271516272e-14
Test MAE: 9.63079171842e-08
Test R2: 0.730614144718 

