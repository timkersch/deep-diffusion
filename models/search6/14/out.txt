Training network with 75120 training samples and 14085 validation samples
Epoch 1 of 200
  training loss:		9.905127E-02
  validation loss:		4.144158E-02
Epoch took 9.296s

Epoch 2 of 200
  training loss:		3.854362E-02
  validation loss:		3.539719E-02
Epoch took 8.579s

Epoch 3 of 200
  training loss:		3.651298E-02
  validation loss:		3.804294E-02
Epoch took 8.689s

Epoch 4 of 200
  training loss:		3.421213E-02
  validation loss:		3.349986E-02
Epoch took 9.587s

Epoch 5 of 200
  training loss:		3.257509E-02
  validation loss:		3.240616E-02
Epoch took 7.918s

Epoch 6 of 200
  training loss:		3.159093E-02
  validation loss:		3.032553E-02
Epoch took 8.489s

Epoch 7 of 200
  training loss:		3.165568E-02
  validation loss:		3.043664E-02
Epoch took 9.241s

Epoch 8 of 200
  training loss:		3.029912E-02
  validation loss:		2.992299E-02
Epoch took 8.906s

Epoch 9 of 200
  training loss:		2.923286E-02
  validation loss:		2.946917E-02
Epoch took 8.510s

Epoch 10 of 200
  training loss:		3.009980E-02
  validation loss:		2.853969E-02
Epoch took 9.050s

Epoch 11 of 200
  training loss:		2.983885E-02
  validation loss:		3.017708E-02
Epoch took 8.907s

Epoch 12 of 200
  training loss:		3.042709E-02
  validation loss:		2.939779E-02
Epoch took 9.183s

Epoch 13 of 200
  training loss:		2.920162E-02
  validation loss:		2.891690E-02
Epoch took 8.935s

Epoch 14 of 200
  training loss:		2.983142E-02
  validation loss:		3.132289E-02
Epoch took 9.574s

Epoch 15 of 200
  training loss:		2.868734E-02
  validation loss:		2.775536E-02
Epoch took 7.936s

Epoch 16 of 200
  training loss:		2.868660E-02
  validation loss:		2.982881E-02
Epoch took 8.929s

Epoch 17 of 200
  training loss:		2.868132E-02
  validation loss:		2.828102E-02
Epoch took 7.723s

Epoch 18 of 200
  training loss:		2.811735E-02
  validation loss:		2.865437E-02
Epoch took 8.207s

Epoch 19 of 200
  training loss:		2.812881E-02
  validation loss:		2.911316E-02
Epoch took 8.067s

Epoch 20 of 200
  training loss:		2.851675E-02
  validation loss:		3.401553E-02
Epoch took 9.341s

Epoch 21 of 200
  training loss:		2.878777E-02
  validation loss:		2.727966E-02
Epoch took 8.570s

Epoch 22 of 200
  training loss:		2.753722E-02
  validation loss:		2.795903E-02
Epoch took 9.228s

Epoch 23 of 200
  training loss:		2.820866E-02
  validation loss:		2.750500E-02
Epoch took 8.674s

Epoch 24 of 200
  training loss:		2.772288E-02
  validation loss:		2.735488E-02
Epoch took 8.779s

Epoch 25 of 200
  training loss:		2.800974E-02
  validation loss:		2.778107E-02
Epoch took 8.267s

Epoch 26 of 200
  training loss:		2.757205E-02
  validation loss:		2.894905E-02
Epoch took 8.381s

Epoch 27 of 200
  training loss:		2.735030E-02
  validation loss:		2.718849E-02
Epoch took 8.707s

Epoch 28 of 200
  training loss:		2.742493E-02
  validation loss:		2.794596E-02
Epoch took 10.146s

Epoch 29 of 200
  training loss:		2.734922E-02
  validation loss:		2.824689E-02
Epoch took 7.681s

Epoch 30 of 200
  training loss:		2.746434E-02
  validation loss:		2.731715E-02
Epoch took 8.095s

Epoch 31 of 200
  training loss:		2.742671E-02
  validation loss:		2.889851E-02
Epoch took 8.881s

Epoch 32 of 200
  training loss:		2.792252E-02
  validation loss:		2.704384E-02
Epoch took 8.607s

Epoch 33 of 200
  training loss:		2.704251E-02
  validation loss:		2.647913E-02
Epoch took 8.710s

Epoch 34 of 200
  training loss:		2.777832E-02
  validation loss:		2.650181E-02
Epoch took 8.453s

Epoch 35 of 200
  training loss:		2.683409E-02
  validation loss:		2.704904E-02
Epoch took 8.481s

Epoch 36 of 200
  training loss:		2.683865E-02
  validation loss:		2.663691E-02
Epoch took 7.111s

Epoch 37 of 200
  training loss:		2.696678E-02
  validation loss:		2.694720E-02
Epoch took 7.480s

Epoch 38 of 200
  training loss:		2.667154E-02
  validation loss:		2.657020E-02
Epoch took 8.039s

Epoch 39 of 200
  training loss:		2.686015E-02
  validation loss:		2.662588E-02
Epoch took 9.474s

Epoch 40 of 200
  training loss:		2.700789E-02
  validation loss:		2.640822E-02
Epoch took 8.780s

Epoch 41 of 200
  training loss:		2.675742E-02
  validation loss:		2.673426E-02
Epoch took 9.199s

Epoch 42 of 200
  training loss:		2.692121E-02
  validation loss:		2.646156E-02
Epoch took 9.333s

Epoch 43 of 200
  training loss:		2.657784E-02
  validation loss:		2.855003E-02
Epoch took 9.329s

Epoch 44 of 200
  training loss:		2.711737E-02
  validation loss:		2.669349E-02
Epoch took 8.205s

Epoch 45 of 200
  training loss:		2.671808E-02
  validation loss:		2.675539E-02
Epoch took 9.388s

Epoch 46 of 200
  training loss:		2.642906E-02
  validation loss:		2.614550E-02
Epoch took 7.589s

Epoch 47 of 200
  training loss:		2.642100E-02
  validation loss:		2.717088E-02
Epoch took 8.955s

Epoch 48 of 200
  training loss:		2.675124E-02
  validation loss:		2.634178E-02
Epoch took 8.856s

Epoch 49 of 200
  training loss:		2.662920E-02
  validation loss:		2.654683E-02
Epoch took 8.243s

Epoch 50 of 200
  training loss:		2.628350E-02
  validation loss:		2.624257E-02
Epoch took 8.050s

Epoch 51 of 200
  training loss:		2.641492E-02
  validation loss:		3.433602E-02
Epoch took 8.734s

Epoch 52 of 200
  training loss:		2.681781E-02
  validation loss:		2.709548E-02
Epoch took 8.100s

Epoch 53 of 200
  training loss:		2.634441E-02
  validation loss:		2.619211E-02
Epoch took 9.446s

Epoch 54 of 200
  training loss:		2.630069E-02
  validation loss:		2.777765E-02
Epoch took 8.009s

Epoch 55 of 200
  training loss:		2.685460E-02
  validation loss:		2.599749E-02
Epoch took 8.835s

Epoch 56 of 200
  training loss:		2.626052E-02
  validation loss:		2.631966E-02
Epoch took 8.153s

Epoch 57 of 200
  training loss:		2.653348E-02
  validation loss:		2.611061E-02
Epoch took 8.810s

Epoch 58 of 200
  training loss:		2.606407E-02
  validation loss:		2.602944E-02
Epoch took 8.701s

Epoch 59 of 200
  training loss:		2.605239E-02
  validation loss:		2.612162E-02
Epoch took 8.833s

Epoch 60 of 200
  training loss:		2.612135E-02
  validation loss:		2.630223E-02
Epoch took 8.012s

Early stopping, val-loss increased over the last 10 epochs from 0.0267642283018 to 0.0272282306637
Training RMSE: 1.58394175703e-07
Validation RMSE: 1.58745094119e-07
Test RMSE: 1.59301642609e-07
Test MSE: 2.5377013338e-14
Test MAE: 8.91542987926e-08
Test R2: 0.734280399883 

