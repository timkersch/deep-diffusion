Training network with 75120 training samples and 14085 validation samples
Epoch 1 of 200
  training loss:		2.401193E-01
  validation loss:		8.025502E-02
Epoch took 7.510s

Epoch 2 of 200
  training loss:		7.024292E-02
  validation loss:		6.413162E-02
Epoch took 8.582s

Epoch 3 of 200
  training loss:		5.815948E-02
  validation loss:		5.440275E-02
Epoch took 7.874s

Epoch 4 of 200
  training loss:		5.034511E-02
  validation loss:		4.785734E-02
Epoch took 6.854s

Epoch 5 of 200
  training loss:		4.476907E-02
  validation loss:		4.314322E-02
Epoch took 8.008s

Epoch 6 of 200
  training loss:		4.096797E-02
  validation loss:		3.969924E-02
Epoch took 8.162s

Epoch 7 of 200
  training loss:		3.823475E-02
  validation loss:		3.748211E-02
Epoch took 8.434s

Epoch 8 of 200
  training loss:		3.622626E-02
  validation loss:		3.542256E-02
Epoch took 8.503s

Epoch 9 of 200
  training loss:		3.455997E-02
  validation loss:		3.426995E-02
Epoch took 8.997s

Epoch 10 of 200
  training loss:		3.327329E-02
  validation loss:		3.303541E-02
Epoch took 8.123s

Epoch 11 of 200
  training loss:		3.243914E-02
  validation loss:		3.190271E-02
Epoch took 7.991s

Epoch 12 of 200
  training loss:		3.155295E-02
  validation loss:		3.110865E-02
Epoch took 8.255s

Epoch 13 of 200
  training loss:		3.077805E-02
  validation loss:		3.060248E-02
Epoch took 7.880s

Epoch 14 of 200
  training loss:		3.046786E-02
  validation loss:		3.229019E-02
Epoch took 8.421s

Epoch 15 of 200
  training loss:		2.998569E-02
  validation loss:		2.984052E-02
Epoch took 7.389s

Epoch 16 of 200
  training loss:		2.955627E-02
  validation loss:		2.998904E-02
Epoch took 7.492s

Epoch 17 of 200
  training loss:		2.945955E-02
  validation loss:		3.034508E-02
Epoch took 8.461s

Epoch 18 of 200
  training loss:		2.900003E-02
  validation loss:		2.935053E-02
Epoch took 8.095s

Epoch 19 of 200
  training loss:		2.874392E-02
  validation loss:		2.909629E-02
Epoch took 8.028s

Epoch 20 of 200
  training loss:		2.872543E-02
  validation loss:		2.865981E-02
Epoch took 8.406s

Epoch 21 of 200
  training loss:		2.850262E-02
  validation loss:		2.868500E-02
Epoch took 8.280s

Epoch 22 of 200
  training loss:		2.803909E-02
  validation loss:		2.841793E-02
Epoch took 7.060s

Epoch 23 of 200
  training loss:		2.833347E-02
  validation loss:		2.823019E-02
Epoch took 8.183s

Epoch 24 of 200
  training loss:		2.812278E-02
  validation loss:		2.800000E-02
Epoch took 7.490s

Epoch 25 of 200
  training loss:		2.821399E-02
  validation loss:		2.806209E-02
Epoch took 9.283s

Epoch 26 of 200
  training loss:		2.782622E-02
  validation loss:		2.769909E-02
Epoch took 7.881s

Epoch 27 of 200
  training loss:		2.789528E-02
  validation loss:		2.803558E-02
Epoch took 7.869s

Epoch 28 of 200
  training loss:		2.797080E-02
  validation loss:		2.912201E-02
Epoch took 7.669s

Epoch 29 of 200
  training loss:		2.763428E-02
  validation loss:		2.754184E-02
Epoch took 8.743s

Epoch 30 of 200
  training loss:		2.761617E-02
  validation loss:		2.816038E-02
Epoch took 8.850s

Epoch 31 of 200
  training loss:		2.733652E-02
  validation loss:		2.744494E-02
Epoch took 7.867s

Epoch 32 of 200
  training loss:		2.774414E-02
  validation loss:		3.071662E-02
Epoch took 7.839s

Epoch 33 of 200
  training loss:		2.746229E-02
  validation loss:		2.702931E-02
Epoch took 8.163s

Epoch 34 of 200
  training loss:		2.762178E-02
  validation loss:		2.715296E-02
Epoch took 7.559s

Epoch 35 of 200
  training loss:		2.719773E-02
  validation loss:		2.755839E-02
Epoch took 8.951s

Epoch 36 of 200
  training loss:		2.721697E-02
  validation loss:		2.746166E-02
Epoch took 7.543s

Epoch 37 of 200
  training loss:		2.692605E-02
  validation loss:		2.688996E-02
Epoch took 7.552s

Epoch 38 of 200
  training loss:		2.726664E-02
  validation loss:		2.689279E-02
Epoch took 7.918s

Epoch 39 of 200
  training loss:		2.732083E-02
  validation loss:		2.693372E-02
Epoch took 7.382s

Epoch 40 of 200
  training loss:		2.730028E-02
  validation loss:		2.718361E-02
Epoch took 9.260s

Epoch 41 of 200
  training loss:		2.706252E-02
  validation loss:		2.689406E-02
Epoch took 8.102s

Epoch 42 of 200
  training loss:		2.707374E-02
  validation loss:		2.720939E-02
Epoch took 7.943s

Epoch 43 of 200
  training loss:		2.709866E-02
  validation loss:		2.728334E-02
Epoch took 8.205s

Epoch 44 of 200
  training loss:		2.710488E-02
  validation loss:		2.632258E-02
Epoch took 8.042s

Epoch 45 of 200
  training loss:		2.689350E-02
  validation loss:		2.679181E-02
Epoch took 7.458s

Epoch 46 of 200
  training loss:		2.680965E-02
  validation loss:		2.680851E-02
Epoch took 7.086s

Epoch 47 of 200
  training loss:		2.695355E-02
  validation loss:		2.751125E-02
Epoch took 7.594s

Epoch 48 of 200
  training loss:		2.704081E-02
  validation loss:		2.734098E-02
Epoch took 8.383s

Epoch 49 of 200
  training loss:		2.684567E-02
  validation loss:		2.716619E-02
Epoch took 7.635s

Epoch 50 of 200
  training loss:		2.694122E-02
  validation loss:		2.682624E-02
Epoch took 8.088s

Epoch 51 of 200
  training loss:		2.674946E-02
  validation loss:		2.692474E-02
Epoch took 8.494s

Epoch 52 of 200
  training loss:		2.681666E-02
  validation loss:		2.649837E-02
Epoch took 7.690s

Epoch 53 of 200
  training loss:		2.689486E-02
  validation loss:		2.644350E-02
Epoch took 7.715s

Epoch 54 of 200
  training loss:		2.685075E-02
  validation loss:		2.709137E-02
Epoch took 7.514s

Epoch 55 of 200
  training loss:		2.689038E-02
  validation loss:		2.655663E-02
Epoch took 7.090s

Epoch 56 of 200
  training loss:		2.680559E-02
  validation loss:		2.654201E-02
Epoch took 7.838s

Epoch 57 of 200
  training loss:		2.700603E-02
  validation loss:		2.675120E-02
Epoch took 8.185s

Epoch 58 of 200
  training loss:		2.673691E-02
  validation loss:		2.652919E-02
Epoch took 7.672s

Epoch 59 of 200
  training loss:		2.683324E-02
  validation loss:		2.628065E-02
Epoch took 8.350s

Epoch 60 of 200
  training loss:		2.687324E-02
  validation loss:		2.649443E-02
Epoch took 9.527s

Epoch 61 of 200
  training loss:		2.686562E-02
  validation loss:		2.822989E-02
Epoch took 8.494s

Epoch 62 of 200
  training loss:		2.686087E-02
  validation loss:		2.636798E-02
Epoch took 8.255s

Epoch 63 of 200
  training loss:		2.675058E-02
  validation loss:		2.647277E-02
Epoch took 7.460s

Epoch 64 of 200
  training loss:		2.675290E-02
  validation loss:		2.679697E-02
Epoch took 8.339s

Epoch 65 of 200
  training loss:		2.678295E-02
  validation loss:		2.696585E-02
Epoch took 7.505s

Epoch 66 of 200
  training loss:		2.679348E-02
  validation loss:		2.717937E-02
Epoch took 8.109s

Epoch 67 of 200
  training loss:		2.682936E-02
  validation loss:		2.673676E-02
Epoch took 7.698s

Epoch 68 of 200
  training loss:		2.670133E-02
  validation loss:		2.687331E-02
Epoch took 8.530s

Epoch 69 of 200
  training loss:		2.685112E-02
  validation loss:		2.629724E-02
Epoch took 7.697s

Epoch 70 of 200
  training loss:		2.649439E-02
  validation loss:		2.840468E-02
Epoch took 7.771s

Early stopping, val-loss increased over the last 10 epochs from 0.0266112072799 to 0.027032482226
Training RMSE: 1.5960721077e-07
Validation RMSE: 1.59680498059e-07
Test RMSE: 1.60539564588e-07
Test MSE: 2.5772951798e-14
Test MAE: 9.12339520649e-08
Test R2: 0.730134576737 

