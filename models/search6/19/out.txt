Training network with 75120 training samples and 14085 validation samples
Epoch 1 of 200
  training loss:		1.010099E-01
  validation loss:		4.622641E-02
Epoch took 7.588s

Epoch 2 of 200
  training loss:		4.017222E-02
  validation loss:		3.821076E-02
Epoch took 7.833s

Epoch 3 of 200
  training loss:		3.519972E-02
  validation loss:		3.217541E-02
Epoch took 7.575s

Epoch 4 of 200
  training loss:		3.532504E-02
  validation loss:		3.312499E-02
Epoch took 7.933s

Epoch 5 of 200
  training loss:		3.147244E-02
  validation loss:		2.999069E-02
Epoch took 8.366s

Epoch 6 of 200
  training loss:		3.116202E-02
  validation loss:		2.962779E-02
Epoch took 8.607s

Epoch 7 of 200
  training loss:		2.943131E-02
  validation loss:		2.801221E-02
Epoch took 7.684s

Epoch 8 of 200
  training loss:		2.920234E-02
  validation loss:		3.025941E-02
Epoch took 7.362s

Epoch 9 of 200
  training loss:		3.003217E-02
  validation loss:		3.405941E-02
Epoch took 7.393s

Epoch 10 of 200
  training loss:		2.995459E-02
  validation loss:		2.876873E-02
Epoch took 8.198s

Epoch 11 of 200
  training loss:		2.901803E-02
  validation loss:		2.883933E-02
Epoch took 7.663s

Epoch 12 of 200
  training loss:		2.825583E-02
  validation loss:		2.843088E-02
Epoch took 8.914s

Epoch 13 of 200
  training loss:		2.843867E-02
  validation loss:		2.813456E-02
Epoch took 8.150s

Epoch 14 of 200
  training loss:		2.833545E-02
  validation loss:		2.878870E-02
Epoch took 9.609s

Epoch 15 of 200
  training loss:		2.801208E-02
  validation loss:		2.796203E-02
Epoch took 8.822s

Epoch 16 of 200
  training loss:		2.895419E-02
  validation loss:		2.921008E-02
Epoch took 8.021s

Epoch 17 of 200
  training loss:		2.784433E-02
  validation loss:		2.793043E-02
Epoch took 7.601s

Epoch 18 of 200
  training loss:		2.826813E-02
  validation loss:		2.768523E-02
Epoch took 7.815s

Epoch 19 of 200
  training loss:		2.841662E-02
  validation loss:		3.028023E-02
Epoch took 8.128s

Epoch 20 of 200
  training loss:		2.821816E-02
  validation loss:		2.723620E-02
Epoch took 7.914s

Epoch 21 of 200
  training loss:		2.786346E-02
  validation loss:		2.791603E-02
Epoch took 8.118s

Epoch 22 of 200
  training loss:		2.764072E-02
  validation loss:		2.808799E-02
Epoch took 7.550s

Epoch 23 of 200
  training loss:		2.782322E-02
  validation loss:		2.740890E-02
Epoch took 7.957s

Epoch 24 of 200
  training loss:		2.802911E-02
  validation loss:		2.890153E-02
Epoch took 8.130s

Epoch 25 of 200
  training loss:		2.801324E-02
  validation loss:		2.878606E-02
Epoch took 8.749s

Epoch 26 of 200
  training loss:		2.880287E-02
  validation loss:		2.772205E-02
Epoch took 8.065s

Epoch 27 of 200
  training loss:		2.774310E-02
  validation loss:		2.754066E-02
Epoch took 9.405s

Epoch 28 of 200
  training loss:		2.741524E-02
  validation loss:		2.708681E-02
Epoch took 7.573s

Epoch 29 of 200
  training loss:		2.792447E-02
  validation loss:		2.828825E-02
Epoch took 7.432s

Epoch 30 of 200
  training loss:		2.735112E-02
  validation loss:		2.793985E-02
Epoch took 8.943s

Epoch 31 of 200
  training loss:		2.801849E-02
  validation loss:		2.973518E-02
Epoch took 8.107s

Epoch 32 of 200
  training loss:		2.741988E-02
  validation loss:		2.761002E-02
Epoch took 7.290s

Epoch 33 of 200
  training loss:		2.704531E-02
  validation loss:		2.720491E-02
Epoch took 7.831s

Epoch 34 of 200
  training loss:		2.713840E-02
  validation loss:		2.718965E-02
Epoch took 8.685s

Epoch 35 of 200
  training loss:		2.842891E-02
  validation loss:		2.800708E-02
Epoch took 8.309s

Epoch 36 of 200
  training loss:		2.732880E-02
  validation loss:		2.704200E-02
Epoch took 7.437s

Epoch 37 of 200
  training loss:		2.710381E-02
  validation loss:		2.750295E-02
Epoch took 8.081s

Epoch 38 of 200
  training loss:		2.741496E-02
  validation loss:		2.653830E-02
Epoch took 8.265s

Epoch 39 of 200
  training loss:		2.696742E-02
  validation loss:		2.667170E-02
Epoch took 7.708s

Epoch 40 of 200
  training loss:		2.693296E-02
  validation loss:		2.731878E-02
Epoch took 9.895s

Epoch 41 of 200
  training loss:		2.727261E-02
  validation loss:		2.701802E-02
Epoch took 8.024s

Epoch 42 of 200
  training loss:		2.707738E-02
  validation loss:		2.689856E-02
Epoch took 8.889s

Epoch 43 of 200
  training loss:		2.722242E-02
  validation loss:		2.768459E-02
Epoch took 7.144s

Epoch 44 of 200
  training loss:		2.765119E-02
  validation loss:		3.250204E-02
Epoch took 7.998s

Epoch 45 of 200
  training loss:		2.749092E-02
  validation loss:		2.692028E-02
Epoch took 8.636s

Epoch 46 of 200
  training loss:		2.740578E-02
  validation loss:		2.687588E-02
Epoch took 8.067s

Epoch 47 of 200
  training loss:		2.704301E-02
  validation loss:		2.678277E-02
Epoch took 8.312s

Epoch 48 of 200
  training loss:		2.740504E-02
  validation loss:		2.727155E-02
Epoch took 8.095s

Epoch 49 of 200
  training loss:		2.687835E-02
  validation loss:		2.698854E-02
Epoch took 8.155s

Epoch 50 of 200
  training loss:		2.683290E-02
  validation loss:		2.668606E-02
Epoch took 7.829s

Early stopping, val-loss increased over the last 10 epochs from 0.027482056944 to 0.0275628309042
Training RMSE: 1.61113384769e-07
Validation RMSE: 1.61703724713e-07
Test RMSE: 1.62197237509e-07
Test MSE: 2.63079438556e-14
Test MAE: 9.58344426438e-08
Test R2: 0.724532740393 

