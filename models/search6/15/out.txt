Training network with 75120 training samples and 14085 validation samples
Epoch 1 of 200
  training loss:		1.151080E-01
  validation loss:		5.716203E-02
Epoch took 8.427s

Epoch 2 of 200
  training loss:		4.189826E-02
  validation loss:		3.638773E-02
Epoch took 8.220s

Epoch 3 of 200
  training loss:		3.835999E-02
  validation loss:		3.579647E-02
Epoch took 9.162s

Epoch 4 of 200
  training loss:		3.559588E-02
  validation loss:		3.315260E-02
Epoch took 8.348s

Epoch 5 of 200
  training loss:		3.324572E-02
  validation loss:		3.271210E-02
Epoch took 9.002s

Epoch 6 of 200
  training loss:		3.175944E-02
  validation loss:		3.226316E-02
Epoch took 9.518s

Epoch 7 of 200
  training loss:		3.060299E-02
  validation loss:		3.072665E-02
Epoch took 8.779s

Epoch 8 of 200
  training loss:		3.055719E-02
  validation loss:		3.034384E-02
Epoch took 8.904s

Epoch 9 of 200
  training loss:		3.038507E-02
  validation loss:		3.153191E-02
Epoch took 7.683s

Epoch 10 of 200
  training loss:		2.985397E-02
  validation loss:		2.945896E-02
Epoch took 8.884s

Epoch 11 of 200
  training loss:		2.923213E-02
  validation loss:		2.885308E-02
Epoch took 9.575s

Epoch 12 of 200
  training loss:		2.911126E-02
  validation loss:		3.099925E-02
Epoch took 8.464s

Epoch 13 of 200
  training loss:		2.942497E-02
  validation loss:		3.586224E-02
Epoch took 8.729s

Epoch 14 of 200
  training loss:		2.908519E-02
  validation loss:		2.886454E-02
Epoch took 8.519s

Epoch 15 of 200
  training loss:		2.821516E-02
  validation loss:		2.763421E-02
Epoch took 8.510s

Epoch 16 of 200
  training loss:		2.844872E-02
  validation loss:		2.845188E-02
Epoch took 8.331s

Epoch 17 of 200
  training loss:		2.829488E-02
  validation loss:		2.744820E-02
Epoch took 8.282s

Epoch 18 of 200
  training loss:		2.798765E-02
  validation loss:		2.773317E-02
Epoch took 9.469s

Epoch 19 of 200
  training loss:		2.769969E-02
  validation loss:		2.891682E-02
Epoch took 9.108s

Epoch 20 of 200
  training loss:		2.813289E-02
  validation loss:		2.742044E-02
Epoch took 8.709s

Epoch 21 of 200
  training loss:		2.840735E-02
  validation loss:		2.834446E-02
Epoch took 8.711s

Epoch 22 of 200
  training loss:		2.882089E-02
  validation loss:		2.694385E-02
Epoch took 8.277s

Epoch 23 of 200
  training loss:		2.780277E-02
  validation loss:		2.867630E-02
Epoch took 8.704s

Epoch 24 of 200
  training loss:		2.744604E-02
  validation loss:		2.757449E-02
Epoch took 9.481s

Epoch 25 of 200
  training loss:		2.712571E-02
  validation loss:		2.644865E-02
Epoch took 9.087s

Epoch 26 of 200
  training loss:		2.728523E-02
  validation loss:		2.775064E-02
Epoch took 8.596s

Epoch 27 of 200
  training loss:		2.772009E-02
  validation loss:		3.039098E-02
Epoch took 8.840s

Epoch 28 of 200
  training loss:		2.846010E-02
  validation loss:		2.717938E-02
Epoch took 8.438s

Epoch 29 of 200
  training loss:		2.729488E-02
  validation loss:		2.673814E-02
Epoch took 8.451s

Epoch 30 of 200
  training loss:		2.977832E-02
  validation loss:		2.831601E-02
Epoch took 8.801s

Epoch 31 of 200
  training loss:		2.685323E-02
  validation loss:		2.685967E-02
Epoch took 9.283s

Epoch 32 of 200
  training loss:		2.652386E-02
  validation loss:		2.642919E-02
Epoch took 9.349s

Epoch 33 of 200
  training loss:		2.783252E-02
  validation loss:		2.610986E-02
Epoch took 8.133s

Epoch 34 of 200
  training loss:		2.620709E-02
  validation loss:		2.626761E-02
Epoch took 8.141s

Epoch 35 of 200
  training loss:		2.651494E-02
  validation loss:		2.637594E-02
Epoch took 9.316s

Epoch 36 of 200
  training loss:		2.715106E-02
  validation loss:		2.761790E-02
Epoch took 8.711s

Epoch 37 of 200
  training loss:		2.853870E-02
  validation loss:		2.695755E-02
Epoch took 8.312s

Epoch 38 of 200
  training loss:		2.630267E-02
  validation loss:		2.751775E-02
Epoch took 8.585s

Epoch 39 of 200
  training loss:		2.619898E-02
  validation loss:		2.632410E-02
Epoch took 8.350s

Epoch 40 of 200
  training loss:		2.748853E-02
  validation loss:		2.660930E-02
Epoch took 8.140s

Epoch 41 of 200
  training loss:		2.667205E-02
  validation loss:		2.606286E-02
Epoch took 8.127s

Epoch 42 of 200
  training loss:		2.608729E-02
  validation loss:		2.612423E-02
Epoch took 7.791s

Epoch 43 of 200
  training loss:		2.600821E-02
  validation loss:		2.627305E-02
Epoch took 8.117s

Epoch 44 of 200
  training loss:		2.657191E-02
  validation loss:		3.638969E-02
Epoch took 8.706s

Epoch 45 of 200
  training loss:		3.013403E-02
  validation loss:		3.175415E-02
Epoch took 7.788s

Epoch 46 of 200
  training loss:		2.679731E-02
  validation loss:		2.603796E-02
Epoch took 8.050s

Epoch 47 of 200
  training loss:		2.600829E-02
  validation loss:		2.589146E-02
Epoch took 8.915s

Epoch 48 of 200
  training loss:		2.593653E-02
  validation loss:		2.604265E-02
Epoch took 8.483s

Epoch 49 of 200
  training loss:		2.597705E-02
  validation loss:		2.602560E-02
Epoch took 8.497s

Epoch 50 of 200
  training loss:		2.626448E-02
  validation loss:		2.726852E-02
Epoch took 10.167s

Early stopping, val-loss increased over the last 10 epochs from 0.026706887523 to 0.0277870157803
Training RMSE: 1.59056048292e-07
Validation RMSE: 1.59841614271e-07
Test RMSE: 1.60337967323e-07
Test MSE: 2.57082637653e-14
Test MAE: 9.12696090019e-08
Test R2: 0.730811917208 

