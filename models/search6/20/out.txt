Training network with 75120 training samples and 14085 validation samples
Epoch 1 of 200
  training loss:		1.660232E-01
  validation loss:		5.692338E-02
Epoch took 7.929s

Epoch 2 of 200
  training loss:		4.678173E-02
  validation loss:		4.016120E-02
Epoch took 8.767s

Epoch 3 of 200
  training loss:		3.919604E-02
  validation loss:		3.624249E-02
Epoch took 9.190s

Epoch 4 of 200
  training loss:		3.539566E-02
  validation loss:		3.590045E-02
Epoch took 7.836s

Epoch 5 of 200
  training loss:		3.412756E-02
  validation loss:		3.147495E-02
Epoch took 7.197s

Epoch 6 of 200
  training loss:		3.283204E-02
  validation loss:		3.210066E-02
Epoch took 6.917s

Epoch 7 of 200
  training loss:		3.123750E-02
  validation loss:		3.106308E-02
Epoch took 6.351s

Epoch 8 of 200
  training loss:		3.059805E-02
  validation loss:		3.052232E-02
Epoch took 6.380s

Epoch 9 of 200
  training loss:		2.984030E-02
  validation loss:		3.007180E-02
Epoch took 6.427s

Epoch 10 of 200
  training loss:		3.025984E-02
  validation loss:		2.965122E-02
Epoch took 6.429s

Epoch 11 of 200
  training loss:		2.904946E-02
  validation loss:		2.876697E-02
Epoch took 7.305s

Epoch 12 of 200
  training loss:		2.910530E-02
  validation loss:		2.881011E-02
Epoch took 8.605s

Epoch 13 of 200
  training loss:		2.871566E-02
  validation loss:		2.837717E-02
Epoch took 8.174s

Epoch 14 of 200
  training loss:		2.886920E-02
  validation loss:		2.929710E-02
Epoch took 7.458s

Epoch 15 of 200
  training loss:		2.788985E-02
  validation loss:		2.673683E-02
Epoch took 7.588s

Epoch 16 of 200
  training loss:		2.772770E-02
  validation loss:		2.794198E-02
Epoch took 8.166s

Epoch 17 of 200
  training loss:		2.801027E-02
  validation loss:		2.765697E-02
Epoch took 8.570s

Epoch 18 of 200
  training loss:		2.797104E-02
  validation loss:		2.771721E-02
Epoch took 7.884s

Epoch 19 of 200
  training loss:		2.812108E-02
  validation loss:		2.761437E-02
Epoch took 7.300s

Epoch 20 of 200
  training loss:		2.787535E-02
  validation loss:		2.877748E-02
Epoch took 7.402s

Epoch 21 of 200
  training loss:		2.740489E-02
  validation loss:		2.719883E-02
Epoch took 8.907s

Epoch 22 of 200
  training loss:		2.812015E-02
  validation loss:		2.724578E-02
Epoch took 8.234s

Epoch 23 of 200
  training loss:		2.792297E-02
  validation loss:		2.912559E-02
Epoch took 8.207s

Epoch 24 of 200
  training loss:		2.726817E-02
  validation loss:		2.746795E-02
Epoch took 8.925s

Epoch 25 of 200
  training loss:		2.734716E-02
  validation loss:		2.735959E-02
Epoch took 8.580s

Epoch 26 of 200
  training loss:		2.783682E-02
  validation loss:		3.207899E-02
Epoch took 9.123s

Epoch 27 of 200
  training loss:		2.824828E-02
  validation loss:		2.724692E-02
Epoch took 8.311s

Epoch 28 of 200
  training loss:		2.752732E-02
  validation loss:		2.861677E-02
Epoch took 8.498s

Epoch 29 of 200
  training loss:		2.795688E-02
  validation loss:		2.839538E-02
Epoch took 8.497s

Epoch 30 of 200
  training loss:		2.749824E-02
  validation loss:		2.728831E-02
Epoch took 8.920s

Early stopping, val-loss increased over the last 10 epochs from 0.0281696189482 to 0.0282024105143
Training RMSE: 1.65187286397e-07
Validation RMSE: 1.6609737953e-07
Test RMSE: 1.66890697876e-07
Test MSE: 2.78525050376e-14
Test MAE: 1.02135407062e-07
Test R2: 0.708359829335 

