Training network with 75120 training samples and 14085 validation samples
Epoch 1 of 100
  training loss:		6.727800E-03
  validation loss:		8.684778E-05
Epoch took 10.758s

Epoch 2 of 100
  training loss:		4.061928E-05
  validation loss:		1.560586E-05
Epoch took 10.995s

Epoch 3 of 100
  training loss:		1.545697E-05
  validation loss:		5.951408E-06
Epoch took 10.223s

Epoch 4 of 100
  training loss:		1.882469E-05
  validation loss:		6.345296E-06
Epoch took 11.788s

Epoch 5 of 100
  training loss:		1.136695E-04
  validation loss:		1.388667E-04
Epoch took 11.001s

Epoch 6 of 100
  training loss:		1.674876E-04
  validation loss:		5.893642E-05
Epoch took 9.485s

Epoch 7 of 100
  training loss:		1.343352E-04
  validation loss:		4.775440E-05
Epoch took 9.371s

Epoch 8 of 100
  training loss:		1.929921E-04
  validation loss:		9.823696E-06
Epoch took 9.983s

Epoch 9 of 100
  training loss:		5.320272E-05
  validation loss:		3.939644E-04
Epoch took 10.377s

Epoch 10 of 100
  training loss:		1.428918E-04
  validation loss:		2.525750E-06
Epoch took 9.766s

Epoch 11 of 100
  training loss:		1.409889E-04
  validation loss:		3.801544E-06
Epoch took 10.495s

Epoch 12 of 100
  training loss:		3.618613E-06
  validation loss:		2.567937E-05
Epoch took 10.651s

Epoch 13 of 100
  training loss:		1.094133E-04
  validation loss:		6.280361E-06
Epoch took 10.089s

Epoch 14 of 100
  training loss:		1.008834E-04
  validation loss:		1.468076E-04
Epoch took 9.926s

Epoch 15 of 100
  training loss:		1.403301E-05
  validation loss:		6.844886E-07
Epoch took 9.554s

Epoch 16 of 100
  training loss:		1.272754E-04
  validation loss:		1.833034E-05
Epoch took 9.300s

Epoch 17 of 100
  training loss:		3.589397E-06
  validation loss:		1.796807E-06
Epoch took 10.634s

Epoch 18 of 100
  training loss:		1.001817E-05
  validation loss:		5.267777E-06
Epoch took 9.406s

Epoch 19 of 100
  training loss:		5.407062E-05
  validation loss:		5.576780E-06
Epoch took 9.424s

Epoch 20 of 100
  training loss:		8.029423E-06
  validation loss:		6.113173E-06
Epoch took 9.695s

Epoch 21 of 100
  training loss:		3.454389E-05
  validation loss:		2.146497E-06
Epoch took 10.805s

Epoch 22 of 100
  training loss:		2.883162E-05
  validation loss:		2.818710E-04
Epoch took 10.772s

Epoch 23 of 100
  training loss:		3.682903E-05
  validation loss:		3.511352E-07
Epoch took 9.557s

Epoch 24 of 100
  training loss:		9.370019E-06
  validation loss:		2.924955E-06
Epoch took 9.928s

Epoch 25 of 100
  training loss:		1.785473E-05
  validation loss:		2.132391E-06
Epoch took 10.151s

Epoch 26 of 100
  training loss:		2.115626E-05
  validation loss:		2.182193E-06
Epoch took 8.988s

Epoch 27 of 100
  training loss:		1.790620E-05
  validation loss:		1.871700E-04
Epoch took 9.460s

Epoch 28 of 100
  training loss:		1.090247E-05
  validation loss:		4.517348E-07
Epoch took 10.078s

Epoch 29 of 100
  training loss:		1.471677E-05
  validation loss:		7.373261E-06
Epoch took 10.096s

Epoch 30 of 100
  training loss:		4.258229E-06
  validation loss:		1.678710E-07
Epoch took 9.997s

Early stopping, val-loss increased over the last 10 epochs from 2.20338290624e-05 to 4.86771079659e-05
Training RMSE: 0.00247390949544
Validation RMSE: 0.00247246001877
