Training network with 75120 training samples and 14085 validation samples
Epoch 1 of 100
  training loss:		1.202620E-01
  validation loss:		5.334615E-02
Epoch took 8.354s

Epoch 2 of 100
  training loss:		4.579177E-02
  validation loss:		3.931312E-02
Epoch took 9.157s

Epoch 3 of 100
  training loss:		3.807805E-02
  validation loss:		3.613161E-02
Epoch took 8.582s

Epoch 4 of 100
  training loss:		3.520201E-02
  validation loss:		3.532714E-02
Epoch took 9.148s

Epoch 5 of 100
  training loss:		3.412899E-02
  validation loss:		3.172750E-02
Epoch took 8.866s

Epoch 6 of 100
  training loss:		3.212029E-02
  validation loss:		3.130247E-02
Epoch took 9.142s

Epoch 7 of 100
  training loss:		3.240910E-02
  validation loss:		3.040256E-02
Epoch took 9.418s

Epoch 8 of 100
  training loss:		3.092020E-02
  validation loss:		2.976486E-02
Epoch took 8.037s

Epoch 9 of 100
  training loss:		3.019256E-02
  validation loss:		3.006031E-02
Epoch took 8.836s

Epoch 10 of 100
  training loss:		3.051008E-02
  validation loss:		2.841275E-02
Epoch took 9.958s

Epoch 11 of 100
  training loss:		2.966664E-02
  validation loss:		3.063227E-02
Epoch took 8.677s

Epoch 12 of 100
  training loss:		2.921722E-02
  validation loss:		2.891360E-02
Epoch took 8.985s

Epoch 13 of 100
  training loss:		2.930967E-02
  validation loss:		2.832421E-02
Epoch took 8.119s

Epoch 14 of 100
  training loss:		2.933118E-02
  validation loss:		2.963617E-02
Epoch took 9.793s

Epoch 15 of 100
  training loss:		2.902407E-02
  validation loss:		2.860983E-02
Epoch took 9.158s

Epoch 16 of 100
  training loss:		2.922108E-02
  validation loss:		2.852472E-02
Epoch took 7.502s

Epoch 17 of 100
  training loss:		2.865025E-02
  validation loss:		2.982018E-02
Epoch took 9.504s

Epoch 18 of 100
  training loss:		2.852604E-02
  validation loss:		2.769611E-02
Epoch took 9.361s

Epoch 19 of 100
  training loss:		2.873648E-02
  validation loss:		2.930073E-02
Epoch took 9.236s

Epoch 20 of 100
  training loss:		2.826028E-02
  validation loss:		2.775314E-02
Epoch took 9.474s

Epoch 21 of 100
  training loss:		2.831167E-02
  validation loss:		2.780153E-02
Epoch took 8.390s

Epoch 22 of 100
  training loss:		2.820058E-02
  validation loss:		2.860304E-02
Epoch took 8.944s

Epoch 23 of 100
  training loss:		2.811075E-02
  validation loss:		2.856827E-02
Epoch took 10.086s

Epoch 24 of 100
  training loss:		2.853374E-02
  validation loss:		2.773994E-02
Epoch took 9.652s

Epoch 25 of 100
  training loss:		2.775157E-02
  validation loss:		2.727396E-02
Epoch took 8.717s

Epoch 26 of 100
  training loss:		2.766959E-02
  validation loss:		2.866893E-02
Epoch took 8.167s

Epoch 27 of 100
  training loss:		2.764774E-02
  validation loss:		2.989514E-02
Epoch took 8.907s

Epoch 28 of 100
  training loss:		2.827933E-02
  validation loss:		2.760542E-02
Epoch took 9.238s

Epoch 29 of 100
  training loss:		2.770861E-02
  validation loss:		2.754478E-02
Epoch took 9.621s

Epoch 30 of 100
  training loss:		2.778604E-02
  validation loss:		2.743577E-02
Epoch took 8.742s

Early stopping, val-loss increased over the last 5 epochs from 0.0279973491466 to 0.0282300110359
Training RMSE: 1.61472707932e-07
Validation RMSE: 1.6182026195e-07
