Training network with 75120 training samples and 14085 validation samples
Epoch 1 of 100
  training loss:		1.322400E-01
  validation loss:		8.487388E-03
Epoch took 8.948s

Epoch 2 of 100
  training loss:		6.057755E-03
  validation loss:		4.044123E-03
Epoch took 8.496s

Epoch 3 of 100
  training loss:		3.232788E-03
  validation loss:		2.374965E-03
Epoch took 8.445s

Epoch 4 of 100
  training loss:		1.988250E-03
  validation loss:		1.535125E-03
Epoch took 8.470s

Epoch 5 of 100
  training loss:		1.301694E-03
  validation loss:		1.036191E-03
Epoch took 8.648s

Epoch 6 of 100
  training loss:		8.849600E-04
  validation loss:		7.140350E-04
Epoch took 8.314s

Epoch 7 of 100
  training loss:		6.147482E-04
  validation loss:		4.981832E-04
Epoch took 9.009s

Epoch 8 of 100
  training loss:		4.279969E-04
  validation loss:		3.460749E-04
Epoch took 8.491s

Epoch 9 of 100
  training loss:		2.965570E-04
  validation loss:		2.411540E-04
Epoch took 8.622s

Epoch 10 of 100
  training loss:		2.046235E-04
  validation loss:		1.666878E-04
Epoch took 8.222s

Epoch 11 of 100
  training loss:		1.414673E-04
  validation loss:		1.150648E-04
Epoch took 9.512s

Epoch 12 of 100
  training loss:		9.772341E-05
  validation loss:		7.977834E-05
Epoch took 8.913s

Epoch 13 of 100
  training loss:		6.732489E-05
  validation loss:		5.506115E-05
Epoch took 8.331s

Epoch 14 of 100
  training loss:		4.737051E-05
  validation loss:		3.977684E-05
Epoch took 8.817s

Epoch 15 of 100
  training loss:		3.345591E-05
  validation loss:		2.747391E-05
Epoch took 8.588s

Epoch 16 of 100
  training loss:		2.345382E-05
  validation loss:		2.063766E-05
Epoch took 7.655s

Epoch 17 of 100
  training loss:		1.684712E-05
  validation loss:		1.394434E-05
Epoch took 8.473s

Epoch 18 of 100
  training loss:		1.238103E-05
  validation loss:		1.146631E-05
Epoch took 8.478s

Epoch 19 of 100
  training loss:		8.847465E-06
  validation loss:		7.092724E-06
Epoch took 9.963s

Epoch 20 of 100
  training loss:		6.274985E-06
  validation loss:		5.651662E-06
Epoch took 7.767s

Epoch 21 of 100
  training loss:		4.799272E-06
  validation loss:		3.805423E-06
Epoch took 8.696s

Epoch 22 of 100
  training loss:		3.415720E-06
  validation loss:		3.978448E-06
Epoch took 8.156s

Epoch 23 of 100
  training loss:		2.684606E-06
  validation loss:		2.330695E-06
Epoch took 7.152s

Epoch 24 of 100
  training loss:		1.792935E-06
  validation loss:		1.362882E-06
Epoch took 6.912s

Epoch 25 of 100
  training loss:		1.342139E-06
  validation loss:		1.161818E-06
Epoch took 7.652s

Epoch 26 of 100
  training loss:		9.261723E-07
  validation loss:		8.697422E-07
Epoch took 7.878s

Epoch 27 of 100
  training loss:		7.281881E-07
  validation loss:		6.723399E-07
Epoch took 8.235s

Epoch 28 of 100
  training loss:		6.246483E-07
  validation loss:		3.122316E-07
Epoch took 8.151s

Epoch 29 of 100
  training loss:		4.469314E-07
  validation loss:		3.321532E-06
Epoch took 8.926s

Epoch 30 of 100
  training loss:		5.602227E-07
  validation loss:		4.633257E-07
Epoch took 8.036s

Epoch 31 of 100
  training loss:		3.478058E-07
  validation loss:		5.226020E-07
Epoch took 7.309s

Epoch 32 of 100
  training loss:		2.890332E-06
  validation loss:		8.482909E-07
Epoch took 6.891s

Epoch 33 of 100
  training loss:		1.429382E-06
  validation loss:		2.329111E-06
Epoch took 7.091s

Epoch 34 of 100
  training loss:		8.363054E-07
  validation loss:		7.684218E-08
Epoch took 8.324s

Epoch 35 of 100
  training loss:		4.793396E-06
  validation loss:		2.608751E-06
Epoch took 8.800s

Early stopping, val-loss increased over the last 5 epochs from 1.12783419072e-06 to 1.27711941068e-06
Training RMSE: 0.000670136152969
Validation RMSE: 0.000680762834524
