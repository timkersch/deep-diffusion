Training network with 75120 training samples and 14085 validation samples
Epoch 1 of 100
  training loss:		6.119471E-02
  validation loss:		3.838074E-02
Epoch took 10.486s

Epoch 2 of 100
  training loss:		3.897473E-02
  validation loss:		3.555792E-02
Epoch took 9.810s

Epoch 3 of 100
  training loss:		3.431279E-02
  validation loss:		3.419947E-02
Epoch took 9.916s

Epoch 4 of 100
  training loss:		3.322294E-02
  validation loss:		3.116339E-02
Epoch took 9.413s

Epoch 5 of 100
  training loss:		3.169928E-02
  validation loss:		2.959221E-02
Epoch took 10.192s

Epoch 6 of 100
  training loss:		3.158199E-02
  validation loss:		2.922920E-02
Epoch took 9.795s

Epoch 7 of 100
  training loss:		3.025349E-02
  validation loss:		3.016727E-02
Epoch took 9.934s

Epoch 8 of 100
  training loss:		3.002776E-02
  validation loss:		2.820725E-02
Epoch took 10.626s

Epoch 9 of 100
  training loss:		2.937786E-02
  validation loss:		2.832437E-02
Epoch took 10.115s

Epoch 10 of 100
  training loss:		2.932049E-02
  validation loss:		2.996785E-02
Epoch took 10.182s

Epoch 11 of 100
  training loss:		2.879790E-02
  validation loss:		2.873523E-02
Epoch took 8.333s

Epoch 12 of 100
  training loss:		2.897136E-02
  validation loss:		2.986920E-02
Epoch took 9.417s

Epoch 13 of 100
  training loss:		2.862305E-02
  validation loss:		2.828298E-02
Epoch took 9.600s

Epoch 14 of 100
  training loss:		2.833109E-02
  validation loss:		2.938845E-02
Epoch took 10.855s

Epoch 15 of 100
  training loss:		2.834180E-02
  validation loss:		2.861902E-02
Epoch took 9.533s

Epoch 16 of 100
  training loss:		2.828048E-02
  validation loss:		2.802260E-02
Epoch took 8.746s

Epoch 17 of 100
  training loss:		2.799846E-02
  validation loss:		2.907700E-02
Epoch took 10.006s

Epoch 18 of 100
  training loss:		2.823234E-02
  validation loss:		2.761815E-02
Epoch took 11.761s

Epoch 19 of 100
  training loss:		2.820718E-02
  validation loss:		2.727325E-02
Epoch took 9.671s

Epoch 20 of 100
  training loss:		2.770054E-02
  validation loss:		2.728197E-02
Epoch took 10.408s

Epoch 21 of 100
  training loss:		2.785273E-02
  validation loss:		2.938632E-02
Epoch took 10.284s

Epoch 22 of 100
  training loss:		2.770277E-02
  validation loss:		2.877101E-02
Epoch took 9.576s

Epoch 23 of 100
  training loss:		2.769595E-02
  validation loss:		2.717003E-02
Epoch took 10.714s

Epoch 24 of 100
  training loss:		2.771127E-02
  validation loss:		2.852035E-02
Epoch took 10.105s

Epoch 25 of 100
  training loss:		2.755111E-02
  validation loss:		2.794916E-02
Epoch took 10.062s

Epoch 26 of 100
  training loss:		2.732525E-02
  validation loss:		2.832982E-02
Epoch took 9.371s

Epoch 27 of 100
  training loss:		2.718549E-02
  validation loss:		2.706965E-02
Epoch took 11.233s

Epoch 28 of 100
  training loss:		2.743566E-02
  validation loss:		2.921802E-02
Epoch took 10.048s

Epoch 29 of 100
  training loss:		2.722319E-02
  validation loss:		2.703039E-02
Epoch took 10.279s

Epoch 30 of 100
  training loss:		2.708893E-02
  validation loss:		2.721964E-02
Epoch took 9.222s

Epoch 31 of 100
  training loss:		2.711111E-02
  validation loss:		2.768650E-02
Epoch took 10.353s

Epoch 32 of 100
  training loss:		2.715274E-02
  validation loss:		2.686788E-02
Epoch took 9.729s

Epoch 33 of 100
  training loss:		2.712755E-02
  validation loss:		2.846147E-02
Epoch took 10.086s

Epoch 34 of 100
  training loss:		2.696713E-02
  validation loss:		2.758131E-02
Epoch took 11.010s

Epoch 35 of 100
  training loss:		2.706916E-02
  validation loss:		2.797963E-02
Epoch took 10.301s

Epoch 36 of 100
  training loss:		2.691433E-02
  validation loss:		2.677387E-02
Epoch took 10.900s

Epoch 37 of 100
  training loss:		2.686347E-02
  validation loss:		2.657133E-02
Epoch took 10.810s

Epoch 38 of 100
  training loss:		2.681282E-02
  validation loss:		2.686735E-02
Epoch took 8.924s

Epoch 39 of 100
  training loss:		2.686583E-02
  validation loss:		2.682915E-02
Epoch took 10.482s

Epoch 40 of 100
  training loss:		2.675668E-02
  validation loss:		2.679197E-02
Epoch took 9.631s

Epoch 41 of 100
  training loss:		2.684886E-02
  validation loss:		2.653107E-02
Epoch took 10.337s

Epoch 42 of 100
  training loss:		2.693748E-02
  validation loss:		2.638436E-02
Epoch took 10.984s

Epoch 43 of 100
  training loss:		2.695215E-02
  validation loss:		2.663155E-02
Epoch took 10.878s

Epoch 44 of 100
  training loss:		2.667471E-02
  validation loss:		2.678152E-02
Epoch took 10.383s

Epoch 45 of 100
  training loss:		2.667189E-02
  validation loss:		2.700988E-02
Epoch took 10.711s

Epoch 46 of 100
  training loss:		2.675974E-02
  validation loss:		2.650141E-02
Epoch took 10.302s

Epoch 47 of 100
  training loss:		2.666374E-02
  validation loss:		2.725898E-02
Epoch took 10.423s

Epoch 48 of 100
  training loss:		2.651107E-02
  validation loss:		2.676275E-02
Epoch took 10.012s

Epoch 49 of 100
  training loss:		2.650182E-02
  validation loss:		2.676420E-02
Epoch took 10.698s

Epoch 50 of 100
  training loss:		2.656606E-02
  validation loss:		2.615723E-02
Epoch took 10.768s

Epoch 51 of 100
  training loss:		2.660670E-02
  validation loss:		2.637677E-02
Epoch took 11.655s

Epoch 52 of 100
  training loss:		2.662674E-02
  validation loss:		2.685124E-02
Epoch took 11.448s

Epoch 53 of 100
  training loss:		2.655094E-02
  validation loss:		2.651084E-02
Epoch took 9.888s

Epoch 54 of 100
  training loss:		2.643209E-02
  validation loss:		2.634408E-02
Epoch took 9.637s

Epoch 55 of 100
  training loss:		2.652152E-02
  validation loss:		2.635240E-02
Epoch took 9.899s

Epoch 56 of 100
  training loss:		2.650293E-02
  validation loss:		2.642645E-02
Epoch took 11.527s

Epoch 57 of 100
  training loss:		2.650568E-02
  validation loss:		2.668159E-02
Epoch took 9.533s

Epoch 58 of 100
  training loss:		2.652961E-02
  validation loss:		2.635784E-02
Epoch took 9.780s

Epoch 59 of 100
  training loss:		2.640280E-02
  validation loss:		2.648192E-02
Epoch took 10.930s

Epoch 60 of 100
  training loss:		2.640230E-02
  validation loss:		2.630071E-02
Epoch took 10.165s

Epoch 61 of 100
  training loss:		2.636527E-02
  validation loss:		2.679421E-02
Epoch took 10.627s

Epoch 62 of 100
  training loss:		2.632203E-02
  validation loss:		2.634502E-02
Epoch took 11.609s

Epoch 63 of 100
  training loss:		2.638469E-02
  validation loss:		2.638684E-02
Epoch took 9.988s

Epoch 64 of 100
  training loss:		2.645413E-02
  validation loss:		2.639757E-02
Epoch took 10.430s

Epoch 65 of 100
  training loss:		2.639609E-02
  validation loss:		2.691452E-02
Epoch took 10.984s

Epoch 66 of 100
  training loss:		2.639480E-02
  validation loss:		2.660754E-02
Epoch took 10.503s

Epoch 67 of 100
  training loss:		2.641330E-02
  validation loss:		2.719909E-02
Epoch took 10.644s

Epoch 68 of 100
  training loss:		2.635050E-02
  validation loss:		2.651559E-02
Epoch took 10.378s

Epoch 69 of 100
  training loss:		2.625618E-02
  validation loss:		2.648782E-02
Epoch took 10.566s

Epoch 70 of 100
  training loss:		2.630773E-02
  validation loss:		2.631506E-02
Epoch took 11.490s

Early stopping, val-loss increased over the last 10 epochs from 0.0264683852649 to 0.0265963254751
Training RMSE: 1.58674268262e-07
Validation RMSE: 1.58909546649e-07
