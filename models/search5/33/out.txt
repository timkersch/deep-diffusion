Training network with 75120 training samples and 14085 validation samples
Epoch 1 of 100
  training loss:		9.216900E-02
  validation loss:		1.700885E-02
Epoch took 7.917s

Epoch 2 of 100
  training loss:		1.082387E-02
  validation loss:		7.356677E-03
Epoch took 7.879s

Epoch 3 of 100
  training loss:		5.898863E-03
  validation loss:		4.817079E-03
Epoch took 8.057s

Epoch 4 of 100
  training loss:		4.144466E-03
  validation loss:		3.550779E-03
Epoch took 7.697s

Epoch 5 of 100
  training loss:		3.141780E-03
  validation loss:		2.792147E-03
Epoch took 9.009s

Epoch 6 of 100
  training loss:		2.494682E-03
  validation loss:		2.235624E-03
Epoch took 8.019s

Epoch 7 of 100
  training loss:		2.015207E-03
  validation loss:		1.826947E-03
Epoch took 8.063s

Epoch 8 of 100
  training loss:		1.640441E-03
  validation loss:		1.489002E-03
Epoch took 8.447s

Epoch 9 of 100
  training loss:		1.356026E-03
  validation loss:		1.241202E-03
Epoch took 8.577s

Epoch 10 of 100
  training loss:		1.124969E-03
  validation loss:		1.041280E-03
Epoch took 7.754s

Epoch 11 of 100
  training loss:		9.448509E-04
  validation loss:		8.748945E-04
Epoch took 7.107s

Epoch 12 of 100
  training loss:		7.991098E-04
  validation loss:		7.457798E-04
Epoch took 7.673s

Epoch 13 of 100
  training loss:		6.755296E-04
  validation loss:		6.321682E-04
Epoch took 8.586s

Epoch 14 of 100
  training loss:		5.746027E-04
  validation loss:		5.410925E-04
Epoch took 7.615s

Epoch 15 of 100
  training loss:		4.906818E-04
  validation loss:		4.629579E-04
Epoch took 8.020s

Epoch 16 of 100
  training loss:		4.212695E-04
  validation loss:		3.970589E-04
Epoch took 8.057s

Epoch 17 of 100
  training loss:		3.618651E-04
  validation loss:		3.424241E-04
Epoch took 7.624s

Epoch 18 of 100
  training loss:		3.106476E-04
  validation loss:		2.965543E-04
Epoch took 7.723s

Epoch 19 of 100
  training loss:		2.682545E-04
  validation loss:		2.566217E-04
Epoch took 8.355s

Epoch 20 of 100
  training loss:		2.311914E-04
  validation loss:		2.208438E-04
Epoch took 8.694s

Epoch 21 of 100
  training loss:		1.999857E-04
  validation loss:		1.909134E-04
Epoch took 7.970s

Epoch 22 of 100
  training loss:		1.727817E-04
  validation loss:		1.649365E-04
Epoch took 8.632s

Epoch 23 of 100
  training loss:		1.494214E-04
  validation loss:		1.427579E-04
Epoch took 8.126s

Epoch 24 of 100
  training loss:		1.291313E-04
  validation loss:		1.226501E-04
Epoch took 7.731s

Epoch 25 of 100
  training loss:		1.121546E-04
  validation loss:		1.070214E-04
Epoch took 8.151s

Epoch 26 of 100
  training loss:		9.685435E-05
  validation loss:		9.133813E-05
Epoch took 8.038s

Epoch 27 of 100
  training loss:		8.356890E-05
  validation loss:		7.993862E-05
Epoch took 7.350s

Epoch 28 of 100
  training loss:		7.252740E-05
  validation loss:		6.841369E-05
Epoch took 7.905s

Epoch 29 of 100
  training loss:		6.284683E-05
  validation loss:		6.002957E-05
Epoch took 7.853s

Epoch 30 of 100
  training loss:		5.456613E-05
  validation loss:		5.207264E-05
Epoch took 8.189s

Epoch 31 of 100
  training loss:		4.734184E-05
  validation loss:		4.528735E-05
Epoch took 7.572s

Epoch 32 of 100
  training loss:		4.141338E-05
  validation loss:		3.951126E-05
Epoch took 8.220s

Epoch 33 of 100
  training loss:		3.614700E-05
  validation loss:		3.445231E-05
Epoch took 7.877s

Epoch 34 of 100
  training loss:		3.194566E-05
  validation loss:		3.066104E-05
Epoch took 8.011s

Epoch 35 of 100
  training loss:		2.825987E-05
  validation loss:		2.678581E-05
Epoch took 7.749s

Epoch 36 of 100
  training loss:		2.517877E-05
  validation loss:		2.408374E-05
Epoch took 8.011s

Epoch 37 of 100
  training loss:		2.265286E-05
  validation loss:		2.217990E-05
Epoch took 8.171s

Epoch 38 of 100
  training loss:		2.020016E-05
  validation loss:		1.954472E-05
Epoch took 8.883s

Epoch 39 of 100
  training loss:		1.807771E-05
  validation loss:		1.668323E-05
Epoch took 8.252s

Epoch 40 of 100
  training loss:		1.596634E-05
  validation loss:		1.510176E-05
Epoch took 8.519s

Epoch 41 of 100
  training loss:		1.424475E-05
  validation loss:		1.363967E-05
Epoch took 8.361s

Epoch 42 of 100
  training loss:		1.284380E-05
  validation loss:		1.214188E-05
Epoch took 7.493s

Epoch 43 of 100
  training loss:		1.160199E-05
  validation loss:		1.103320E-05
Epoch took 8.769s

Epoch 44 of 100
  training loss:		1.046664E-05
  validation loss:		1.043074E-05
Epoch took 8.333s

Epoch 45 of 100
  training loss:		9.336662E-06
  validation loss:		8.895257E-06
Epoch took 8.357s

Epoch 46 of 100
  training loss:		8.467708E-06
  validation loss:		9.218714E-06
Epoch took 7.485s

Epoch 47 of 100
  training loss:		7.719291E-06
  validation loss:		7.481482E-06
Epoch took 7.628s

Epoch 48 of 100
  training loss:		6.911359E-06
  validation loss:		6.597466E-06
Epoch took 7.803s

Epoch 49 of 100
  training loss:		6.412545E-06
  validation loss:		5.889494E-06
Epoch took 8.491s

Epoch 50 of 100
  training loss:		5.675185E-06
  validation loss:		5.408303E-06
Epoch took 8.140s

Epoch 51 of 100
  training loss:		5.276600E-06
  validation loss:		5.009757E-06
Epoch took 8.125s

Epoch 52 of 100
  training loss:		4.719040E-06
  validation loss:		4.712130E-06
Epoch took 8.086s

Epoch 53 of 100
  training loss:		4.460150E-06
  validation loss:		3.859724E-06
Epoch took 8.366s

Epoch 54 of 100
  training loss:		3.929166E-06
  validation loss:		4.081968E-06
Epoch took 8.181s

Epoch 55 of 100
  training loss:		3.571438E-06
  validation loss:		3.252563E-06
Epoch took 8.122s

Epoch 56 of 100
  training loss:		3.294930E-06
  validation loss:		2.865534E-06
Epoch took 8.427s

Epoch 57 of 100
  training loss:		3.108929E-06
  validation loss:		2.591694E-06
Epoch took 7.648s

Epoch 58 of 100
  training loss:		2.852716E-06
  validation loss:		2.938922E-06
Epoch took 8.101s

Epoch 59 of 100
  training loss:		2.570403E-06
  validation loss:		2.026220E-06
Epoch took 7.617s

Epoch 60 of 100
  training loss:		2.252526E-06
  validation loss:		2.008754E-06
Epoch took 8.667s

Epoch 61 of 100
  training loss:		1.942600E-06
  validation loss:		2.717886E-06
Epoch took 9.076s

Epoch 62 of 100
  training loss:		1.730354E-06
  validation loss:		1.498159E-06
Epoch took 8.027s

Epoch 63 of 100
  training loss:		1.508945E-06
  validation loss:		1.352486E-06
Epoch took 8.100s

Epoch 64 of 100
  training loss:		1.390292E-06
  validation loss:		1.910043E-06
Epoch took 8.282s

Epoch 65 of 100
  training loss:		1.520223E-06
  validation loss:		1.090982E-06
Epoch took 8.612s

Epoch 66 of 100
  training loss:		1.072812E-06
  validation loss:		1.157792E-06
Epoch took 8.008s

Epoch 67 of 100
  training loss:		1.204685E-06
  validation loss:		9.093084E-07
Epoch took 8.084s

Epoch 68 of 100
  training loss:		1.303449E-06
  validation loss:		1.381855E-06
Epoch took 7.138s

Epoch 69 of 100
  training loss:		9.404219E-07
  validation loss:		6.300179E-07
Epoch took 7.791s

Epoch 70 of 100
  training loss:		8.929122E-07
  validation loss:		6.070661E-07
Epoch took 8.148s

Epoch 71 of 100
  training loss:		8.837396E-07
  validation loss:		9.340839E-07
Epoch took 7.811s

Epoch 72 of 100
  training loss:		7.945923E-07
  validation loss:		4.130224E-07
Epoch took 7.487s

Epoch 73 of 100
  training loss:		8.209635E-07
  validation loss:		3.391833E-07
Epoch took 7.295s

Epoch 74 of 100
  training loss:		1.123080E-06
  validation loss:		1.324490E-06
Epoch took 7.800s

Epoch 75 of 100
  training loss:		7.035995E-07
  validation loss:		4.038235E-07
Epoch took 7.696s

Epoch 76 of 100
  training loss:		3.206341E-06
  validation loss:		1.854572E-07
Epoch took 8.651s

Epoch 77 of 100
  training loss:		2.441916E-07
  validation loss:		1.539179E-07
Epoch took 7.440s

Epoch 78 of 100
  training loss:		2.777270E-07
  validation loss:		3.766116E-07
Epoch took 8.818s

Epoch 79 of 100
  training loss:		3.920709E-07
  validation loss:		7.108965E-07
Epoch took 7.851s

Epoch 80 of 100
  training loss:		5.611305E-07
  validation loss:		5.441571E-07
Epoch took 8.414s

Epoch 81 of 100
  training loss:		2.375331E-06
  validation loss:		2.081086E-07
Epoch took 8.463s

Epoch 82 of 100
  training loss:		1.355868E-07
  validation loss:		1.248695E-07
Epoch took 9.128s

Epoch 83 of 100
  training loss:		2.967878E-07
  validation loss:		1.300558E-07
Epoch took 8.549s

Epoch 84 of 100
  training loss:		2.520347E-07
  validation loss:		9.652760E-07
Epoch took 9.316s

Epoch 85 of 100
  training loss:		3.347769E-07
  validation loss:		8.734786E-08
Epoch took 7.704s

Epoch 86 of 100
  training loss:		8.199257E-07
  validation loss:		4.874829E-07
Epoch took 8.657s

Epoch 87 of 100
  training loss:		6.806421E-07
  validation loss:		3.331625E-06
Epoch took 8.374s

Epoch 88 of 100
  training loss:		3.939112E-07
  validation loss:		3.352154E-07
Epoch took 8.516s

Epoch 89 of 100
  training loss:		1.039321E-06
  validation loss:		4.441115E-07
Epoch took 8.419s

Epoch 90 of 100
  training loss:		1.831908E-07
  validation loss:		4.778526E-08
Epoch took 8.738s

Early stopping, val-loss increased over the last 10 epochs from 5.38564316696e-07 to 6.16187789022e-07
Training RMSE: 0.000742282823463
Validation RMSE: 0.000739932066057
