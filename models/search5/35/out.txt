Training network with 75120 training samples and 14085 validation samples
Epoch 1 of 100
  training loss:		2.636146E-02
  validation loss:		2.630275E-03
Epoch took 7.811s

Epoch 2 of 100
  training loss:		1.599771E-03
  validation loss:		8.872719E-04
Epoch took 7.515s

Epoch 3 of 100
  training loss:		6.200527E-04
  validation loss:		3.951040E-04
Epoch took 8.337s

Epoch 4 of 100
  training loss:		2.981401E-04
  validation loss:		1.977830E-04
Epoch took 7.535s

Epoch 5 of 100
  training loss:		1.538086E-04
  validation loss:		1.097252E-04
Epoch took 8.151s

Epoch 6 of 100
  training loss:		8.644248E-05
  validation loss:		6.617534E-05
Epoch took 8.040s

Epoch 7 of 100
  training loss:		5.239302E-05
  validation loss:		3.934004E-05
Epoch took 7.684s

Epoch 8 of 100
  training loss:		3.186998E-05
  validation loss:		2.478062E-05
Epoch took 9.180s

Epoch 9 of 100
  training loss:		1.986381E-05
  validation loss:		1.560819E-05
Epoch took 8.057s

Epoch 10 of 100
  training loss:		1.286882E-05
  validation loss:		1.040818E-05
Epoch took 7.397s

Epoch 11 of 100
  training loss:		8.424999E-06
  validation loss:		6.744893E-06
Epoch took 7.781s

Epoch 12 of 100
  training loss:		5.722182E-06
  validation loss:		4.648787E-06
Epoch took 8.713s

Epoch 13 of 100
  training loss:		4.100918E-06
  validation loss:		5.000598E-06
Epoch took 7.597s

Epoch 14 of 100
  training loss:		3.285495E-06
  validation loss:		3.028341E-06
Epoch took 7.947s

Epoch 15 of 100
  training loss:		2.258851E-06
  validation loss:		2.649313E-06
Epoch took 7.526s

Epoch 16 of 100
  training loss:		1.740551E-06
  validation loss:		1.537075E-06
Epoch took 7.534s

Epoch 17 of 100
  training loss:		1.461318E-06
  validation loss:		1.123399E-06
Epoch took 7.920s

Epoch 18 of 100
  training loss:		1.051499E-06
  validation loss:		7.940421E-07
Epoch took 8.746s

Epoch 19 of 100
  training loss:		1.137207E-06
  validation loss:		7.869548E-07
Epoch took 9.716s

Epoch 20 of 100
  training loss:		5.927510E-07
  validation loss:		4.659203E-07
Epoch took 7.618s

Epoch 21 of 100
  training loss:		4.513785E-07
  validation loss:		3.658082E-07
Epoch took 9.232s

Epoch 22 of 100
  training loss:		6.914569E-07
  validation loss:		3.284640E-07
Epoch took 8.375s

Epoch 23 of 100
  training loss:		1.908219E-06
  validation loss:		3.975839E-06
Epoch took 7.357s

Epoch 24 of 100
  training loss:		2.983037E-05
  validation loss:		1.428393E-06
Epoch took 7.592s

Epoch 25 of 100
  training loss:		1.123535E-05
  validation loss:		3.100470E-05
Epoch took 8.843s

Epoch 26 of 100
  training loss:		5.223655E-06
  validation loss:		4.091830E-07
Epoch took 8.494s

Epoch 27 of 100
  training loss:		1.498520E-04
  validation loss:		5.162802E-06
Epoch took 8.475s

Epoch 28 of 100
  training loss:		7.575656E-07
  validation loss:		1.803628E-07
Epoch took 7.892s

Epoch 29 of 100
  training loss:		1.999802E-07
  validation loss:		5.221207E-08
Epoch took 8.064s

Epoch 30 of 100
  training loss:		2.539260E-07
  validation loss:		6.380520E-08
Epoch took 8.337s

Early stopping, val-loss increased over the last 10 epochs from 2.67793219674e-06 to 4.29715676639e-06
Training RMSE: 0.000662322826616
Validation RMSE: 0.000680215909712
