Training network with 75120 training samples and 14085 validation samples
Epoch 1 of 100
  training loss:		2.354828E-01
  validation loss:		2.319423E-02
Epoch took 8.523s

Epoch 2 of 100
  training loss:		1.505086E-02
  validation loss:		1.000594E-02
Epoch took 8.249s

Epoch 3 of 100
  training loss:		7.640230E-03
  validation loss:		5.617391E-03
Epoch took 7.954s

Epoch 4 of 100
  training loss:		4.478712E-03
  validation loss:		3.435153E-03
Epoch took 7.723s

Epoch 5 of 100
  training loss:		2.878892E-03
  validation loss:		2.337782E-03
Epoch took 8.375s

Epoch 6 of 100
  training loss:		2.030427E-03
  validation loss:		1.713549E-03
Epoch took 8.495s

Epoch 7 of 100
  training loss:		1.505357E-03
  validation loss:		1.291640E-03
Epoch took 9.241s

Epoch 8 of 100
  training loss:		1.148526E-03
  validation loss:		9.963375E-04
Epoch took 9.479s

Epoch 9 of 100
  training loss:		8.946984E-04
  validation loss:		7.925632E-04
Epoch took 8.301s

Epoch 10 of 100
  training loss:		7.115850E-04
  validation loss:		6.343681E-04
Epoch took 9.042s

Epoch 11 of 100
  training loss:		5.701351E-04
  validation loss:		5.100995E-04
Epoch took 8.072s

Epoch 12 of 100
  training loss:		4.593014E-04
  validation loss:		4.141006E-04
Epoch took 8.608s

Epoch 13 of 100
  training loss:		3.726428E-04
  validation loss:		3.414676E-04
Epoch took 8.605s

Epoch 14 of 100
  training loss:		3.030804E-04
  validation loss:		2.734383E-04
Epoch took 9.308s

Epoch 15 of 100
  training loss:		2.461530E-04
  validation loss:		2.234758E-04
Epoch took 8.413s

Epoch 16 of 100
  training loss:		2.002605E-04
  validation loss:		1.834418E-04
Epoch took 8.608s

Epoch 17 of 100
  training loss:		1.633220E-04
  validation loss:		1.591576E-04
Epoch took 8.444s

Epoch 18 of 100
  training loss:		1.327491E-04
  validation loss:		1.218933E-04
Epoch took 8.042s

Epoch 19 of 100
  training loss:		1.070437E-04
  validation loss:		1.005038E-04
Epoch took 8.285s

Epoch 20 of 100
  training loss:		8.478080E-05
  validation loss:		7.658446E-05
Epoch took 9.242s

Epoch 21 of 100
  training loss:		6.750897E-05
  validation loss:		6.006572E-05
Epoch took 8.429s

Epoch 22 of 100
  training loss:		5.314829E-05
  validation loss:		4.703243E-05
Epoch took 8.917s

Epoch 23 of 100
  training loss:		4.170276E-05
  validation loss:		3.708881E-05
Epoch took 7.709s

Epoch 24 of 100
  training loss:		3.212023E-05
  validation loss:		2.799853E-05
Epoch took 8.741s

Epoch 25 of 100
  training loss:		2.437496E-05
  validation loss:		2.084600E-05
Epoch took 7.962s

Epoch 26 of 100
  training loss:		1.836228E-05
  validation loss:		1.527222E-05
Epoch took 8.205s

Epoch 27 of 100
  training loss:		1.339517E-05
  validation loss:		1.375024E-05
Epoch took 8.103s

Epoch 28 of 100
  training loss:		9.673812E-06
  validation loss:		7.830829E-06
Epoch took 8.392s

Epoch 29 of 100
  training loss:		7.291418E-06
  validation loss:		5.452793E-06
Epoch took 8.983s

Epoch 30 of 100
  training loss:		5.208068E-06
  validation loss:		4.490422E-06
Epoch took 8.924s

Epoch 31 of 100
  training loss:		3.460742E-06
  validation loss:		2.541657E-06
Epoch took 8.124s

Epoch 32 of 100
  training loss:		2.486857E-06
  validation loss:		1.796388E-06
Epoch took 7.805s

Epoch 33 of 100
  training loss:		1.790131E-06
  validation loss:		1.571175E-06
Epoch took 8.633s

Epoch 34 of 100
  training loss:		1.719513E-06
  validation loss:		3.326511E-06
Epoch took 9.077s

Epoch 35 of 100
  training loss:		1.306416E-06
  validation loss:		5.667244E-07
Epoch took 9.155s

Epoch 36 of 100
  training loss:		7.008901E-07
  validation loss:		5.062135E-07
Epoch took 8.776s

Epoch 37 of 100
  training loss:		8.126743E-07
  validation loss:		6.522406E-07
Epoch took 9.582s

Epoch 38 of 100
  training loss:		7.859269E-07
  validation loss:		1.979506E-07
Epoch took 8.115s

Epoch 39 of 100
  training loss:		1.096238E-06
  validation loss:		1.724377E-07
Epoch took 8.669s

Epoch 40 of 100
  training loss:		3.263000E-07
  validation loss:		1.838975E-06
Epoch took 8.138s

Epoch 41 of 100
  training loss:		4.533556E-07
  validation loss:		1.419643E-07
Epoch took 7.701s

Epoch 42 of 100
  training loss:		7.012612E-07
  validation loss:		3.351189E-07
Epoch took 8.145s

Epoch 43 of 100
  training loss:		1.883937E-06
  validation loss:		2.107109E-07
Epoch took 9.261s

Epoch 44 of 100
  training loss:		6.990256E-08
  validation loss:		8.093168E-08
Epoch took 9.324s

Epoch 45 of 100
  training loss:		3.687851E-07
  validation loss:		4.672843E-07
Epoch took 10.572s

Epoch 46 of 100
  training loss:		4.016286E-06
  validation loss:		2.032008E-08
Epoch took 9.100s

Epoch 47 of 100
  training loss:		2.062800E-08
  validation loss:		1.019595E-08
Epoch took 8.479s

Epoch 48 of 100
  training loss:		2.211946E-08
  validation loss:		1.154691E-08
Epoch took 8.791s

Epoch 49 of 100
  training loss:		1.008007E-06
  validation loss:		4.555957E-06
Epoch took 7.799s

Epoch 50 of 100
  training loss:		1.807043E-06
  validation loss:		8.834588E-09
Epoch took 9.208s

Epoch 51 of 100
  training loss:		3.784050E-08
  validation loss:		3.745818E-07
Epoch took 8.621s

Epoch 52 of 100
  training loss:		4.241576E-07
  validation loss:		1.214943E-06
Epoch took 8.823s

Epoch 53 of 100
  training loss:		1.516423E-06
  validation loss:		6.286426E-08
Epoch took 9.163s

Epoch 54 of 100
  training loss:		6.984883E-08
  validation loss:		7.364016E-07
Epoch took 8.270s

Epoch 55 of 100
  training loss:		6.701969E-07
  validation loss:		5.883468E-08
Epoch took 9.282s

Epoch 56 of 100
  training loss:		2.063520E-06
  validation loss:		4.168057E-08
Epoch took 8.537s

Epoch 57 of 100
  training loss:		1.745735E-07
  validation loss:		1.376613E-06
Epoch took 8.225s

Epoch 58 of 100
  training loss:		5.619802E-07
  validation loss:		1.814751E-07
Epoch took 8.207s

Epoch 59 of 100
  training loss:		3.888464E-06
  validation loss:		1.497585E-07
Epoch took 8.612s

Epoch 60 of 100
  training loss:		1.696306E-08
  validation loss:		1.432181E-09
Epoch took 9.683s

Epoch 61 of 100
  training loss:		1.535965E-08
  validation loss:		6.448330E-08
Epoch took 9.974s

Epoch 62 of 100
  training loss:		2.117896E-06
  validation loss:		5.806539E-07
Epoch took 8.272s

Epoch 63 of 100
  training loss:		1.077452E-07
  validation loss:		5.320529E-09
Epoch took 9.393s

Epoch 64 of 100
  training loss:		1.236211E-08
  validation loss:		7.074288E-08
Epoch took 9.248s

Epoch 65 of 100
  training loss:		2.457331E-06
  validation loss:		1.561983E-08
Epoch took 8.902s

Epoch 66 of 100
  training loss:		7.167010E-09
  validation loss:		1.096020E-09
Epoch took 7.435s

Epoch 67 of 100
  training loss:		1.313065E-06
  validation loss:		1.512915E-06
Epoch took 7.532s

Epoch 68 of 100
  training loss:		3.720515E-07
  validation loss:		5.674679E-09
Epoch took 8.001s

Epoch 69 of 100
  training loss:		5.177115E-07
  validation loss:		7.786656E-06
Epoch took 8.348s

Epoch 70 of 100
  training loss:		1.697217E-06
  validation loss:		2.015201E-08
Epoch took 7.962s

Epoch 71 of 100
  training loss:		9.493768E-09
  validation loss:		2.608270E-09
Epoch took 8.112s

Epoch 72 of 100
  training loss:		1.525376E-06
  validation loss:		1.000409E-08
Epoch took 9.300s

Epoch 73 of 100
  training loss:		2.563034E-07
  validation loss:		2.337947E-07
Epoch took 7.999s

Epoch 74 of 100
  training loss:		2.771443E-06
  validation loss:		3.449971E-08
Epoch took 8.021s

Epoch 75 of 100
  training loss:		1.037115E-08
  validation loss:		1.287456E-08
Epoch took 8.365s

Epoch 76 of 100
  training loss:		9.247817E-07
  validation loss:		1.355435E-05
Epoch took 10.250s

Epoch 77 of 100
  training loss:		5.673834E-07
  validation loss:		1.985562E-07
Epoch took 9.103s

Epoch 78 of 100
  training loss:		7.156885E-07
  validation loss:		7.770525E-07
Epoch took 9.659s

Epoch 79 of 100
  training loss:		2.349961E-06
  validation loss:		3.617236E-09
Epoch took 8.551s

Epoch 80 of 100
  training loss:		2.146925E-08
  validation loss:		1.716905E-07
Epoch took 6.938s

Epoch 81 of 100
  training loss:		5.857001E-07
  validation loss:		4.860852E-07
Epoch took 6.800s

Epoch 82 of 100
  training loss:		3.766748E-06
  validation loss:		3.241975E-06
Epoch took 6.924s

Epoch 83 of 100
  training loss:		3.825380E-07
  validation loss:		2.261899E-09
Epoch took 6.803s

Epoch 84 of 100
  training loss:		1.625083E-09
  validation loss:		2.217348E-10
Epoch took 6.813s

Epoch 85 of 100
  training loss:		6.779218E-10
  validation loss:		2.658685E-09
Epoch took 6.827s

Epoch 86 of 100
  training loss:		1.353073E-06
  validation loss:		1.101250E-07
Epoch took 7.465s

Epoch 87 of 100
  training loss:		1.150443E-08
  validation loss:		1.452818E-08
Epoch took 7.581s

Epoch 88 of 100
  training loss:		9.014520E-07
  validation loss:		7.532846E-08
Epoch took 8.734s

Epoch 89 of 100
  training loss:		1.528407E-06
  validation loss:		1.877350E-08
Epoch took 8.807s

Epoch 90 of 100
  training loss:		5.738758E-09
  validation loss:		1.611813E-08
Epoch took 8.827s

Epoch 91 of 100
  training loss:		1.455829E-06
  validation loss:		2.080808E-09
Epoch took 8.642s

Epoch 92 of 100
  training loss:		1.964374E-06
  validation loss:		6.179717E-07
Epoch took 9.015s

Epoch 93 of 100
  training loss:		4.144013E-08
  validation loss:		2.065608E-09
Epoch took 8.878s

Epoch 94 of 100
  training loss:		3.938094E-08
  validation loss:		3.785623E-07
Epoch took 9.032s

Epoch 95 of 100
  training loss:		2.809061E-06
  validation loss:		1.101802E-08
Epoch took 9.064s

Epoch 96 of 100
  training loss:		1.935425E-08
  validation loss:		1.670043E-08
Epoch took 8.787s

Epoch 97 of 100
  training loss:		9.076100E-07
  validation loss:		2.978479E-06
Epoch took 8.629s

Epoch 98 of 100
  training loss:		1.495479E-06
  validation loss:		2.094073E-09
Epoch took 9.333s

Epoch 99 of 100
  training loss:		7.746384E-09
  validation loss:		5.776025E-09
Epoch took 8.092s

Epoch 100 of 100
  training loss:		1.908896E-06
  validation loss:		5.738039E-08
Epoch took 8.271s

Training RMSE: 0.000241031362841
Validation RMSE: 0.000239535274875
