Training network with 75120 training samples and 14085 validation samples
Epoch 1 of 100
  training loss:		3.237270E-01
  validation loss:		2.820311E-02
Epoch took 7.275s

Epoch 2 of 100
  training loss:		1.669636E-02
  validation loss:		1.054779E-02
Epoch took 8.311s

Epoch 3 of 100
  training loss:		7.867659E-03
  validation loss:		6.177881E-03
Epoch took 8.785s

Epoch 4 of 100
  training loss:		5.025866E-03
  validation loss:		4.308084E-03
Epoch took 8.709s

Epoch 5 of 100
  training loss:		3.657459E-03
  validation loss:		3.246029E-03
Epoch took 8.081s

Epoch 6 of 100
  training loss:		2.801741E-03
  validation loss:		2.561857E-03
Epoch took 7.634s

Epoch 7 of 100
  training loss:		2.228557E-03
  validation loss:		2.055280E-03
Epoch took 8.048s

Epoch 8 of 100
  training loss:		1.814597E-03
  validation loss:		1.709470E-03
Epoch took 8.043s

Epoch 9 of 100
  training loss:		1.497463E-03
  validation loss:		1.408461E-03
Epoch took 7.422s

Epoch 10 of 100
  training loss:		1.237139E-03
  validation loss:		1.181574E-03
Epoch took 7.844s

Epoch 11 of 100
  training loss:		1.028499E-03
  validation loss:		9.847049E-04
Epoch took 7.560s

Epoch 12 of 100
  training loss:		8.541765E-04
  validation loss:		8.203613E-04
Epoch took 8.344s

Epoch 13 of 100
  training loss:		7.100470E-04
  validation loss:		6.804623E-04
Epoch took 7.760s

Epoch 14 of 100
  training loss:		5.890594E-04
  validation loss:		5.641021E-04
Epoch took 8.424s

Epoch 15 of 100
  training loss:		4.894800E-04
  validation loss:		4.691694E-04
Epoch took 7.892s

Epoch 16 of 100
  training loss:		4.068968E-04
  validation loss:		3.864011E-04
Epoch took 8.635s

Epoch 17 of 100
  training loss:		3.360015E-04
  validation loss:		3.233964E-04
Epoch took 7.674s

Epoch 18 of 100
  training loss:		2.797078E-04
  validation loss:		2.677486E-04
Epoch took 7.982s

Epoch 19 of 100
  training loss:		2.321535E-04
  validation loss:		2.227168E-04
Epoch took 7.998s

Epoch 20 of 100
  training loss:		1.940545E-04
  validation loss:		1.854798E-04
Epoch took 7.422s

Epoch 21 of 100
  training loss:		1.603080E-04
  validation loss:		1.534992E-04
Epoch took 7.456s

Epoch 22 of 100
  training loss:		1.335051E-04
  validation loss:		1.272015E-04
Epoch took 9.246s

Epoch 23 of 100
  training loss:		1.115421E-04
  validation loss:		1.077202E-04
Epoch took 7.125s

Epoch 24 of 100
  training loss:		9.195340E-05
  validation loss:		8.724601E-05
Epoch took 7.181s

Epoch 25 of 100
  training loss:		7.623741E-05
  validation loss:		7.198278E-05
Epoch took 7.994s

Epoch 26 of 100
  training loss:		6.307522E-05
  validation loss:		5.930886E-05
Epoch took 7.959s

Epoch 27 of 100
  training loss:		5.224524E-05
  validation loss:		4.947000E-05
Epoch took 7.082s

Epoch 28 of 100
  training loss:		4.290812E-05
  validation loss:		4.018255E-05
Epoch took 7.280s

Epoch 29 of 100
  training loss:		3.508465E-05
  validation loss:		3.262309E-05
Epoch took 7.386s

Epoch 30 of 100
  training loss:		2.869984E-05
  validation loss:		2.649648E-05
Epoch took 8.353s

Epoch 31 of 100
  training loss:		2.343050E-05
  validation loss:		2.200345E-05
Epoch took 7.414s

Epoch 32 of 100
  training loss:		1.892534E-05
  validation loss:		1.740192E-05
Epoch took 7.526s

Epoch 33 of 100
  training loss:		1.570967E-05
  validation loss:		1.431575E-05
Epoch took 8.168s

Epoch 34 of 100
  training loss:		1.239326E-05
  validation loss:		1.132787E-05
Epoch took 7.129s

Epoch 35 of 100
  training loss:		1.022176E-05
  validation loss:		9.069340E-06
Epoch took 7.987s

Epoch 36 of 100
  training loss:		8.698028E-06
  validation loss:		7.958486E-06
Epoch took 8.412s

Epoch 37 of 100
  training loss:		6.737855E-06
  validation loss:		7.127222E-06
Epoch took 8.169s

Epoch 38 of 100
  training loss:		5.633418E-06
  validation loss:		8.278464E-06
Epoch took 8.393s

Epoch 39 of 100
  training loss:		4.706063E-06
  validation loss:		4.308717E-06
Epoch took 8.038s

Epoch 40 of 100
  training loss:		3.778329E-06
  validation loss:		3.424806E-06
Epoch took 8.407s

Epoch 41 of 100
  training loss:		3.229382E-06
  validation loss:		2.789560E-06
Epoch took 8.424s

Epoch 42 of 100
  training loss:		2.673828E-06
  validation loss:		2.408424E-06
Epoch took 7.756s

Epoch 43 of 100
  training loss:		2.268996E-06
  validation loss:		1.940154E-06
Epoch took 8.261s

Epoch 44 of 100
  training loss:		2.074669E-06
  validation loss:		2.093021E-06
Epoch took 8.293s

Epoch 45 of 100
  training loss:		1.757928E-06
  validation loss:		2.055140E-06
Epoch took 8.658s

Epoch 46 of 100
  training loss:		1.557132E-06
  validation loss:		1.367277E-06
Epoch took 7.775s

Epoch 47 of 100
  training loss:		1.288580E-06
  validation loss:		2.348535E-06
Epoch took 9.358s

Epoch 48 of 100
  training loss:		1.422565E-06
  validation loss:		2.620366E-06
Epoch took 8.117s

Epoch 49 of 100
  training loss:		1.281378E-06
  validation loss:		3.349950E-06
Epoch took 7.734s

Epoch 50 of 100
  training loss:		2.665344E-06
  validation loss:		2.388733E-06
Epoch took 7.551s

Epoch 51 of 100
  training loss:		9.697652E-07
  validation loss:		1.794668E-06
Epoch took 7.988s

Epoch 52 of 100
  training loss:		7.180669E-07
  validation loss:		6.822120E-07
Epoch took 7.557s

Epoch 53 of 100
  training loss:		9.369578E-07
  validation loss:		1.516599E-06
Epoch took 7.719s

Epoch 54 of 100
  training loss:		6.649524E-07
  validation loss:		7.450579E-07
Epoch took 6.873s

Epoch 55 of 100
  training loss:		3.965249E-06
  validation loss:		1.883158E-05
Epoch took 8.222s

Epoch 56 of 100
  training loss:		1.563836E-06
  validation loss:		3.462138E-07
Epoch took 7.426s

Epoch 57 of 100
  training loss:		3.799200E-07
  validation loss:		8.688380E-07
Epoch took 7.818s

Epoch 58 of 100
  training loss:		6.926508E-07
  validation loss:		2.390233E-07
Epoch took 7.879s

Epoch 59 of 100
  training loss:		6.205888E-07
  validation loss:		2.973434E-07
Epoch took 7.525s

Epoch 60 of 100
  training loss:		1.048285E-06
  validation loss:		4.780913E-07
Epoch took 8.428s

Early stopping, val-loss increased over the last 10 epochs from 2.3361159947e-06 to 2.57996245143e-06
Training RMSE: 0.00152442381571
Validation RMSE: 0.00154456805049
