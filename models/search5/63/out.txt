Training network with 75120 training samples and 14085 validation samples
Epoch 1 of 100
  training loss:		6.313654E-02
  validation loss:		4.412110E-02
Epoch took 9.482s

Epoch 2 of 100
  training loss:		4.150495E-02
  validation loss:		4.061564E-02
Epoch took 9.963s

Epoch 3 of 100
  training loss:		3.654731E-02
  validation loss:		3.791721E-02
Epoch took 9.875s

Epoch 4 of 100
  training loss:		3.384395E-02
  validation loss:		3.313338E-02
Epoch took 9.980s

Epoch 5 of 100
  training loss:		3.227126E-02
  validation loss:		3.012079E-02
Epoch took 10.339s

Epoch 6 of 100
  training loss:		3.115377E-02
  validation loss:		3.125142E-02
Epoch took 9.311s

Epoch 7 of 100
  training loss:		3.065030E-02
  validation loss:		3.151008E-02
Epoch took 10.775s

Epoch 8 of 100
  training loss:		2.957046E-02
  validation loss:		3.000769E-02
Epoch took 10.243s

Epoch 9 of 100
  training loss:		2.955596E-02
  validation loss:		2.877481E-02
Epoch took 9.833s

Epoch 10 of 100
  training loss:		2.926309E-02
  validation loss:		2.885072E-02
Epoch took 10.149s

Epoch 11 of 100
  training loss:		2.908234E-02
  validation loss:		2.931386E-02
Epoch took 10.688s

Epoch 12 of 100
  training loss:		2.842757E-02
  validation loss:		2.759541E-02
Epoch took 10.953s

Epoch 13 of 100
  training loss:		2.859048E-02
  validation loss:		2.744504E-02
Epoch took 11.650s

Epoch 14 of 100
  training loss:		2.817114E-02
  validation loss:		2.702911E-02
Epoch took 9.983s

Epoch 15 of 100
  training loss:		2.846843E-02
  validation loss:		3.068941E-02
Epoch took 9.863s

Epoch 16 of 100
  training loss:		2.821080E-02
  validation loss:		2.793379E-02
Epoch took 10.246s

Epoch 17 of 100
  training loss:		2.808390E-02
  validation loss:		2.774989E-02
Epoch took 10.374s

Epoch 18 of 100
  training loss:		2.791992E-02
  validation loss:		2.976536E-02
Epoch took 9.592s

Epoch 19 of 100
  training loss:		2.791084E-02
  validation loss:		2.713929E-02
Epoch took 9.465s

Epoch 20 of 100
  training loss:		2.798051E-02
  validation loss:		2.681934E-02
Epoch took 10.630s

Epoch 21 of 100
  training loss:		2.773332E-02
  validation loss:		2.779155E-02
Epoch took 10.600s

Epoch 22 of 100
  training loss:		2.777931E-02
  validation loss:		2.686290E-02
Epoch took 10.619s

Epoch 23 of 100
  training loss:		2.763340E-02
  validation loss:		2.680194E-02
Epoch took 10.289s

Epoch 24 of 100
  training loss:		2.756409E-02
  validation loss:		2.753322E-02
Epoch took 10.959s

Epoch 25 of 100
  training loss:		2.755901E-02
  validation loss:		2.705775E-02
Epoch took 10.868s

Epoch 26 of 100
  training loss:		2.772460E-02
  validation loss:		2.758971E-02
Epoch took 10.266s

Epoch 27 of 100
  training loss:		2.745385E-02
  validation loss:		2.706470E-02
Epoch took 10.524s

Epoch 28 of 100
  training loss:		2.753981E-02
  validation loss:		2.813202E-02
Epoch took 10.569s

Epoch 29 of 100
  training loss:		2.746231E-02
  validation loss:		2.703437E-02
Epoch took 9.652s

Epoch 30 of 100
  training loss:		2.727743E-02
  validation loss:		2.948094E-02
Epoch took 10.189s

Epoch 31 of 100
  training loss:		2.726193E-02
  validation loss:		2.705624E-02
Epoch took 11.295s

Epoch 32 of 100
  training loss:		2.729770E-02
  validation loss:		2.804346E-02
Epoch took 9.216s

Epoch 33 of 100
  training loss:		2.718174E-02
  validation loss:		2.715623E-02
Epoch took 9.724s

Epoch 34 of 100
  training loss:		2.732533E-02
  validation loss:		2.791605E-02
Epoch took 11.448s

Epoch 35 of 100
  training loss:		2.727406E-02
  validation loss:		2.688551E-02
Epoch took 11.162s

Epoch 36 of 100
  training loss:		2.705542E-02
  validation loss:		2.728734E-02
Epoch took 10.199s

Epoch 37 of 100
  training loss:		2.702646E-02
  validation loss:		2.754421E-02
Epoch took 10.583s

Epoch 38 of 100
  training loss:		2.719966E-02
  validation loss:		2.665282E-02
Epoch took 10.385s

Epoch 39 of 100
  training loss:		2.705830E-02
  validation loss:		2.720799E-02
Epoch took 10.272s

Epoch 40 of 100
  training loss:		2.697670E-02
  validation loss:		2.680476E-02
Epoch took 10.554s

Epoch 41 of 100
  training loss:		2.692883E-02
  validation loss:		2.685681E-02
Epoch took 9.813s

Epoch 42 of 100
  training loss:		2.694850E-02
  validation loss:		2.667235E-02
Epoch took 9.616s

Epoch 43 of 100
  training loss:		2.688914E-02
  validation loss:		2.749386E-02
Epoch took 10.766s

Epoch 44 of 100
  training loss:		2.693645E-02
  validation loss:		2.734537E-02
Epoch took 9.589s

Epoch 45 of 100
  training loss:		2.698854E-02
  validation loss:		2.703569E-02
Epoch took 9.342s

Epoch 46 of 100
  training loss:		2.668631E-02
  validation loss:		2.635868E-02
Epoch took 10.081s

Epoch 47 of 100
  training loss:		2.680573E-02
  validation loss:		2.705562E-02
Epoch took 10.597s

Epoch 48 of 100
  training loss:		2.697084E-02
  validation loss:		2.732723E-02
Epoch took 11.079s

Epoch 49 of 100
  training loss:		2.683441E-02
  validation loss:		2.719640E-02
Epoch took 10.119s

Epoch 50 of 100
  training loss:		2.679771E-02
  validation loss:		2.660366E-02
Epoch took 11.479s

Epoch 51 of 100
  training loss:		2.685205E-02
  validation loss:		2.722395E-02
Epoch took 10.265s

Epoch 52 of 100
  training loss:		2.683549E-02
  validation loss:		2.681455E-02
Epoch took 10.070s

Epoch 53 of 100
  training loss:		2.680961E-02
  validation loss:		2.645977E-02
Epoch took 8.961s

Epoch 54 of 100
  training loss:		2.671459E-02
  validation loss:		2.683540E-02
Epoch took 10.221s

Epoch 55 of 100
  training loss:		2.675757E-02
  validation loss:		2.638383E-02
Epoch took 8.683s

Epoch 56 of 100
  training loss:		2.671963E-02
  validation loss:		2.686637E-02
Epoch took 7.754s

Epoch 57 of 100
  training loss:		2.669721E-02
  validation loss:		2.714411E-02
Epoch took 7.726s

Epoch 58 of 100
  training loss:		2.668645E-02
  validation loss:		2.694871E-02
Epoch took 8.872s

Epoch 59 of 100
  training loss:		2.680294E-02
  validation loss:		2.653165E-02
Epoch took 9.822s

Epoch 60 of 100
  training loss:		2.652190E-02
  validation loss:		2.630439E-02
Epoch took 10.008s

Epoch 61 of 100
  training loss:		2.661379E-02
  validation loss:		2.746595E-02
Epoch took 10.131s

Epoch 62 of 100
  training loss:		2.664834E-02
  validation loss:		2.647919E-02
Epoch took 10.385s

Epoch 63 of 100
  training loss:		2.656642E-02
  validation loss:		2.692264E-02
Epoch took 11.044s

Epoch 64 of 100
  training loss:		2.647787E-02
  validation loss:		2.665153E-02
Epoch took 8.998s

Epoch 65 of 100
  training loss:		2.651567E-02
  validation loss:		2.646810E-02
Epoch took 9.707s

Epoch 66 of 100
  training loss:		2.664566E-02
  validation loss:		2.677985E-02
Epoch took 9.592s

Epoch 67 of 100
  training loss:		2.658221E-02
  validation loss:		2.639679E-02
Epoch took 9.003s

Epoch 68 of 100
  training loss:		2.651781E-02
  validation loss:		2.685471E-02
Epoch took 10.198s

Epoch 69 of 100
  training loss:		2.661744E-02
  validation loss:		2.698238E-02
Epoch took 9.868s

Epoch 70 of 100
  training loss:		2.658579E-02
  validation loss:		2.682230E-02
Epoch took 9.939s

Early stopping, val-loss increased over the last 10 epochs from 0.0267512728012 to 0.0267823446348
Training RMSE: 1.58649504179e-07
Validation RMSE: 1.58919760881e-07
