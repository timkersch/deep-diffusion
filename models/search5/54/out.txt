Training network with 75120 training samples and 14085 validation samples
Epoch 1 of 100
  training loss:		1.520327E-01
  validation loss:		8.413633E-02
Epoch took 7.990s

Epoch 2 of 100
  training loss:		7.706420E-02
  validation loss:		7.001655E-02
Epoch took 9.322s

Epoch 3 of 100
  training loss:		6.694119E-02
  validation loss:		6.223081E-02
Epoch took 8.522s

Epoch 4 of 100
  training loss:		6.039022E-02
  validation loss:		5.634706E-02
Epoch took 8.526s

Epoch 5 of 100
  training loss:		5.524642E-02
  validation loss:		5.182337E-02
Epoch took 9.155s

Epoch 6 of 100
  training loss:		5.151993E-02
  validation loss:		4.949280E-02
Epoch took 8.697s

Epoch 7 of 100
  training loss:		4.859789E-02
  validation loss:		4.705809E-02
Epoch took 8.782s

Epoch 8 of 100
  training loss:		4.627011E-02
  validation loss:		4.479237E-02
Epoch took 8.101s

Epoch 9 of 100
  training loss:		4.433963E-02
  validation loss:		4.318274E-02
Epoch took 8.849s

Epoch 10 of 100
  training loss:		4.279706E-02
  validation loss:		4.116834E-02
Epoch took 8.967s

Epoch 11 of 100
  training loss:		4.137341E-02
  validation loss:		3.993427E-02
Epoch took 9.027s

Epoch 12 of 100
  training loss:		4.038707E-02
  validation loss:		3.924662E-02
Epoch took 8.648s

Epoch 13 of 100
  training loss:		3.928175E-02
  validation loss:		3.844580E-02
Epoch took 8.791s

Epoch 14 of 100
  training loss:		3.841846E-02
  validation loss:		3.740906E-02
Epoch took 9.155s

Epoch 15 of 100
  training loss:		3.785231E-02
  validation loss:		3.681654E-02
Epoch took 8.637s

Epoch 16 of 100
  training loss:		3.692640E-02
  validation loss:		3.632621E-02
Epoch took 8.371s

Epoch 17 of 100
  training loss:		3.636918E-02
  validation loss:		3.552482E-02
Epoch took 9.237s

Epoch 18 of 100
  training loss:		3.580266E-02
  validation loss:		3.511671E-02
Epoch took 8.998s

Epoch 19 of 100
  training loss:		3.535687E-02
  validation loss:		3.495289E-02
Epoch took 8.397s

Epoch 20 of 100
  training loss:		3.492233E-02
  validation loss:		3.512885E-02
Epoch took 8.784s

Epoch 21 of 100
  training loss:		3.452496E-02
  validation loss:		3.407081E-02
Epoch took 8.446s

Epoch 22 of 100
  training loss:		3.400878E-02
  validation loss:		3.359316E-02
Epoch took 8.741s

Epoch 23 of 100
  training loss:		3.369317E-02
  validation loss:		3.301237E-02
Epoch took 7.998s

Epoch 24 of 100
  training loss:		3.346108E-02
  validation loss:		3.309328E-02
Epoch took 8.678s

Epoch 25 of 100
  training loss:		3.313415E-02
  validation loss:		3.269196E-02
Epoch took 8.348s

Epoch 26 of 100
  training loss:		3.284848E-02
  validation loss:		3.302357E-02
Epoch took 8.649s

Epoch 27 of 100
  training loss:		3.258516E-02
  validation loss:		3.255082E-02
Epoch took 7.866s

Epoch 28 of 100
  training loss:		3.242564E-02
  validation loss:		3.266019E-02
Epoch took 7.546s

Epoch 29 of 100
  training loss:		3.217990E-02
  validation loss:		3.181765E-02
Epoch took 8.755s

Epoch 30 of 100
  training loss:		3.199935E-02
  validation loss:		3.172033E-02
Epoch took 8.786s

Epoch 31 of 100
  training loss:		3.169805E-02
  validation loss:		3.246839E-02
Epoch took 8.040s

Epoch 32 of 100
  training loss:		3.143734E-02
  validation loss:		3.185939E-02
Epoch took 8.757s

Epoch 33 of 100
  training loss:		3.137557E-02
  validation loss:		3.115298E-02
Epoch took 9.639s

Epoch 34 of 100
  training loss:		3.101109E-02
  validation loss:		3.061096E-02
Epoch took 8.438s

Epoch 35 of 100
  training loss:		3.088332E-02
  validation loss:		3.076635E-02
Epoch took 9.561s

Epoch 36 of 100
  training loss:		3.072031E-02
  validation loss:		3.125122E-02
Epoch took 9.189s

Epoch 37 of 100
  training loss:		3.069242E-02
  validation loss:		3.038544E-02
Epoch took 9.011s

Epoch 38 of 100
  training loss:		3.039914E-02
  validation loss:		3.008572E-02
Epoch took 9.567s

Epoch 39 of 100
  training loss:		3.026129E-02
  validation loss:		3.017776E-02
Epoch took 8.056s

Epoch 40 of 100
  training loss:		3.013484E-02
  validation loss:		3.053644E-02
Epoch took 8.434s

Epoch 41 of 100
  training loss:		3.003557E-02
  validation loss:		2.976991E-02
Epoch took 8.513s

Epoch 42 of 100
  training loss:		2.992307E-02
  validation loss:		3.009740E-02
Epoch took 7.816s

Epoch 43 of 100
  training loss:		2.987959E-02
  validation loss:		2.972254E-02
Epoch took 8.268s

Epoch 44 of 100
  training loss:		2.966026E-02
  validation loss:		2.944370E-02
Epoch took 8.587s

Epoch 45 of 100
  training loss:		2.951262E-02
  validation loss:		3.041814E-02
Epoch took 8.207s

Epoch 46 of 100
  training loss:		2.954578E-02
  validation loss:		2.906726E-02
Epoch took 8.005s

Epoch 47 of 100
  training loss:		2.937320E-02
  validation loss:		2.931744E-02
Epoch took 8.661s

Epoch 48 of 100
  training loss:		2.929319E-02
  validation loss:		3.030428E-02
Epoch took 9.202s

Epoch 49 of 100
  training loss:		2.925236E-02
  validation loss:		2.902587E-02
Epoch took 8.641s

Epoch 50 of 100
  training loss:		2.912434E-02
  validation loss:		2.870118E-02
Epoch took 8.067s

Epoch 51 of 100
  training loss:		2.902329E-02
  validation loss:		2.873354E-02
Epoch took 8.395s

Epoch 52 of 100
  training loss:		2.888129E-02
  validation loss:		2.992798E-02
Epoch took 9.097s

Epoch 53 of 100
  training loss:		2.902494E-02
  validation loss:		2.920551E-02
Epoch took 9.153s

Epoch 54 of 100
  training loss:		2.879622E-02
  validation loss:		2.839205E-02
Epoch took 8.685s

Epoch 55 of 100
  training loss:		2.888670E-02
  validation loss:		2.886986E-02
Epoch took 9.354s

Epoch 56 of 100
  training loss:		2.878439E-02
  validation loss:		2.855492E-02
Epoch took 8.550s

Epoch 57 of 100
  training loss:		2.856364E-02
  validation loss:		2.862368E-02
Epoch took 9.186s

Epoch 58 of 100
  training loss:		2.850555E-02
  validation loss:		2.870635E-02
Epoch took 7.751s

Epoch 59 of 100
  training loss:		2.844558E-02
  validation loss:		2.860498E-02
Epoch took 9.375s

Epoch 60 of 100
  training loss:		2.851128E-02
  validation loss:		3.022748E-02
Epoch took 7.648s

Epoch 61 of 100
  training loss:		2.833064E-02
  validation loss:		2.856819E-02
Epoch took 8.722s

Epoch 62 of 100
  training loss:		2.824084E-02
  validation loss:		2.825758E-02
Epoch took 9.503s

Epoch 63 of 100
  training loss:		2.814496E-02
  validation loss:		2.833732E-02
Epoch took 8.931s

Epoch 64 of 100
  training loss:		2.807147E-02
  validation loss:		2.810010E-02
Epoch took 9.657s

Epoch 65 of 100
  training loss:		2.814740E-02
  validation loss:		2.771400E-02
Epoch took 8.573s

Epoch 66 of 100
  training loss:		2.801898E-02
  validation loss:		2.817996E-02
Epoch took 8.325s

Epoch 67 of 100
  training loss:		2.799718E-02
  validation loss:		2.786618E-02
Epoch took 8.667s

Epoch 68 of 100
  training loss:		2.788322E-02
  validation loss:		2.766314E-02
Epoch took 8.643s

Epoch 69 of 100
  training loss:		2.797342E-02
  validation loss:		2.788773E-02
Epoch took 8.899s

Epoch 70 of 100
  training loss:		2.783558E-02
  validation loss:		2.784967E-02
Epoch took 10.247s

Epoch 71 of 100
  training loss:		2.771975E-02
  validation loss:		2.847726E-02
Epoch took 7.927s

Epoch 72 of 100
  training loss:		2.772250E-02
  validation loss:		2.837048E-02
Epoch took 9.090s

Epoch 73 of 100
  training loss:		2.768103E-02
  validation loss:		2.826251E-02
Epoch took 8.903s

Epoch 74 of 100
  training loss:		2.761997E-02
  validation loss:		2.741689E-02
Epoch took 8.346s

Epoch 75 of 100
  training loss:		2.771805E-02
  validation loss:		2.761811E-02
Epoch took 9.164s

Early stopping, val-loss increased over the last 5 epochs from 0.0278893355315 to 0.0280290472028
Training RMSE: 1.63205191578e-07
Validation RMSE: 1.63598122149e-07
