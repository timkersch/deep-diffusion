Training network with 75120 training samples and 14085 validation samples
Epoch 1 of 100
  training loss:		2.325096E-01
  validation loss:		1.163938E-01
Epoch took 8.156s

Epoch 2 of 100
  training loss:		1.028560E-01
  validation loss:		9.225102E-02
Epoch took 7.157s

Epoch 3 of 100
  training loss:		8.623912E-02
  validation loss:		8.025332E-02
Epoch took 8.016s

Epoch 4 of 100
  training loss:		7.656984E-02
  validation loss:		7.264014E-02
Epoch took 7.826s

Epoch 5 of 100
  training loss:		7.014165E-02
  validation loss:		6.771443E-02
Epoch took 8.394s

Epoch 6 of 100
  training loss:		6.558527E-02
  validation loss:		6.297015E-02
Epoch took 7.508s

Epoch 7 of 100
  training loss:		6.172760E-02
  validation loss:		5.893499E-02
Epoch took 9.840s

Epoch 8 of 100
  training loss:		5.848463E-02
  validation loss:		5.654228E-02
Epoch took 8.811s

Epoch 9 of 100
  training loss:		5.584608E-02
  validation loss:		5.381211E-02
Epoch took 8.154s

Epoch 10 of 100
  training loss:		5.346596E-02
  validation loss:		5.142397E-02
Epoch took 7.729s

Epoch 11 of 100
  training loss:		5.142791E-02
  validation loss:		5.040508E-02
Epoch took 7.937s

Epoch 12 of 100
  training loss:		4.962845E-02
  validation loss:		4.822368E-02
Epoch took 7.640s

Epoch 13 of 100
  training loss:		4.805106E-02
  validation loss:		4.688364E-02
Epoch took 7.622s

Epoch 14 of 100
  training loss:		4.663167E-02
  validation loss:		4.544805E-02
Epoch took 8.305s

Epoch 15 of 100
  training loss:		4.538851E-02
  validation loss:		4.455928E-02
Epoch took 8.930s

Epoch 16 of 100
  training loss:		4.415459E-02
  validation loss:		4.306902E-02
Epoch took 7.545s

Epoch 17 of 100
  training loss:		4.303304E-02
  validation loss:		4.176378E-02
Epoch took 8.129s

Epoch 18 of 100
  training loss:		4.195811E-02
  validation loss:		4.163334E-02
Epoch took 7.645s

Epoch 19 of 100
  training loss:		4.106071E-02
  validation loss:		4.050471E-02
Epoch took 8.273s

Epoch 20 of 100
  training loss:		4.033816E-02
  validation loss:		3.943805E-02
Epoch took 7.638s

Epoch 21 of 100
  training loss:		3.943926E-02
  validation loss:		3.938018E-02
Epoch took 8.983s

Epoch 22 of 100
  training loss:		3.890891E-02
  validation loss:		3.796890E-02
Epoch took 8.658s

Epoch 23 of 100
  training loss:		3.805640E-02
  validation loss:		3.820199E-02
Epoch took 7.186s

Epoch 24 of 100
  training loss:		3.754864E-02
  validation loss:		3.727029E-02
Epoch took 6.763s

Epoch 25 of 100
  training loss:		3.679957E-02
  validation loss:		3.659287E-02
Epoch took 6.934s

Epoch 26 of 100
  training loss:		3.615516E-02
  validation loss:		3.591705E-02
Epoch took 8.443s

Epoch 27 of 100
  training loss:		3.579015E-02
  validation loss:		3.524613E-02
Epoch took 8.116s

Epoch 28 of 100
  training loss:		3.524806E-02
  validation loss:		3.513326E-02
Epoch took 7.849s

Epoch 29 of 100
  training loss:		3.471877E-02
  validation loss:		3.488111E-02
Epoch took 7.932s

Epoch 30 of 100
  training loss:		3.432056E-02
  validation loss:		3.398313E-02
Epoch took 7.241s

Epoch 31 of 100
  training loss:		3.407565E-02
  validation loss:		3.386536E-02
Epoch took 7.748s

Epoch 32 of 100
  training loss:		3.374230E-02
  validation loss:		3.320560E-02
Epoch took 8.747s

Epoch 33 of 100
  training loss:		3.326070E-02
  validation loss:		3.315144E-02
Epoch took 9.044s

Epoch 34 of 100
  training loss:		3.315905E-02
  validation loss:		3.284028E-02
Epoch took 7.770s

Epoch 35 of 100
  training loss:		3.271157E-02
  validation loss:		3.230144E-02
Epoch took 8.092s

Epoch 36 of 100
  training loss:		3.236101E-02
  validation loss:		3.200810E-02
Epoch took 7.863s

Epoch 37 of 100
  training loss:		3.219986E-02
  validation loss:		3.263165E-02
Epoch took 8.083s

Epoch 38 of 100
  training loss:		3.193551E-02
  validation loss:		3.152621E-02
Epoch took 7.421s

Epoch 39 of 100
  training loss:		3.157512E-02
  validation loss:		3.136128E-02
Epoch took 7.691s

Epoch 40 of 100
  training loss:		3.142413E-02
  validation loss:		3.147414E-02
Epoch took 8.301s

Epoch 41 of 100
  training loss:		3.129821E-02
  validation loss:		3.115049E-02
Epoch took 8.179s

Epoch 42 of 100
  training loss:		3.098461E-02
  validation loss:		3.074829E-02
Epoch took 8.341s

Epoch 43 of 100
  training loss:		3.080392E-02
  validation loss:		3.125063E-02
Epoch took 6.782s

Epoch 44 of 100
  training loss:		3.065405E-02
  validation loss:		3.092146E-02
Epoch took 6.323s

Epoch 45 of 100
  training loss:		3.050646E-02
  validation loss:		3.065480E-02
Epoch took 6.424s

Epoch 46 of 100
  training loss:		3.033372E-02
  validation loss:		3.014114E-02
Epoch took 6.335s

Epoch 47 of 100
  training loss:		3.026672E-02
  validation loss:		3.064588E-02
Epoch took 6.585s

Epoch 48 of 100
  training loss:		2.994195E-02
  validation loss:		3.004055E-02
Epoch took 7.705s

Epoch 49 of 100
  training loss:		2.983239E-02
  validation loss:		2.961796E-02
Epoch took 8.110s

Epoch 50 of 100
  training loss:		2.974970E-02
  validation loss:		2.960539E-02
Epoch took 8.400s

Epoch 51 of 100
  training loss:		2.954085E-02
  validation loss:		2.967609E-02
Epoch took 7.524s

Epoch 52 of 100
  training loss:		2.947839E-02
  validation loss:		2.925282E-02
Epoch took 9.342s

Epoch 53 of 100
  training loss:		2.941981E-02
  validation loss:		2.930870E-02
Epoch took 7.549s

Epoch 54 of 100
  training loss:		2.922382E-02
  validation loss:		2.953995E-02
Epoch took 8.197s

Epoch 55 of 100
  training loss:		2.914987E-02
  validation loss:		2.925029E-02
Epoch took 8.111s

Epoch 56 of 100
  training loss:		2.919571E-02
  validation loss:		2.898941E-02
Epoch took 7.445s

Epoch 57 of 100
  training loss:		2.886457E-02
  validation loss:		2.895816E-02
Epoch took 7.870s

Epoch 58 of 100
  training loss:		2.886699E-02
  validation loss:		2.913475E-02
Epoch took 7.513s

Epoch 59 of 100
  training loss:		2.878773E-02
  validation loss:		2.863124E-02
Epoch took 8.938s

Epoch 60 of 100
  training loss:		2.860184E-02
  validation loss:		2.852332E-02
Epoch took 8.310s

Epoch 61 of 100
  training loss:		2.852902E-02
  validation loss:		2.861012E-02
Epoch took 7.684s

Epoch 62 of 100
  training loss:		2.856889E-02
  validation loss:		2.852460E-02
Epoch took 7.214s

Epoch 63 of 100
  training loss:		2.846688E-02
  validation loss:		2.861425E-02
Epoch took 7.437s

Epoch 64 of 100
  training loss:		2.832422E-02
  validation loss:		2.841747E-02
Epoch took 8.142s

Epoch 65 of 100
  training loss:		2.828292E-02
  validation loss:		2.825877E-02
Epoch took 8.538s

Epoch 66 of 100
  training loss:		2.821149E-02
  validation loss:		2.848604E-02
Epoch took 8.602s

Epoch 67 of 100
  training loss:		2.818796E-02
  validation loss:		2.812726E-02
Epoch took 7.305s

Epoch 68 of 100
  training loss:		2.810269E-02
  validation loss:		2.814484E-02
Epoch took 7.391s

Epoch 69 of 100
  training loss:		2.803128E-02
  validation loss:		2.787997E-02
Epoch took 7.735s

Epoch 70 of 100
  training loss:		2.799010E-02
  validation loss:		2.857091E-02
Epoch took 8.387s

Epoch 71 of 100
  training loss:		2.794290E-02
  validation loss:		2.778340E-02
Epoch took 8.507s

Epoch 72 of 100
  training loss:		2.787792E-02
  validation loss:		2.777732E-02
Epoch took 7.588s

Epoch 73 of 100
  training loss:		2.779010E-02
  validation loss:		2.785199E-02
Epoch took 8.593s

Epoch 74 of 100
  training loss:		2.778890E-02
  validation loss:		2.813516E-02
Epoch took 9.110s

Epoch 75 of 100
  training loss:		2.775434E-02
  validation loss:		2.776301E-02
Epoch took 8.353s

Epoch 76 of 100
  training loss:		2.798805E-02
  validation loss:		2.778472E-02
Epoch took 8.365s

Epoch 77 of 100
  training loss:		2.771109E-02
  validation loss:		3.009661E-02
Epoch took 7.490s

Epoch 78 of 100
  training loss:		2.757137E-02
  validation loss:		2.775457E-02
Epoch took 8.341s

Epoch 79 of 100
  training loss:		2.754902E-02
  validation loss:		2.793504E-02
Epoch took 7.651s

Epoch 80 of 100
  training loss:		2.749252E-02
  validation loss:		2.818553E-02
Epoch took 8.397s

Early stopping, val-loss increased over the last 5 epochs from 0.027862177298 to 0.028351294631
Training RMSE: 1.62578451695e-07
Validation RMSE: 1.63175784139e-07
