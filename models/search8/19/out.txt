Training network with 56340 training samples and 18780 validation samples
Epoch 1 of 500
  training loss:		1.449172E-01
  validation loss:		2.581048E-01
Epoch took 0.812s

Epoch 2 of 500
  training loss:		7.808938E-02
  validation loss:		7.014084E-02
Epoch took 0.774s

Epoch 3 of 500
  training loss:		6.022130E-02
  validation loss:		4.987618E-02
Epoch took 0.774s

Epoch 4 of 500
  training loss:		5.179309E-02
  validation loss:		5.220523E-02
Epoch took 0.774s

Epoch 5 of 500
  training loss:		4.119262E-02
  validation loss:		4.403027E-02
Epoch took 0.774s

Epoch 6 of 500
  training loss:		3.600785E-02
  validation loss:		3.016120E-02
Epoch took 0.774s

Epoch 7 of 500
  training loss:		3.706978E-02
  validation loss:		4.054020E-02
Epoch took 0.774s

Epoch 8 of 500
  training loss:		3.336807E-02
  validation loss:		4.043723E-02
Epoch took 0.774s

Epoch 9 of 500
  training loss:		3.285041E-02
  validation loss:		2.798475E-02
Epoch took 0.774s

Epoch 10 of 500
  training loss:		2.811620E-02
  validation loss:		3.846619E-02
Epoch took 0.774s

Epoch 11 of 500
  training loss:		2.578836E-02
  validation loss:		2.287559E-02
Epoch took 0.774s

Epoch 12 of 500
  training loss:		2.512974E-02
  validation loss:		1.857669E-02
Epoch took 0.774s

Epoch 13 of 500
  training loss:		2.181203E-02
  validation loss:		1.880124E-02
Epoch took 0.774s

Epoch 14 of 500
  training loss:		2.390577E-02
  validation loss:		2.576180E-02
Epoch took 0.774s

Epoch 15 of 500
  training loss:		2.312936E-02
  validation loss:		2.354593E-02
Epoch took 0.774s

Epoch 16 of 500
  training loss:		1.941645E-02
  validation loss:		1.874687E-02
Epoch took 0.774s

Epoch 17 of 500
  training loss:		1.930724E-02
  validation loss:		1.898982E-02
Epoch took 0.774s

Epoch 18 of 500
  training loss:		1.998332E-02
  validation loss:		1.918570E-02
Epoch took 0.774s

Epoch 19 of 500
  training loss:		2.005274E-02
  validation loss:		2.131630E-02
Epoch took 0.773s

Epoch 20 of 500
  training loss:		2.021925E-02
  validation loss:		1.173049E-02
Epoch took 0.774s

Epoch 21 of 500
  training loss:		1.722762E-02
  validation loss:		1.206501E-02
Epoch took 0.774s

Epoch 22 of 500
  training loss:		1.637416E-02
  validation loss:		1.147640E-02
Epoch took 0.774s

Epoch 23 of 500
  training loss:		1.689300E-02
  validation loss:		1.876292E-02
Epoch took 0.774s

Epoch 24 of 500
  training loss:		1.433789E-02
  validation loss:		1.554732E-02
Epoch took 0.774s

Epoch 25 of 500
  training loss:		1.402874E-02
  validation loss:		1.890188E-02
Epoch took 0.774s

Epoch 26 of 500
  training loss:		1.623968E-02
  validation loss:		3.756555E-02
Epoch took 0.774s

Epoch 27 of 500
  training loss:		1.643629E-02
  validation loss:		9.142984E-03
Epoch took 0.774s

Epoch 28 of 500
  training loss:		1.434757E-02
  validation loss:		1.207984E-02
Epoch took 0.774s

Epoch 29 of 500
  training loss:		1.453589E-02
  validation loss:		9.656098E-03
Epoch took 0.774s

Epoch 30 of 500
  training loss:		1.352150E-02
  validation loss:		9.806549E-03
Epoch took 0.774s

Epoch 31 of 500
  training loss:		1.353587E-02
  validation loss:		1.308502E-02
Epoch took 0.774s

Epoch 32 of 500
  training loss:		1.322942E-02
  validation loss:		1.451132E-02
Epoch took 0.774s

Epoch 33 of 500
  training loss:		1.206567E-02
  validation loss:		9.010371E-03
Epoch took 0.774s

Epoch 34 of 500
  training loss:		1.333135E-02
  validation loss:		1.210899E-02
Epoch took 0.773s

Epoch 35 of 500
  training loss:		1.315828E-02
  validation loss:		1.477758E-02
Epoch took 0.774s

Epoch 36 of 500
  training loss:		1.493549E-02
  validation loss:		1.512530E-02
Epoch took 0.774s

Epoch 37 of 500
  training loss:		1.350250E-02
  validation loss:		7.732825E-03
Epoch took 0.774s

Epoch 38 of 500
  training loss:		1.232727E-02
  validation loss:		1.288565E-02
Epoch took 0.774s

Epoch 39 of 500
  training loss:		1.350718E-02
  validation loss:		1.246336E-02
Epoch took 0.774s

Epoch 40 of 500
  training loss:		1.121820E-02
  validation loss:		9.753722E-03
Epoch took 0.774s

Epoch 41 of 500
  training loss:		1.162223E-02
  validation loss:		1.295799E-02
Epoch took 0.774s

Epoch 42 of 500
  training loss:		1.301345E-02
  validation loss:		1.096576E-02
Epoch took 0.774s

Epoch 43 of 500
  training loss:		1.255197E-02
  validation loss:		1.379338E-02
Epoch took 0.774s

Epoch 44 of 500
  training loss:		1.240279E-02
  validation loss:		1.596564E-02
Epoch took 0.774s

Epoch 45 of 500
  training loss:		1.139926E-02
  validation loss:		1.108774E-02
Epoch took 0.774s

Epoch 46 of 500
  training loss:		1.058890E-02
  validation loss:		6.250467E-03
Epoch took 0.774s

Epoch 47 of 500
  training loss:		1.009424E-02
  validation loss:		8.577254E-03
Epoch took 0.774s

Epoch 48 of 500
  training loss:		1.216046E-02
  validation loss:		1.239284E-02
Epoch took 0.773s

Epoch 49 of 500
  training loss:		1.368473E-02
  validation loss:		7.320696E-03
Epoch took 0.773s

Epoch 50 of 500
  training loss:		1.182277E-02
  validation loss:		1.073864E-02
Epoch took 0.774s

Epoch 51 of 500
  training loss:		9.807547E-03
  validation loss:		8.708582E-03
Epoch took 0.774s

Epoch 52 of 500
  training loss:		9.929520E-03
  validation loss:		7.754214E-03
Epoch took 0.774s

Epoch 53 of 500
  training loss:		1.025705E-02
  validation loss:		5.716032E-03
Epoch took 0.774s

Epoch 54 of 500
  training loss:		1.069920E-02
  validation loss:		6.493577E-03
Epoch took 0.774s

Epoch 55 of 500
  training loss:		9.877788E-03
  validation loss:		7.705667E-03
Epoch took 0.774s

Epoch 56 of 500
  training loss:		1.031571E-02
  validation loss:		9.855864E-03
Epoch took 0.774s

Epoch 57 of 500
  training loss:		9.990421E-03
  validation loss:		7.635066E-03
Epoch took 0.774s

Epoch 58 of 500
  training loss:		9.217268E-03
  validation loss:		1.323541E-02
Epoch took 0.777s

Epoch 59 of 500
  training loss:		8.475960E-03
  validation loss:		8.646555E-03
Epoch took 0.777s

Epoch 60 of 500
  training loss:		1.015355E-02
  validation loss:		9.259722E-03
Epoch took 0.776s

Epoch 61 of 500
  training loss:		1.245337E-02
  validation loss:		6.625622E-03
Epoch took 0.777s

Epoch 62 of 500
  training loss:		8.741906E-03
  validation loss:		5.854831E-03
Epoch took 0.776s

Epoch 63 of 500
  training loss:		9.526602E-03
  validation loss:		7.211326E-03
Epoch took 0.776s

Epoch 64 of 500
  training loss:		9.328670E-03
  validation loss:		9.001499E-03
Epoch took 0.776s

Epoch 65 of 500
  training loss:		9.498954E-03
  validation loss:		9.009465E-03
Epoch took 0.777s

Epoch 66 of 500
  training loss:		9.129253E-03
  validation loss:		1.131756E-02
Epoch took 0.776s

Epoch 67 of 500
  training loss:		8.973064E-03
  validation loss:		8.384903E-03
Epoch took 0.776s

Epoch 68 of 500
  training loss:		8.612008E-03
  validation loss:		1.184478E-02
Epoch took 0.776s

Epoch 69 of 500
  training loss:		9.283560E-03
  validation loss:		1.363891E-02
Epoch took 0.776s

Epoch 70 of 500
  training loss:		1.043431E-02
  validation loss:		8.405065E-03
Epoch took 0.777s

Early stopping, val-loss increased over the last 10 epochs from 0.0085010688946 to 0.00912939649619
Saving model from epoch 60
Training RMSE: 0.00925595
Validation RMSE: 0.00925598
Test RMSE: 0.00919502880424
Test MSE: 8.45485556056e-05
Test MAE: 0.00827477127314
Test R2: -900088616.401 

