Training network with 56340 training samples and 18780 validation samples
Epoch 1 of 500
  training loss:		1.531790E-01
  validation loss:		5.155962E-02
Epoch took 2.057s

Epoch 2 of 500
  training loss:		4.625418E-02
  validation loss:		3.568479E-02
Epoch took 2.015s

Epoch 3 of 500
  training loss:		3.305231E-02
  validation loss:		3.150684E-02
Epoch took 2.016s

Epoch 4 of 500
  training loss:		2.680194E-02
  validation loss:		2.626649E-02
Epoch took 2.015s

Epoch 5 of 500
  training loss:		2.334636E-02
  validation loss:		1.731398E-02
Epoch took 2.015s

Epoch 6 of 500
  training loss:		2.102425E-02
  validation loss:		1.736772E-02
Epoch took 2.015s

Epoch 7 of 500
  training loss:		1.748220E-02
  validation loss:		9.869760E-03
Epoch took 2.015s

Epoch 8 of 500
  training loss:		1.622468E-02
  validation loss:		2.737744E-02
Epoch took 2.015s

Epoch 9 of 500
  training loss:		1.609286E-02
  validation loss:		2.363252E-02
Epoch took 2.015s

Epoch 10 of 500
  training loss:		1.452010E-02
  validation loss:		2.292846E-02
Epoch took 2.014s

Epoch 11 of 500
  training loss:		1.520067E-02
  validation loss:		2.299994E-02
Epoch took 2.015s

Epoch 12 of 500
  training loss:		1.336291E-02
  validation loss:		7.686294E-03
Epoch took 2.015s

Epoch 13 of 500
  training loss:		1.343102E-02
  validation loss:		1.013820E-02
Epoch took 2.015s

Epoch 14 of 500
  training loss:		1.182040E-02
  validation loss:		6.852964E-03
Epoch took 2.015s

Epoch 15 of 500
  training loss:		1.179028E-02
  validation loss:		8.132573E-03
Epoch took 2.015s

Epoch 16 of 500
  training loss:		1.013480E-02
  validation loss:		1.158744E-02
Epoch took 2.017s

Epoch 17 of 500
  training loss:		1.009550E-02
  validation loss:		8.983226E-03
Epoch took 2.018s

Epoch 18 of 500
  training loss:		1.070989E-02
  validation loss:		7.088683E-03
Epoch took 2.015s

Epoch 19 of 500
  training loss:		1.034151E-02
  validation loss:		1.569032E-02
Epoch took 2.015s

Epoch 20 of 500
  training loss:		9.364431E-03
  validation loss:		6.320010E-03
Epoch took 2.015s

Epoch 21 of 500
  training loss:		9.050689E-03
  validation loss:		7.957445E-03
Epoch took 2.015s

Epoch 22 of 500
  training loss:		8.666622E-03
  validation loss:		4.635299E-03
Epoch took 2.015s

Epoch 23 of 500
  training loss:		8.650528E-03
  validation loss:		1.575347E-02
Epoch took 2.015s

Epoch 24 of 500
  training loss:		8.172465E-03
  validation loss:		3.644478E-03
Epoch took 2.016s

Epoch 25 of 500
  training loss:		7.936911E-03
  validation loss:		5.590677E-03
Epoch took 2.015s

Epoch 26 of 500
  training loss:		7.365389E-03
  validation loss:		4.900491E-03
Epoch took 2.015s

Epoch 27 of 500
  training loss:		7.441315E-03
  validation loss:		4.347430E-03
Epoch took 2.015s

Epoch 28 of 500
  training loss:		7.137336E-03
  validation loss:		1.282339E-02
Epoch took 2.015s

Epoch 29 of 500
  training loss:		6.719498E-03
  validation loss:		3.406759E-03
Epoch took 2.015s

Epoch 30 of 500
  training loss:		7.020762E-03
  validation loss:		5.477990E-03
Epoch took 2.015s

Epoch 31 of 500
  training loss:		6.517555E-03
  validation loss:		1.153468E-02
Epoch took 2.015s

Epoch 32 of 500
  training loss:		7.027408E-03
  validation loss:		8.180360E-03
Epoch took 2.015s

Epoch 33 of 500
  training loss:		6.697404E-03
  validation loss:		1.300690E-02
Epoch took 2.016s

Epoch 34 of 500
  training loss:		6.076819E-03
  validation loss:		1.556682E-02
Epoch took 2.015s

Epoch 35 of 500
  training loss:		6.525038E-03
  validation loss:		3.774907E-03
Epoch took 2.015s

Epoch 36 of 500
  training loss:		6.187821E-03
  validation loss:		7.087129E-03
Epoch took 2.015s

Epoch 37 of 500
  training loss:		5.734156E-03
  validation loss:		9.484889E-03
Epoch took 2.015s

Epoch 38 of 500
  training loss:		6.173971E-03
  validation loss:		1.369839E-02
Epoch took 2.015s

Epoch 39 of 500
  training loss:		5.727891E-03
  validation loss:		6.736261E-03
Epoch took 2.015s

Epoch 40 of 500
  training loss:		5.783371E-03
  validation loss:		6.571397E-03
Epoch took 2.015s

Epoch 41 of 500
  training loss:		5.461177E-03
  validation loss:		5.017738E-03
Epoch took 2.015s

Epoch 42 of 500
  training loss:		5.627594E-03
  validation loss:		4.422369E-03
Epoch took 2.015s

Epoch 43 of 500
  training loss:		5.343972E-03
  validation loss:		2.920211E-03
Epoch took 2.015s

Epoch 44 of 500
  training loss:		5.089867E-03
  validation loss:		4.965311E-03
Epoch took 2.015s

Epoch 45 of 500
  training loss:		5.372352E-03
  validation loss:		3.830965E-03
Epoch took 2.015s

Epoch 46 of 500
  training loss:		5.617498E-03
  validation loss:		3.608882E-03
Epoch took 2.015s

Epoch 47 of 500
  training loss:		5.423251E-03
  validation loss:		2.295829E-03
Epoch took 2.015s

Epoch 48 of 500
  training loss:		5.268292E-03
  validation loss:		8.416307E-03
Epoch took 2.016s

Epoch 49 of 500
  training loss:		4.984881E-03
  validation loss:		4.417627E-03
Epoch took 2.015s

Epoch 50 of 500
  training loss:		4.774796E-03
  validation loss:		6.280416E-03
Epoch took 2.015s

Epoch 51 of 500
  training loss:		4.994104E-03
  validation loss:		6.869712E-03
Epoch took 2.015s

Epoch 52 of 500
  training loss:		4.957799E-03
  validation loss:		7.196798E-03
Epoch took 2.015s

Epoch 53 of 500
  training loss:		4.620847E-03
  validation loss:		2.069529E-03
Epoch took 2.015s

Epoch 54 of 500
  training loss:		4.997264E-03
  validation loss:		7.313304E-03
Epoch took 2.015s

Epoch 55 of 500
  training loss:		4.467811E-03
  validation loss:		5.373095E-03
Epoch took 2.015s

Epoch 56 of 500
  training loss:		4.943408E-03
  validation loss:		6.322373E-03
Epoch took 2.015s

Epoch 57 of 500
  training loss:		4.503829E-03
  validation loss:		3.261680E-03
Epoch took 2.015s

Epoch 58 of 500
  training loss:		4.416033E-03
  validation loss:		5.730944E-03
Epoch took 2.015s

Epoch 59 of 500
  training loss:		4.086878E-03
  validation loss:		1.080307E-02
Epoch took 2.015s

Epoch 60 of 500
  training loss:		4.157461E-03
  validation loss:		7.770338E-03
Epoch took 2.015s

Epoch 61 of 500
  training loss:		4.067677E-03
  validation loss:		7.119982E-03
Epoch took 2.015s

Epoch 62 of 500
  training loss:		4.023578E-03
  validation loss:		3.958011E-03
Epoch took 2.015s

Epoch 63 of 500
  training loss:		4.204300E-03
  validation loss:		1.702144E-03
Epoch took 2.016s

Epoch 64 of 500
  training loss:		4.343607E-03
  validation loss:		5.735053E-03
Epoch took 2.015s

Epoch 65 of 500
  training loss:		4.051619E-03
  validation loss:		3.261549E-03
Epoch took 2.015s

Epoch 66 of 500
  training loss:		3.753741E-03
  validation loss:		5.365510E-03
Epoch took 2.015s

Epoch 67 of 500
  training loss:		4.171984E-03
  validation loss:		3.644765E-03
Epoch took 2.015s

Epoch 68 of 500
  training loss:		4.179181E-03
  validation loss:		5.664794E-03
Epoch took 2.016s

Epoch 69 of 500
  training loss:		4.238014E-03
  validation loss:		5.542722E-03
Epoch took 2.015s

Epoch 70 of 500
  training loss:		3.654179E-03
  validation loss:		1.200271E-02
Epoch took 2.015s

Epoch 71 of 500
  training loss:		3.934964E-03
  validation loss:		8.700159E-03
Epoch took 2.015s

Epoch 72 of 500
  training loss:		3.826111E-03
  validation loss:		2.679000E-03
Epoch took 2.015s

Epoch 73 of 500
  training loss:		3.860713E-03
  validation loss:		1.020236E-02
Epoch took 2.015s

Epoch 74 of 500
  training loss:		3.681593E-03
  validation loss:		3.336556E-03
Epoch took 2.015s

Epoch 75 of 500
  training loss:		3.896536E-03
  validation loss:		1.557761E-03
Epoch took 2.015s

Epoch 76 of 500
  training loss:		3.970964E-03
  validation loss:		4.720414E-03
Epoch took 2.015s

Epoch 77 of 500
  training loss:		3.432228E-03
  validation loss:		5.524325E-03
Epoch took 2.015s

Epoch 78 of 500
  training loss:		3.600158E-03
  validation loss:		2.971051E-03
Epoch took 2.015s

Epoch 79 of 500
  training loss:		3.306591E-03
  validation loss:		7.651269E-03
Epoch took 2.015s

Epoch 80 of 500
  training loss:		3.473109E-03
  validation loss:		5.280882E-03
Epoch took 2.016s

Epoch 81 of 500
  training loss:		3.985352E-03
  validation loss:		9.589867E-03
Epoch took 2.015s

Epoch 82 of 500
  training loss:		3.960634E-03
  validation loss:		3.882808E-03
Epoch took 2.015s

Epoch 83 of 500
  training loss:		3.495271E-03
  validation loss:		4.388735E-03
Epoch took 2.015s

Epoch 84 of 500
  training loss:		3.502368E-03
  validation loss:		7.425278E-03
Epoch took 2.015s

Epoch 85 of 500
  training loss:		3.337575E-03
  validation loss:		3.285510E-03
Epoch took 2.015s

Epoch 86 of 500
  training loss:		3.428809E-03
  validation loss:		2.980799E-03
Epoch took 2.015s

Epoch 87 of 500
  training loss:		3.438408E-03
  validation loss:		3.849720E-03
Epoch took 2.015s

Epoch 88 of 500
  training loss:		3.450822E-03
  validation loss:		6.470684E-03
Epoch took 2.015s

Epoch 89 of 500
  training loss:		3.246948E-03
  validation loss:		6.359199E-03
Epoch took 2.015s

Epoch 90 of 500
  training loss:		3.251620E-03
  validation loss:		7.480707E-03
Epoch took 2.016s

Epoch 91 of 500
  training loss:		3.380066E-03
  validation loss:		1.520047E-02
Epoch took 2.016s

Epoch 92 of 500
  training loss:		3.483562E-03
  validation loss:		7.655942E-03
Epoch took 2.016s

Epoch 93 of 500
  training loss:		3.247203E-03
  validation loss:		1.906517E-03
Epoch took 2.016s

Epoch 94 of 500
  training loss:		3.383212E-03
  validation loss:		5.575801E-03
Epoch took 2.016s

Epoch 95 of 500
  training loss:		3.356890E-03
  validation loss:		3.012278E-03
Epoch took 2.018s

Epoch 96 of 500
  training loss:		3.473604E-03
  validation loss:		4.638080E-03
Epoch took 2.016s

Epoch 97 of 500
  training loss:		3.210655E-03
  validation loss:		5.373826E-03
Epoch took 2.017s

Epoch 98 of 500
  training loss:		3.128303E-03
  validation loss:		3.530180E-03
Epoch took 2.017s

Epoch 99 of 500
  training loss:		3.264732E-03
  validation loss:		3.875115E-03
Epoch took 2.016s

Epoch 100 of 500
  training loss:		3.188452E-03
  validation loss:		2.979776E-03
Epoch took 2.016s

Early stopping, val-loss increased over the last 20 epochs from 0.00533105076178 to 0.00547306483682
Saving model from epoch 80
Training RMSE: 0.00535583
Validation RMSE: 0.0053112
Test RMSE: 0.00536245014518
Test MSE: 2.87558705168e-05
Test MAE: 0.00355494930409
Test R2: -306129793.423 

