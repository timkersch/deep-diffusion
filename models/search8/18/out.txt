Training network with 56340 training samples and 18780 validation samples
Epoch 1 of 500
  training loss:		2.458583E-01
  validation loss:		3.768107E-01
Epoch took 0.830s

Epoch 2 of 500
  training loss:		8.513160E-02
  validation loss:		7.911745E-02
Epoch took 0.775s

Epoch 3 of 500
  training loss:		7.071410E-02
  validation loss:		7.366828E-02
Epoch took 0.775s

Epoch 4 of 500
  training loss:		6.744523E-02
  validation loss:		5.920114E-02
Epoch took 0.775s

Epoch 5 of 500
  training loss:		5.997380E-02
  validation loss:		5.860016E-02
Epoch took 0.774s

Epoch 6 of 500
  training loss:		5.294695E-02
  validation loss:		4.774178E-02
Epoch took 0.774s

Epoch 7 of 500
  training loss:		4.911825E-02
  validation loss:		4.619016E-02
Epoch took 0.774s

Epoch 8 of 500
  training loss:		4.577863E-02
  validation loss:		4.266320E-02
Epoch took 0.775s

Epoch 9 of 500
  training loss:		4.193552E-02
  validation loss:		4.992373E-02
Epoch took 0.775s

Epoch 10 of 500
  training loss:		3.982843E-02
  validation loss:		5.824449E-02
Epoch took 0.774s

Epoch 11 of 500
  training loss:		4.127907E-02
  validation loss:		4.021380E-02
Epoch took 0.774s

Epoch 12 of 500
  training loss:		3.822136E-02
  validation loss:		4.716368E-02
Epoch took 0.774s

Epoch 13 of 500
  training loss:		3.483757E-02
  validation loss:		2.830663E-02
Epoch took 0.774s

Epoch 14 of 500
  training loss:		3.438773E-02
  validation loss:		4.068604E-02
Epoch took 0.774s

Epoch 15 of 500
  training loss:		3.415576E-02
  validation loss:		3.824659E-02
Epoch took 0.775s

Epoch 16 of 500
  training loss:		3.200187E-02
  validation loss:		3.041120E-02
Epoch took 0.774s

Epoch 17 of 500
  training loss:		3.225377E-02
  validation loss:		3.364647E-02
Epoch took 0.774s

Epoch 18 of 500
  training loss:		3.115818E-02
  validation loss:		4.243897E-02
Epoch took 0.774s

Epoch 19 of 500
  training loss:		3.061156E-02
  validation loss:		2.596998E-02
Epoch took 0.774s

Epoch 20 of 500
  training loss:		2.765593E-02
  validation loss:		3.455874E-02
Epoch took 0.774s

Epoch 21 of 500
  training loss:		2.752904E-02
  validation loss:		3.534693E-02
Epoch took 0.775s

Epoch 22 of 500
  training loss:		2.625583E-02
  validation loss:		2.734452E-02
Epoch took 0.774s

Epoch 23 of 500
  training loss:		2.556095E-02
  validation loss:		3.021243E-02
Epoch took 0.774s

Epoch 24 of 500
  training loss:		2.540531E-02
  validation loss:		1.996742E-02
Epoch took 0.774s

Epoch 25 of 500
  training loss:		2.611063E-02
  validation loss:		2.209072E-02
Epoch took 0.774s

Epoch 26 of 500
  training loss:		2.370774E-02
  validation loss:		1.602583E-02
Epoch took 0.774s

Epoch 27 of 500
  training loss:		2.352749E-02
  validation loss:		1.983688E-02
Epoch took 0.774s

Epoch 28 of 500
  training loss:		2.414247E-02
  validation loss:		4.204112E-02
Epoch took 0.774s

Epoch 29 of 500
  training loss:		2.187055E-02
  validation loss:		1.748915E-02
Epoch took 0.774s

Epoch 30 of 500
  training loss:		2.139840E-02
  validation loss:		1.924095E-02
Epoch took 0.774s

Epoch 31 of 500
  training loss:		2.094352E-02
  validation loss:		1.961588E-02
Epoch took 0.774s

Epoch 32 of 500
  training loss:		2.301928E-02
  validation loss:		1.978163E-02
Epoch took 0.774s

Epoch 33 of 500
  training loss:		1.940141E-02
  validation loss:		1.353246E-02
Epoch took 0.774s

Epoch 34 of 500
  training loss:		2.032794E-02
  validation loss:		2.162840E-02
Epoch took 0.774s

Epoch 35 of 500
  training loss:		1.993053E-02
  validation loss:		1.404797E-02
Epoch took 0.774s

Epoch 36 of 500
  training loss:		1.874224E-02
  validation loss:		1.805730E-02
Epoch took 0.774s

Epoch 37 of 500
  training loss:		1.879471E-02
  validation loss:		1.659662E-02
Epoch took 0.774s

Epoch 38 of 500
  training loss:		1.828306E-02
  validation loss:		2.235528E-02
Epoch took 0.774s

Epoch 39 of 500
  training loss:		1.699695E-02
  validation loss:		1.792754E-02
Epoch took 0.774s

Epoch 40 of 500
  training loss:		1.652018E-02
  validation loss:		1.585846E-02
Epoch took 0.774s

Epoch 41 of 500
  training loss:		1.798764E-02
  validation loss:		1.593132E-02
Epoch took 0.775s

Epoch 42 of 500
  training loss:		1.886962E-02
  validation loss:		1.350226E-02
Epoch took 0.774s

Epoch 43 of 500
  training loss:		1.944954E-02
  validation loss:		1.685891E-02
Epoch took 0.774s

Epoch 44 of 500
  training loss:		1.731182E-02
  validation loss:		1.574986E-02
Epoch took 0.774s

Epoch 45 of 500
  training loss:		1.732365E-02
  validation loss:		1.366877E-02
Epoch took 0.774s

Epoch 46 of 500
  training loss:		1.583099E-02
  validation loss:		1.454208E-02
Epoch took 0.774s

Epoch 47 of 500
  training loss:		1.516003E-02
  validation loss:		1.308493E-02
Epoch took 0.777s

Epoch 48 of 500
  training loss:		1.511409E-02
  validation loss:		1.436576E-02
Epoch took 0.775s

Epoch 49 of 500
  training loss:		1.662777E-02
  validation loss:		1.731121E-02
Epoch took 0.774s

Epoch 50 of 500
  training loss:		1.441879E-02
  validation loss:		1.454851E-02
Epoch took 0.774s

Epoch 51 of 500
  training loss:		1.721728E-02
  validation loss:		1.445919E-02
Epoch took 0.774s

Epoch 52 of 500
  training loss:		1.728626E-02
  validation loss:		1.847995E-02
Epoch took 0.774s

Epoch 53 of 500
  training loss:		1.525578E-02
  validation loss:		1.005451E-02
Epoch took 0.774s

Epoch 54 of 500
  training loss:		1.472030E-02
  validation loss:		1.343216E-02
Epoch took 0.775s

Epoch 55 of 500
  training loss:		1.356319E-02
  validation loss:		1.173369E-02
Epoch took 0.774s

Epoch 56 of 500
  training loss:		1.329420E-02
  validation loss:		1.320963E-02
Epoch took 0.774s

Epoch 57 of 500
  training loss:		1.403003E-02
  validation loss:		1.105601E-02
Epoch took 0.774s

Epoch 58 of 500
  training loss:		1.444596E-02
  validation loss:		1.397737E-02
Epoch took 0.775s

Epoch 59 of 500
  training loss:		1.253808E-02
  validation loss:		1.117769E-02
Epoch took 0.777s

Epoch 60 of 500
  training loss:		1.315712E-02
  validation loss:		1.158924E-02
Epoch took 0.776s

Epoch 61 of 500
  training loss:		1.283425E-02
  validation loss:		1.144194E-02
Epoch took 0.777s

Epoch 62 of 500
  training loss:		1.417597E-02
  validation loss:		1.541837E-02
Epoch took 0.777s

Epoch 63 of 500
  training loss:		1.412928E-02
  validation loss:		1.458985E-02
Epoch took 0.777s

Epoch 64 of 500
  training loss:		1.206201E-02
  validation loss:		9.358398E-03
Epoch took 0.777s

Epoch 65 of 500
  training loss:		1.199333E-02
  validation loss:		9.010509E-03
Epoch took 0.777s

Epoch 66 of 500
  training loss:		1.418133E-02
  validation loss:		1.014518E-02
Epoch took 0.777s

Epoch 67 of 500
  training loss:		1.176185E-02
  validation loss:		1.109191E-02
Epoch took 0.777s

Epoch 68 of 500
  training loss:		1.233531E-02
  validation loss:		1.556993E-02
Epoch took 0.777s

Epoch 69 of 500
  training loss:		1.173367E-02
  validation loss:		1.154862E-02
Epoch took 0.776s

Epoch 70 of 500
  training loss:		1.278968E-02
  validation loss:		1.575240E-02
Epoch took 0.777s

Epoch 71 of 500
  training loss:		1.190739E-02
  validation loss:		1.231573E-02
Epoch took 0.777s

Epoch 72 of 500
  training loss:		1.182491E-02
  validation loss:		1.468238E-02
Epoch took 0.777s

Epoch 73 of 500
  training loss:		1.210371E-02
  validation loss:		1.460849E-02
Epoch took 0.777s

Epoch 74 of 500
  training loss:		1.335990E-02
  validation loss:		1.441924E-02
Epoch took 0.777s

Epoch 75 of 500
  training loss:		1.230140E-02
  validation loss:		1.076516E-02
Epoch took 0.776s

Epoch 76 of 500
  training loss:		1.105966E-02
  validation loss:		1.023999E-02
Epoch took 0.776s

Epoch 77 of 500
  training loss:		1.159099E-02
  validation loss:		1.172836E-02
Epoch took 0.777s

Epoch 78 of 500
  training loss:		1.166005E-02
  validation loss:		1.047713E-02
Epoch took 0.777s

Epoch 79 of 500
  training loss:		1.172764E-02
  validation loss:		9.380999E-03
Epoch took 0.777s

Epoch 80 of 500
  training loss:		1.025368E-02
  validation loss:		8.738451E-03
Epoch took 0.777s

Epoch 81 of 500
  training loss:		1.122777E-02
  validation loss:		9.272500E-03
Epoch took 0.777s

Epoch 82 of 500
  training loss:		1.131186E-02
  validation loss:		8.447170E-03
Epoch took 0.777s

Epoch 83 of 500
  training loss:		9.651398E-03
  validation loss:		9.090757E-03
Epoch took 0.777s

Epoch 84 of 500
  training loss:		1.069368E-02
  validation loss:		8.547571E-03
Epoch took 0.777s

Epoch 85 of 500
  training loss:		1.126709E-02
  validation loss:		1.006764E-02
Epoch took 0.777s

Epoch 86 of 500
  training loss:		1.106448E-02
  validation loss:		1.017486E-02
Epoch took 0.777s

Epoch 87 of 500
  training loss:		1.043928E-02
  validation loss:		7.747958E-03
Epoch took 0.777s

Epoch 88 of 500
  training loss:		1.160429E-02
  validation loss:		9.055296E-03
Epoch took 0.777s

Epoch 89 of 500
  training loss:		1.077716E-02
  validation loss:		9.142827E-03
Epoch took 0.777s

Epoch 90 of 500
  training loss:		9.198173E-03
  validation loss:		8.776054E-03
Epoch took 0.776s

Epoch 91 of 500
  training loss:		1.160718E-02
  validation loss:		1.050629E-02
Epoch took 0.777s

Epoch 92 of 500
  training loss:		9.390899E-03
  validation loss:		5.562776E-03
Epoch took 0.777s

Epoch 93 of 500
  training loss:		8.987286E-03
  validation loss:		1.075813E-02
Epoch took 0.777s

Epoch 94 of 500
  training loss:		1.019919E-02
  validation loss:		9.363493E-03
Epoch took 0.777s

Epoch 95 of 500
  training loss:		9.405153E-03
  validation loss:		7.136977E-03
Epoch took 0.777s

Epoch 96 of 500
  training loss:		8.023385E-03
  validation loss:		7.613520E-03
Epoch took 0.777s

Epoch 97 of 500
  training loss:		9.356989E-03
  validation loss:		1.543875E-02
Epoch took 0.777s

Epoch 98 of 500
  training loss:		9.715706E-03
  validation loss:		1.131862E-02
Epoch took 0.777s

Epoch 99 of 500
  training loss:		8.719286E-03
  validation loss:		7.253461E-03
Epoch took 0.777s

Epoch 100 of 500
  training loss:		8.241422E-03
  validation loss:		5.922347E-03
Epoch took 0.777s

Early stopping, val-loss increased over the last 10 epochs from 0.00903226339595 to 0.00908743641469
Saving model from epoch 90
Training RMSE: 0.00870597
Validation RMSE: 0.00877643
Test RMSE: 0.00878864433616
Test MSE: 7.72402709117e-05
Test MAE: 0.00710884062573
Test R2: -822285891.214 

