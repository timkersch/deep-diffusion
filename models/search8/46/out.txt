Training network with 56340 training samples and 18780 validation samples
Epoch 1 of 500
  training loss:		1.510047E-01
  validation loss:		3.189208E-01
Epoch took 0.838s

Epoch 2 of 500
  training loss:		7.990039E-02
  validation loss:		1.045709E-01
Epoch took 0.774s

Epoch 3 of 500
  training loss:		6.642778E-02
  validation loss:		6.380468E-02
Epoch took 0.774s

Epoch 4 of 500
  training loss:		5.736466E-02
  validation loss:		5.685827E-02
Epoch took 0.775s

Epoch 5 of 500
  training loss:		5.164380E-02
  validation loss:		4.406326E-02
Epoch took 0.774s

Epoch 6 of 500
  training loss:		5.440094E-02
  validation loss:		5.902322E-02
Epoch took 0.774s

Epoch 7 of 500
  training loss:		4.843859E-02
  validation loss:		4.536317E-02
Epoch took 0.774s

Epoch 8 of 500
  training loss:		4.441508E-02
  validation loss:		3.723223E-02
Epoch took 0.774s

Epoch 9 of 500
  training loss:		4.327892E-02
  validation loss:		4.082808E-02
Epoch took 0.774s

Epoch 10 of 500
  training loss:		3.542148E-02
  validation loss:		4.160173E-02
Epoch took 0.774s

Epoch 11 of 500
  training loss:		3.576277E-02
  validation loss:		3.653183E-02
Epoch took 0.774s

Epoch 12 of 500
  training loss:		3.164967E-02
  validation loss:		3.571978E-02
Epoch took 0.774s

Epoch 13 of 500
  training loss:		3.166530E-02
  validation loss:		2.851343E-02
Epoch took 0.774s

Epoch 14 of 500
  training loss:		3.127120E-02
  validation loss:		2.506680E-02
Epoch took 0.774s

Epoch 15 of 500
  training loss:		2.857771E-02
  validation loss:		2.099540E-02
Epoch took 0.774s

Epoch 16 of 500
  training loss:		3.004357E-02
  validation loss:		2.366304E-02
Epoch took 0.774s

Epoch 17 of 500
  training loss:		2.607437E-02
  validation loss:		1.978219E-02
Epoch took 0.774s

Epoch 18 of 500
  training loss:		2.781607E-02
  validation loss:		3.048385E-02
Epoch took 0.774s

Epoch 19 of 500
  training loss:		2.487195E-02
  validation loss:		1.998140E-02
Epoch took 0.774s

Epoch 20 of 500
  training loss:		2.594118E-02
  validation loss:		2.762565E-02
Epoch took 0.774s

Epoch 21 of 500
  training loss:		2.419972E-02
  validation loss:		1.965840E-02
Epoch took 0.774s

Epoch 22 of 500
  training loss:		2.102249E-02
  validation loss:		3.066516E-02
Epoch took 0.774s

Epoch 23 of 500
  training loss:		2.259200E-02
  validation loss:		1.925455E-02
Epoch took 0.774s

Epoch 24 of 500
  training loss:		2.081726E-02
  validation loss:		2.018213E-02
Epoch took 0.774s

Epoch 25 of 500
  training loss:		2.230758E-02
  validation loss:		1.718106E-02
Epoch took 0.774s

Epoch 26 of 500
  training loss:		2.005032E-02
  validation loss:		4.168392E-02
Epoch took 0.774s

Epoch 27 of 500
  training loss:		2.163367E-02
  validation loss:		1.688912E-02
Epoch took 0.774s

Epoch 28 of 500
  training loss:		1.963981E-02
  validation loss:		2.153509E-02
Epoch took 0.774s

Epoch 29 of 500
  training loss:		2.052363E-02
  validation loss:		1.463924E-02
Epoch took 0.774s

Epoch 30 of 500
  training loss:		2.237871E-02
  validation loss:		1.511142E-02
Epoch took 0.774s

Epoch 31 of 500
  training loss:		1.924864E-02
  validation loss:		2.638613E-02
Epoch took 0.774s

Epoch 32 of 500
  training loss:		1.907389E-02
  validation loss:		1.610575E-02
Epoch took 0.774s

Epoch 33 of 500
  training loss:		1.728749E-02
  validation loss:		1.455163E-02
Epoch took 0.774s

Epoch 34 of 500
  training loss:		1.819908E-02
  validation loss:		1.377120E-02
Epoch took 0.774s

Epoch 35 of 500
  training loss:		1.757491E-02
  validation loss:		1.396494E-02
Epoch took 0.774s

Epoch 36 of 500
  training loss:		1.845516E-02
  validation loss:		1.264680E-02
Epoch took 0.774s

Epoch 37 of 500
  training loss:		1.496617E-02
  validation loss:		1.008324E-02
Epoch took 0.774s

Epoch 38 of 500
  training loss:		1.562295E-02
  validation loss:		1.580068E-02
Epoch took 0.774s

Epoch 39 of 500
  training loss:		1.533179E-02
  validation loss:		1.197075E-02
Epoch took 0.774s

Epoch 40 of 500
  training loss:		1.385688E-02
  validation loss:		1.837444E-02
Epoch took 0.774s

Epoch 41 of 500
  training loss:		1.405787E-02
  validation loss:		1.555542E-02
Epoch took 0.774s

Epoch 42 of 500
  training loss:		1.596549E-02
  validation loss:		1.128435E-02
Epoch took 0.774s

Epoch 43 of 500
  training loss:		1.297370E-02
  validation loss:		1.299678E-02
Epoch took 0.775s

Epoch 44 of 500
  training loss:		1.511705E-02
  validation loss:		2.102380E-02
Epoch took 0.774s

Epoch 45 of 500
  training loss:		1.363016E-02
  validation loss:		1.412007E-02
Epoch took 0.773s

Epoch 46 of 500
  training loss:		1.263362E-02
  validation loss:		1.362139E-02
Epoch took 0.774s

Epoch 47 of 500
  training loss:		1.402061E-02
  validation loss:		1.565128E-02
Epoch took 0.774s

Epoch 48 of 500
  training loss:		1.379218E-02
  validation loss:		1.125306E-02
Epoch took 0.774s

Epoch 49 of 500
  training loss:		1.273474E-02
  validation loss:		1.427567E-02
Epoch took 0.774s

Epoch 50 of 500
  training loss:		1.379087E-02
  validation loss:		1.299196E-02
Epoch took 0.774s

Epoch 51 of 500
  training loss:		1.396378E-02
  validation loss:		1.453021E-02
Epoch took 0.774s

Epoch 52 of 500
  training loss:		1.206983E-02
  validation loss:		8.404923E-03
Epoch took 0.774s

Epoch 53 of 500
  training loss:		1.222886E-02
  validation loss:		8.926598E-03
Epoch took 0.774s

Epoch 54 of 500
  training loss:		1.170455E-02
  validation loss:		9.767683E-03
Epoch took 0.774s

Epoch 55 of 500
  training loss:		1.222848E-02
  validation loss:		1.584836E-02
Epoch took 0.774s

Epoch 56 of 500
  training loss:		1.134784E-02
  validation loss:		1.100378E-02
Epoch took 0.774s

Epoch 57 of 500
  training loss:		1.185230E-02
  validation loss:		1.278633E-02
Epoch took 0.774s

Epoch 58 of 500
  training loss:		1.230225E-02
  validation loss:		2.009939E-02
Epoch took 0.774s

Epoch 59 of 500
  training loss:		1.111933E-02
  validation loss:		1.001274E-02
Epoch took 0.774s

Epoch 60 of 500
  training loss:		1.046312E-02
  validation loss:		9.722556E-03
Epoch took 0.774s

Epoch 61 of 500
  training loss:		1.145678E-02
  validation loss:		7.531499E-03
Epoch took 0.774s

Epoch 62 of 500
  training loss:		1.085153E-02
  validation loss:		6.896529E-03
Epoch took 0.774s

Epoch 63 of 500
  training loss:		9.505766E-03
  validation loss:		7.705208E-03
Epoch took 0.774s

Epoch 64 of 500
  training loss:		9.766034E-03
  validation loss:		1.171187E-02
Epoch took 0.774s

Epoch 65 of 500
  training loss:		1.217796E-02
  validation loss:		1.223530E-02
Epoch took 0.773s

Epoch 66 of 500
  training loss:		9.577645E-03
  validation loss:		7.069065E-03
Epoch took 0.773s

Epoch 67 of 500
  training loss:		1.043652E-02
  validation loss:		1.051810E-02
Epoch took 0.774s

Epoch 68 of 500
  training loss:		9.596164E-03
  validation loss:		1.169978E-02
Epoch took 0.774s

Epoch 69 of 500
  training loss:		9.053073E-03
  validation loss:		7.067089E-03
Epoch took 0.774s

Epoch 70 of 500
  training loss:		1.042728E-02
  validation loss:		7.971494E-03
Epoch took 0.774s

Epoch 71 of 500
  training loss:		9.706810E-03
  validation loss:		5.546253E-03
Epoch took 0.774s

Epoch 72 of 500
  training loss:		9.723888E-03
  validation loss:		6.821753E-03
Epoch took 0.774s

Epoch 73 of 500
  training loss:		8.767341E-03
  validation loss:		8.027399E-03
Epoch took 0.776s

Epoch 74 of 500
  training loss:		8.866252E-03
  validation loss:		9.757600E-03
Epoch took 0.776s

Epoch 75 of 500
  training loss:		9.693150E-03
  validation loss:		7.394971E-03
Epoch took 0.776s

Epoch 76 of 500
  training loss:		7.930556E-03
  validation loss:		7.529597E-03
Epoch took 0.776s

Epoch 77 of 500
  training loss:		9.831700E-03
  validation loss:		6.348394E-03
Epoch took 0.776s

Epoch 78 of 500
  training loss:		9.909576E-03
  validation loss:		8.412792E-03
Epoch took 0.776s

Epoch 79 of 500
  training loss:		1.034586E-02
  validation loss:		9.464767E-03
Epoch took 0.776s

Epoch 80 of 500
  training loss:		8.291378E-03
  validation loss:		9.662875E-03
Epoch took 0.776s

Epoch 81 of 500
  training loss:		7.886050E-03
  validation loss:		5.509912E-03
Epoch took 0.777s

Epoch 82 of 500
  training loss:		8.920365E-03
  validation loss:		9.692595E-03
Epoch took 0.776s

Epoch 83 of 500
  training loss:		8.216448E-03
  validation loss:		5.422595E-03
Epoch took 0.776s

Epoch 84 of 500
  training loss:		9.292392E-03
  validation loss:		5.949995E-03
Epoch took 0.776s

Epoch 85 of 500
  training loss:		9.391848E-03
  validation loss:		8.487007E-03
Epoch took 0.776s

Epoch 86 of 500
  training loss:		8.005303E-03
  validation loss:		6.673521E-03
Epoch took 0.776s

Epoch 87 of 500
  training loss:		7.489629E-03
  validation loss:		6.945642E-03
Epoch took 0.776s

Epoch 88 of 500
  training loss:		9.199530E-03
  validation loss:		1.161103E-02
Epoch took 0.777s

Epoch 89 of 500
  training loss:		1.005751E-02
  validation loss:		6.588766E-03
Epoch took 0.776s

Epoch 90 of 500
  training loss:		7.032910E-03
  validation loss:		8.703951E-03
Epoch took 0.776s

Epoch 91 of 500
  training loss:		7.517787E-03
  validation loss:		4.696113E-03
Epoch took 0.777s

Epoch 92 of 500
  training loss:		8.544144E-03
  validation loss:		7.286822E-03
Epoch took 0.777s

Epoch 93 of 500
  training loss:		7.464995E-03
  validation loss:		1.065247E-02
Epoch took 0.777s

Epoch 94 of 500
  training loss:		7.818604E-03
  validation loss:		4.391627E-03
Epoch took 0.777s

Epoch 95 of 500
  training loss:		8.187326E-03
  validation loss:		1.526721E-02
Epoch took 0.776s

Epoch 96 of 500
  training loss:		8.240101E-03
  validation loss:		6.963656E-03
Epoch took 0.776s

Epoch 97 of 500
  training loss:		7.334288E-03
  validation loss:		9.402765E-03
Epoch took 0.776s

Epoch 98 of 500
  training loss:		6.915797E-03
  validation loss:		6.207027E-03
Epoch took 0.776s

Epoch 99 of 500
  training loss:		8.216053E-03
  validation loss:		1.005278E-02
Epoch took 0.776s

Epoch 100 of 500
  training loss:		7.620093E-03
  validation loss:		8.942308E-03
Epoch took 0.776s

Epoch 101 of 500
  training loss:		7.901181E-03
  validation loss:		8.035375E-03
Epoch took 0.776s

Epoch 102 of 500
  training loss:		9.731364E-03
  validation loss:		3.374765E-03
Epoch took 0.776s

Epoch 103 of 500
  training loss:		7.141606E-03
  validation loss:		6.376646E-03
Epoch took 0.776s

Epoch 104 of 500
  training loss:		8.978277E-03
  validation loss:		3.568126E-03
Epoch took 0.776s

Epoch 105 of 500
  training loss:		8.003601E-03
  validation loss:		6.764561E-03
Epoch took 0.776s

Epoch 106 of 500
  training loss:		6.856601E-03
  validation loss:		7.794339E-03
Epoch took 0.776s

Epoch 107 of 500
  training loss:		7.179672E-03
  validation loss:		8.281729E-03
Epoch took 0.776s

Epoch 108 of 500
  training loss:		7.444020E-03
  validation loss:		6.606950E-03
Epoch took 0.777s

Epoch 109 of 500
  training loss:		7.537230E-03
  validation loss:		5.436063E-03
Epoch took 0.776s

Epoch 110 of 500
  training loss:		6.997033E-03
  validation loss:		4.402205E-03
Epoch took 0.776s

Epoch 111 of 500
  training loss:		8.111932E-03
  validation loss:		6.596334E-03
Epoch took 0.776s

Epoch 112 of 500
  training loss:		6.457137E-03
  validation loss:		9.310766E-03
Epoch took 0.776s

Epoch 113 of 500
  training loss:		8.122002E-03
  validation loss:		8.786532E-03
Epoch took 0.776s

Epoch 114 of 500
  training loss:		8.429941E-03
  validation loss:		5.891165E-03
Epoch took 0.776s

Epoch 115 of 500
  training loss:		6.466422E-03
  validation loss:		5.917052E-03
Epoch took 0.776s

Epoch 116 of 500
  training loss:		5.778001E-03
  validation loss:		6.726500E-03
Epoch took 0.776s

Epoch 117 of 500
  training loss:		6.805657E-03
  validation loss:		1.016781E-02
Epoch took 0.776s

Epoch 118 of 500
  training loss:		7.664931E-03
  validation loss:		4.075781E-03
Epoch took 0.777s

Epoch 119 of 500
  training loss:		6.837702E-03
  validation loss:		5.244643E-03
Epoch took 0.776s

Epoch 120 of 500
  training loss:		5.924479E-03
  validation loss:		6.009082E-03
Epoch took 0.777s

Epoch 121 of 500
  training loss:		6.126323E-03
  validation loss:		7.182381E-03
Epoch took 0.777s

Epoch 122 of 500
  training loss:		6.575629E-03
  validation loss:		6.333593E-03
Epoch took 0.776s

Epoch 123 of 500
  training loss:		7.570394E-03
  validation loss:		4.456768E-03
Epoch took 0.776s

Epoch 124 of 500
  training loss:		6.010466E-03
  validation loss:		5.541387E-03
Epoch took 0.776s

Epoch 125 of 500
  training loss:		6.750675E-03
  validation loss:		5.629387E-03
Epoch took 0.776s

Epoch 126 of 500
  training loss:		6.129387E-03
  validation loss:		1.129202E-02
Epoch took 0.776s

Epoch 127 of 500
  training loss:		6.149414E-03
  validation loss:		5.548662E-03
Epoch took 0.776s

Epoch 128 of 500
  training loss:		5.974635E-03
  validation loss:		5.348848E-03
Epoch took 0.776s

Epoch 129 of 500
  training loss:		5.772361E-03
  validation loss:		5.012521E-03
Epoch took 0.776s

Epoch 130 of 500
  training loss:		6.262782E-03
  validation loss:		3.634442E-03
Epoch took 0.776s

Epoch 131 of 500
  training loss:		6.749270E-03
  validation loss:		5.945401E-03
Epoch took 0.776s

Epoch 132 of 500
  training loss:		5.751437E-03
  validation loss:		4.439508E-03
Epoch took 0.776s

Epoch 133 of 500
  training loss:		6.221285E-03
  validation loss:		5.329444E-03
Epoch took 0.776s

Epoch 134 of 500
  training loss:		6.302082E-03
  validation loss:		6.282540E-03
Epoch took 0.776s

Epoch 135 of 500
  training loss:		6.987670E-03
  validation loss:		3.319433E-03
Epoch took 0.776s

Epoch 136 of 500
  training loss:		6.120952E-03
  validation loss:		1.345075E-02
Epoch took 0.777s

Epoch 137 of 500
  training loss:		5.430886E-03
  validation loss:		6.851259E-03
Epoch took 0.776s

Epoch 138 of 500
  training loss:		6.154052E-03
  validation loss:		3.324001E-03
Epoch took 0.776s

Epoch 139 of 500
  training loss:		5.540194E-03
  validation loss:		3.028277E-03
Epoch took 0.776s

Epoch 140 of 500
  training loss:		5.864982E-03
  validation loss:		6.721038E-03
Epoch took 0.776s

Epoch 141 of 500
  training loss:		6.118455E-03
  validation loss:		4.456273E-03
Epoch took 0.776s

Epoch 142 of 500
  training loss:		5.518303E-03
  validation loss:		5.822739E-03
Epoch took 0.776s

Epoch 143 of 500
  training loss:		7.361732E-03
  validation loss:		7.615229E-03
Epoch took 0.776s

Epoch 144 of 500
  training loss:		5.370893E-03
  validation loss:		5.673408E-03
Epoch took 0.776s

Epoch 145 of 500
  training loss:		7.257071E-03
  validation loss:		5.893950E-03
Epoch took 0.776s

Epoch 146 of 500
  training loss:		6.475547E-03
  validation loss:		8.831586E-03
Epoch took 0.777s

Epoch 147 of 500
  training loss:		5.967368E-03
  validation loss:		3.592210E-03
Epoch took 0.776s

Epoch 148 of 500
  training loss:		6.270943E-03
  validation loss:		1.393640E-02
Epoch took 0.776s

Epoch 149 of 500
  training loss:		5.835424E-03
  validation loss:		3.219273E-03
Epoch took 0.776s

Epoch 150 of 500
  training loss:		5.997184E-03
  validation loss:		9.549050E-03
Epoch took 0.776s

Early stopping, val-loss increased over the last 15 epochs from 0.00568642226367 to 0.00679769582995
Saving model from epoch 135
Training RMSE: 0.00332014
Validation RMSE: 0.0033196
Test RMSE: 0.00333293713629
Test MSE: 1.1108470062e-05
Test MAE: 0.00247565493919
Test R2: -118258751.5 

