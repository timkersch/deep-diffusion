Training network with 56340 training samples and 18780 validation samples
Epoch 1 of 500
  training loss:		2.685127E-01
  validation loss:		2.122424E+01
Epoch took 0.739s

Epoch 2 of 500
  training loss:		6.331156E-02
  validation loss:		8.526783E-02
Epoch took 0.701s

Epoch 3 of 500
  training loss:		5.437984E-02
  validation loss:		2.726644E-02
Epoch took 0.702s

Epoch 4 of 500
  training loss:		5.083695E-02
  validation loss:		5.176633E-02
Epoch took 0.702s

Epoch 5 of 500
  training loss:		3.993942E-02
  validation loss:		4.165507E-02
Epoch took 0.702s

Epoch 6 of 500
  training loss:		3.728499E-02
  validation loss:		4.244955E-02
Epoch took 0.701s

Epoch 7 of 500
  training loss:		3.441907E-02
  validation loss:		3.166277E-02
Epoch took 0.702s

Epoch 8 of 500
  training loss:		3.184679E-02
  validation loss:		4.771574E-02
Epoch took 0.701s

Epoch 9 of 500
  training loss:		3.312768E-02
  validation loss:		3.061622E-02
Epoch took 0.701s

Epoch 10 of 500
  training loss:		2.901060E-02
  validation loss:		1.863623E-02
Epoch took 0.702s

Epoch 11 of 500
  training loss:		2.544431E-02
  validation loss:		2.573090E-02
Epoch took 0.701s

Epoch 12 of 500
  training loss:		2.598489E-02
  validation loss:		1.382057E-02
Epoch took 0.702s

Epoch 13 of 500
  training loss:		2.510212E-02
  validation loss:		2.915773E-02
Epoch took 0.701s

Epoch 14 of 500
  training loss:		2.460317E-02
  validation loss:		1.991692E-02
Epoch took 0.702s

Epoch 15 of 500
  training loss:		2.708926E-02
  validation loss:		3.361863E-02
Epoch took 0.701s

Epoch 16 of 500
  training loss:		2.484947E-02
  validation loss:		2.059354E-02
Epoch took 0.701s

Epoch 17 of 500
  training loss:		2.095561E-02
  validation loss:		1.654128E-02
Epoch took 0.701s

Epoch 18 of 500
  training loss:		2.326614E-02
  validation loss:		2.713930E-02
Epoch took 0.701s

Epoch 19 of 500
  training loss:		2.244183E-02
  validation loss:		1.874005E-02
Epoch took 0.702s

Epoch 20 of 500
  training loss:		2.140198E-02
  validation loss:		2.334534E-02
Epoch took 0.701s

Epoch 21 of 500
  training loss:		2.174470E-02
  validation loss:		2.245340E-02
Epoch took 0.701s

Epoch 22 of 500
  training loss:		2.039872E-02
  validation loss:		1.321167E-02
Epoch took 0.701s

Epoch 23 of 500
  training loss:		2.054540E-02
  validation loss:		1.935544E-02
Epoch took 0.702s

Epoch 24 of 500
  training loss:		2.021608E-02
  validation loss:		2.448209E-02
Epoch took 0.702s

Epoch 25 of 500
  training loss:		1.798366E-02
  validation loss:		2.667617E-02
Epoch took 0.701s

Epoch 26 of 500
  training loss:		1.828578E-02
  validation loss:		1.617358E-02
Epoch took 0.701s

Epoch 27 of 500
  training loss:		1.600497E-02
  validation loss:		3.423089E-02
Epoch took 0.702s

Epoch 28 of 500
  training loss:		2.088215E-02
  validation loss:		2.619772E-02
Epoch took 0.701s

Epoch 29 of 500
  training loss:		2.674116E-02
  validation loss:		2.722271E-02
Epoch took 0.701s

Epoch 30 of 500
  training loss:		2.144928E-02
  validation loss:		2.537003E-02
Epoch took 0.701s

Epoch 31 of 500
  training loss:		1.791990E-02
  validation loss:		1.883272E-02
Epoch took 0.701s

Epoch 32 of 500
  training loss:		1.716599E-02
  validation loss:		1.840501E-02
Epoch took 0.701s

Epoch 33 of 500
  training loss:		1.689030E-02
  validation loss:		1.839859E-02
Epoch took 0.701s

Epoch 34 of 500
  training loss:		1.875220E-02
  validation loss:		6.883850E-03
Epoch took 0.704s

Epoch 35 of 500
  training loss:		1.564606E-02
  validation loss:		2.126602E-02
Epoch took 0.704s

Epoch 36 of 500
  training loss:		1.957560E-02
  validation loss:		1.247126E-02
Epoch took 0.703s

Epoch 37 of 500
  training loss:		1.901240E-02
  validation loss:		1.554083E-02
Epoch took 0.704s

Epoch 38 of 500
  training loss:		1.751502E-02
  validation loss:		1.635289E-02
Epoch took 0.703s

Epoch 39 of 500
  training loss:		1.606202E-02
  validation loss:		1.841055E-02
Epoch took 0.704s

Epoch 40 of 500
  training loss:		1.391671E-02
  validation loss:		3.090509E-02
Epoch took 0.703s

Epoch 41 of 500
  training loss:		1.820535E-02
  validation loss:		2.965494E-02
Epoch took 0.704s

Epoch 42 of 500
  training loss:		1.646076E-02
  validation loss:		9.723322E-03
Epoch took 0.703s

Epoch 43 of 500
  training loss:		1.671316E-02
  validation loss:		2.285300E-02
Epoch took 0.704s

Epoch 44 of 500
  training loss:		1.553475E-02
  validation loss:		2.238788E-02
Epoch took 0.704s

Epoch 45 of 500
  training loss:		1.565933E-02
  validation loss:		2.414360E-02
Epoch took 0.703s

Epoch 46 of 500
  training loss:		1.489361E-02
  validation loss:		1.474738E-02
Epoch took 0.703s

Epoch 47 of 500
  training loss:		1.591773E-02
  validation loss:		2.297084E-02
Epoch took 0.704s

Epoch 48 of 500
  training loss:		1.486424E-02
  validation loss:		1.495929E-02
Epoch took 0.704s

Epoch 49 of 500
  training loss:		1.604583E-02
  validation loss:		2.339716E-02
Epoch took 0.703s

Epoch 50 of 500
  training loss:		1.656981E-02
  validation loss:		1.807316E-02
Epoch took 0.703s

Epoch 51 of 500
  training loss:		1.464685E-02
  validation loss:		1.617766E-02
Epoch took 0.704s

Epoch 52 of 500
  training loss:		1.371478E-02
  validation loss:		8.439643E-03
Epoch took 0.704s

Epoch 53 of 500
  training loss:		1.443160E-02
  validation loss:		1.131665E-02
Epoch took 0.704s

Epoch 54 of 500
  training loss:		1.568807E-02
  validation loss:		2.472321E-02
Epoch took 0.704s

Epoch 55 of 500
  training loss:		1.628536E-02
  validation loss:		1.953230E-02
Epoch took 0.705s

Epoch 56 of 500
  training loss:		1.565190E-02
  validation loss:		1.555269E-02
Epoch took 0.704s

Epoch 57 of 500
  training loss:		1.675775E-02
  validation loss:		1.215585E-02
Epoch took 0.705s

Epoch 58 of 500
  training loss:		1.325551E-02
  validation loss:		1.527888E-02
Epoch took 0.704s

Epoch 59 of 500
  training loss:		1.379748E-02
  validation loss:		1.799396E-02
Epoch took 0.704s

Epoch 60 of 500
  training loss:		1.338748E-02
  validation loss:		5.972444E-03
Epoch took 0.704s

Epoch 61 of 500
  training loss:		1.338330E-02
  validation loss:		2.874868E-02
Epoch took 0.704s

Epoch 62 of 500
  training loss:		1.633487E-02
  validation loss:		1.318509E-02
Epoch took 0.704s

Epoch 63 of 500
  training loss:		1.540117E-02
  validation loss:		2.626057E-02
Epoch took 0.704s

Epoch 64 of 500
  training loss:		1.485931E-02
  validation loss:		1.395137E-02
Epoch took 0.704s

Epoch 65 of 500
  training loss:		1.438792E-02
  validation loss:		2.332409E-02
Epoch took 0.703s

Epoch 66 of 500
  training loss:		1.400073E-02
  validation loss:		9.197101E-03
Epoch took 0.704s

Epoch 67 of 500
  training loss:		1.334115E-02
  validation loss:		1.387699E-02
Epoch took 0.704s

Epoch 68 of 500
  training loss:		1.309318E-02
  validation loss:		1.523841E-02
Epoch took 0.704s

Epoch 69 of 500
  training loss:		1.309541E-02
  validation loss:		1.672335E-02
Epoch took 0.704s

Epoch 70 of 500
  training loss:		1.121967E-02
  validation loss:		1.879025E-02
Epoch took 0.704s

Epoch 71 of 500
  training loss:		1.168987E-02
  validation loss:		2.781580E-02
Epoch took 0.705s

Epoch 72 of 500
  training loss:		1.306793E-02
  validation loss:		1.140987E-02
Epoch took 0.705s

Epoch 73 of 500
  training loss:		1.434288E-02
  validation loss:		2.294580E-02
Epoch took 0.704s

Epoch 74 of 500
  training loss:		1.510464E-02
  validation loss:		1.687426E-02
Epoch took 0.705s

Epoch 75 of 500
  training loss:		1.453334E-02
  validation loss:		2.371536E-02
Epoch took 0.705s

Epoch 76 of 500
  training loss:		1.411907E-02
  validation loss:		1.521506E-02
Epoch took 0.705s

Epoch 77 of 500
  training loss:		1.372549E-02
  validation loss:		2.185890E-02
Epoch took 0.704s

Epoch 78 of 500
  training loss:		1.336463E-02
  validation loss:		1.588998E-02
Epoch took 0.705s

Epoch 79 of 500
  training loss:		1.300735E-02
  validation loss:		2.044563E-02
Epoch took 0.704s

Epoch 80 of 500
  training loss:		1.279202E-02
  validation loss:		1.273703E-02
Epoch took 0.705s

Early stopping, val-loss increased over the last 20 epochs from 0.0175026924893 to 0.0184101785947
Saving model from epoch 60
Training RMSE: 0.00597955
Validation RMSE: 0.00597935
Test RMSE: 0.00595958670601
Test MSE: 3.5516673961e-05
Test MAE: 0.00402649398893
Test R2: -378104087.426 

