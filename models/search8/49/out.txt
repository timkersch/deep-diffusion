Training network with 56340 training samples and 18780 validation samples
Epoch 1 of 500
  training loss:		1.787497E-01
  validation loss:		6.748753E-02
Epoch took 0.829s

Epoch 2 of 500
  training loss:		5.237087E-02
  validation loss:		3.648233E-02
Epoch took 0.775s

Epoch 3 of 500
  training loss:		3.966891E-02
  validation loss:		4.627426E-02
Epoch took 0.775s

Epoch 4 of 500
  training loss:		3.266800E-02
  validation loss:		4.113883E-02
Epoch took 0.774s

Epoch 5 of 500
  training loss:		3.112611E-02
  validation loss:		4.804927E-02
Epoch took 0.774s

Epoch 6 of 500
  training loss:		3.663380E-02
  validation loss:		3.144857E-02
Epoch took 0.774s

Epoch 7 of 500
  training loss:		2.606064E-02
  validation loss:		2.943941E-02
Epoch took 0.775s

Epoch 8 of 500
  training loss:		2.429491E-02
  validation loss:		2.531116E-02
Epoch took 0.776s

Epoch 9 of 500
  training loss:		2.220568E-02
  validation loss:		1.851549E-02
Epoch took 0.774s

Epoch 10 of 500
  training loss:		2.098508E-02
  validation loss:		2.300695E-02
Epoch took 0.774s

Epoch 11 of 500
  training loss:		2.616484E-02
  validation loss:		2.199325E-02
Epoch took 0.774s

Epoch 12 of 500
  training loss:		2.707857E-02
  validation loss:		2.776491E-02
Epoch took 0.774s

Epoch 13 of 500
  training loss:		1.993326E-02
  validation loss:		2.886298E-02
Epoch took 0.774s

Epoch 14 of 500
  training loss:		2.001031E-02
  validation loss:		1.983608E-02
Epoch took 0.774s

Epoch 15 of 500
  training loss:		1.945646E-02
  validation loss:		1.056852E-02
Epoch took 0.775s

Epoch 16 of 500
  training loss:		1.900307E-02
  validation loss:		1.878425E-02
Epoch took 0.774s

Epoch 17 of 500
  training loss:		2.246026E-02
  validation loss:		2.620038E-02
Epoch took 0.774s

Epoch 18 of 500
  training loss:		2.095508E-02
  validation loss:		2.498420E-02
Epoch took 0.774s

Epoch 19 of 500
  training loss:		1.752517E-02
  validation loss:		4.407613E-02
Epoch took 0.774s

Epoch 20 of 500
  training loss:		1.791834E-02
  validation loss:		1.740787E-02
Epoch took 0.774s

Epoch 21 of 500
  training loss:		1.637819E-02
  validation loss:		1.573147E-02
Epoch took 0.774s

Epoch 22 of 500
  training loss:		1.706506E-02
  validation loss:		2.616823E-02
Epoch took 0.774s

Epoch 23 of 500
  training loss:		1.632120E-02
  validation loss:		2.798671E-02
Epoch took 0.774s

Epoch 24 of 500
  training loss:		1.790270E-02
  validation loss:		1.894172E-02
Epoch took 0.774s

Epoch 25 of 500
  training loss:		1.630832E-02
  validation loss:		2.325393E-02
Epoch took 0.775s

Epoch 26 of 500
  training loss:		1.427853E-02
  validation loss:		1.133173E-02
Epoch took 0.774s

Epoch 27 of 500
  training loss:		1.548230E-02
  validation loss:		2.119770E-02
Epoch took 0.774s

Epoch 28 of 500
  training loss:		1.484076E-02
  validation loss:		2.981683E-02
Epoch took 0.774s

Epoch 29 of 500
  training loss:		1.581843E-02
  validation loss:		2.626921E-02
Epoch took 0.775s

Epoch 30 of 500
  training loss:		1.553611E-02
  validation loss:		4.800193E-02
Epoch took 0.774s

Epoch 31 of 500
  training loss:		2.009653E-02
  validation loss:		2.688373E-02
Epoch took 0.775s

Epoch 32 of 500
  training loss:		1.857368E-02
  validation loss:		3.769716E-02
Epoch took 0.776s

Epoch 33 of 500
  training loss:		1.578355E-02
  validation loss:		8.507468E-03
Epoch took 0.776s

Epoch 34 of 500
  training loss:		1.667875E-02
  validation loss:		5.555141E-03
Epoch took 0.776s

Epoch 35 of 500
  training loss:		1.683006E-02
  validation loss:		3.606950E-02
Epoch took 0.776s

Epoch 36 of 500
  training loss:		1.780966E-02
  validation loss:		3.151781E-02
Epoch took 0.776s

Epoch 37 of 500
  training loss:		1.505222E-02
  validation loss:		4.223569E-02
Epoch took 0.776s

Epoch 38 of 500
  training loss:		1.397904E-02
  validation loss:		2.785880E-02
Epoch took 0.776s

Epoch 39 of 500
  training loss:		1.751358E-02
  validation loss:		2.729336E-02
Epoch took 0.776s

Epoch 40 of 500
  training loss:		1.510988E-02
  validation loss:		2.765918E-02
Epoch took 0.776s

Epoch 41 of 500
  training loss:		1.758889E-02
  validation loss:		1.762479E-02
Epoch took 0.776s

Epoch 42 of 500
  training loss:		1.664884E-02
  validation loss:		2.072753E-02
Epoch took 0.776s

Epoch 43 of 500
  training loss:		1.543257E-02
  validation loss:		1.812742E-02
Epoch took 0.777s

Epoch 44 of 500
  training loss:		1.451023E-02
  validation loss:		1.770459E-02
Epoch took 0.777s

Epoch 45 of 500
  training loss:		1.336678E-02
  validation loss:		1.885338E-02
Epoch took 0.779s

Epoch 46 of 500
  training loss:		1.241268E-02
  validation loss:		2.396407E-02
Epoch took 0.779s

Epoch 47 of 500
  training loss:		1.207888E-02
  validation loss:		1.488845E-02
Epoch took 0.779s

Epoch 48 of 500
  training loss:		1.878495E-02
  validation loss:		3.232726E-02
Epoch took 0.779s

Epoch 49 of 500
  training loss:		2.013073E-02
  validation loss:		3.538844E-02
Epoch took 0.779s

Epoch 50 of 500
  training loss:		1.927886E-02
  validation loss:		1.196639E-02
Epoch took 0.778s

Epoch 51 of 500
  training loss:		1.760418E-02
  validation loss:		1.433569E-02
Epoch took 0.778s

Epoch 52 of 500
  training loss:		1.388855E-02
  validation loss:		2.388204E-02
Epoch took 0.779s

Epoch 53 of 500
  training loss:		1.314063E-02
  validation loss:		3.742587E-02
Epoch took 0.779s

Epoch 54 of 500
  training loss:		1.246707E-02
  validation loss:		1.728790E-02
Epoch took 0.778s

Epoch 55 of 500
  training loss:		1.570014E-02
  validation loss:		1.398426E-02
Epoch took 0.779s

Epoch 56 of 500
  training loss:		1.149483E-02
  validation loss:		2.232509E-02
Epoch took 0.779s

Epoch 57 of 500
  training loss:		1.358228E-02
  validation loss:		2.333229E-02
Epoch took 0.779s

Epoch 58 of 500
  training loss:		1.318120E-02
  validation loss:		2.583684E-02
Epoch took 0.780s

Epoch 59 of 500
  training loss:		1.603029E-02
  validation loss:		2.356047E-02
Epoch took 0.780s

Epoch 60 of 500
  training loss:		1.688406E-02
  validation loss:		2.654096E-02
Epoch took 0.780s

Epoch 61 of 500
  training loss:		1.392534E-02
  validation loss:		1.678666E-02
Epoch took 0.780s

Epoch 62 of 500
  training loss:		1.564582E-02
  validation loss:		2.062701E-02
Epoch took 0.779s

Epoch 63 of 500
  training loss:		1.456755E-02
  validation loss:		1.501277E-02
Epoch took 0.780s

Epoch 64 of 500
  training loss:		1.314086E-02
  validation loss:		1.288159E-02
Epoch took 0.780s

Epoch 65 of 500
  training loss:		1.241470E-02
  validation loss:		3.649201E-03
Epoch took 0.780s

Epoch 66 of 500
  training loss:		1.099837E-02
  validation loss:		1.600874E-02
Epoch took 0.780s

Epoch 67 of 500
  training loss:		1.279802E-02
  validation loss:		2.518042E-02
Epoch took 0.780s

Epoch 68 of 500
  training loss:		1.291699E-02
  validation loss:		3.116949E-02
Epoch took 0.780s

Epoch 69 of 500
  training loss:		1.339492E-02
  validation loss:		1.493966E-02
Epoch took 0.779s

Epoch 70 of 500
  training loss:		1.066269E-02
  validation loss:		1.776153E-02
Epoch took 0.780s

Epoch 71 of 500
  training loss:		1.308782E-02
  validation loss:		7.043910E-03
Epoch took 0.780s

Epoch 72 of 500
  training loss:		1.138725E-02
  validation loss:		1.453224E-02
Epoch took 0.780s

Epoch 73 of 500
  training loss:		1.228649E-02
  validation loss:		2.016698E-02
Epoch took 0.780s

Epoch 74 of 500
  training loss:		1.209766E-02
  validation loss:		1.927509E-02
Epoch took 0.780s

Epoch 75 of 500
  training loss:		1.314855E-02
  validation loss:		2.419583E-02
Epoch took 0.780s

Epoch 76 of 500
  training loss:		1.219643E-02
  validation loss:		1.237498E-02
Epoch took 0.780s

Epoch 77 of 500
  training loss:		1.136113E-02
  validation loss:		1.467897E-02
Epoch took 0.780s

Epoch 78 of 500
  training loss:		1.082357E-02
  validation loss:		1.118223E-02
Epoch took 0.780s

Epoch 79 of 500
  training loss:		1.121782E-02
  validation loss:		2.001784E-02
Epoch took 0.780s

Epoch 80 of 500
  training loss:		1.372251E-02
  validation loss:		2.672997E-02
Epoch took 0.780s

Epoch 81 of 500
  training loss:		1.060438E-02
  validation loss:		1.154395E-02
Epoch took 0.780s

Epoch 82 of 500
  training loss:		1.086319E-02
  validation loss:		1.811770E-02
Epoch took 0.780s

Epoch 83 of 500
  training loss:		1.123709E-02
  validation loss:		1.925583E-02
Epoch took 0.780s

Epoch 84 of 500
  training loss:		1.029913E-02
  validation loss:		1.691961E-02
Epoch took 0.779s

Epoch 85 of 500
  training loss:		1.150002E-02
  validation loss:		1.480018E-02
Epoch took 0.779s

Epoch 86 of 500
  training loss:		1.130714E-02
  validation loss:		1.507416E-02
Epoch took 0.780s

Epoch 87 of 500
  training loss:		1.238675E-02
  validation loss:		2.267692E-02
Epoch took 0.780s

Epoch 88 of 500
  training loss:		1.151092E-02
  validation loss:		5.320324E-03
Epoch took 0.780s

Epoch 89 of 500
  training loss:		1.100183E-02
  validation loss:		1.643075E-02
Epoch took 0.780s

Epoch 90 of 500
  training loss:		1.058096E-02
  validation loss:		8.935993E-03
Epoch took 0.780s

Epoch 91 of 500
  training loss:		1.017377E-02
  validation loss:		1.158889E-02
Epoch took 0.780s

Epoch 92 of 500
  training loss:		1.138027E-02
  validation loss:		3.045771E-03
Epoch took 0.780s

Epoch 93 of 500
  training loss:		1.066870E-02
  validation loss:		7.227493E-03
Epoch took 0.780s

Epoch 94 of 500
  training loss:		1.190971E-02
  validation loss:		5.979449E-03
Epoch took 0.780s

Epoch 95 of 500
  training loss:		1.207862E-02
  validation loss:		2.497068E-02
Epoch took 0.780s

Epoch 96 of 500
  training loss:		1.026853E-02
  validation loss:		8.253092E-03
Epoch took 0.779s

Epoch 97 of 500
  training loss:		1.307277E-02
  validation loss:		8.619385E-03
Epoch took 0.780s

Epoch 98 of 500
  training loss:		1.409238E-02
  validation loss:		1.392745E-02
Epoch took 0.780s

Epoch 99 of 500
  training loss:		1.116896E-02
  validation loss:		1.099494E-02
Epoch took 0.780s

Epoch 100 of 500
  training loss:		1.050614E-02
  validation loss:		5.825639E-03
Epoch took 0.780s

Epoch 101 of 500
  training loss:		1.194268E-02
  validation loss:		1.342651E-02
Epoch took 0.781s

Epoch 102 of 500
  training loss:		9.082909E-03
  validation loss:		6.212879E-03
Epoch took 0.780s

Epoch 103 of 500
  training loss:		1.001664E-02
  validation loss:		6.008497E-03
Epoch took 0.780s

Epoch 104 of 500
  training loss:		9.623302E-03
  validation loss:		1.515749E-02
Epoch took 0.780s

Epoch 105 of 500
  training loss:		1.122390E-02
  validation loss:		1.420361E-02
Epoch took 0.781s

Epoch 106 of 500
  training loss:		1.077689E-02
  validation loss:		1.724596E-02
Epoch took 0.780s

Epoch 107 of 500
  training loss:		1.033611E-02
  validation loss:		1.498348E-02
Epoch took 0.780s

Epoch 108 of 500
  training loss:		9.284243E-03
  validation loss:		7.555607E-03
Epoch took 0.780s

Epoch 109 of 500
  training loss:		9.523813E-03
  validation loss:		2.627594E-02
Epoch took 0.781s

Epoch 110 of 500
  training loss:		1.456365E-02
  validation loss:		1.855611E-02
Epoch took 0.781s

Epoch 111 of 500
  training loss:		1.293702E-02
  validation loss:		1.665608E-02
Epoch took 0.781s

Epoch 112 of 500
  training loss:		1.115862E-02
  validation loss:		1.600594E-02
Epoch took 0.781s

Epoch 113 of 500
  training loss:		1.120052E-02
  validation loss:		1.529908E-02
Epoch took 0.781s

Epoch 114 of 500
  training loss:		1.086219E-02
  validation loss:		1.160087E-02
Epoch took 0.781s

Epoch 115 of 500
  training loss:		9.449236E-03
  validation loss:		1.498661E-02
Epoch took 0.782s

Epoch 116 of 500
  training loss:		1.078671E-02
  validation loss:		6.546021E-03
Epoch took 0.782s

Epoch 117 of 500
  training loss:		1.182191E-02
  validation loss:		7.354682E-03
Epoch took 0.782s

Epoch 118 of 500
  training loss:		1.026398E-02
  validation loss:		8.161366E-03
Epoch took 0.781s

Epoch 119 of 500
  training loss:		1.106943E-02
  validation loss:		1.109566E-02
Epoch took 0.782s

Epoch 120 of 500
  training loss:		1.051675E-02
  validation loss:		1.835281E-02
Epoch took 0.782s

Early stopping, val-loss increased over the last 15 epochs from 0.0103627855363 to 0.0140450803684
Saving model from epoch 105
Training RMSE: 0.0142255
Validation RMSE: 0.0142417
Test RMSE: 0.0138188954443
Test MSE: 0.000190961873159
Test MAE: 0.00546036660671
Test R2: -2032945503.34 

