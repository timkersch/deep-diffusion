Training network with 56340 training samples and 18780 validation samples
Epoch 1 of 500
  training loss:		3.848269E-01
  validation loss:		8.560929E+01
Epoch took 0.742s

Epoch 2 of 500
  training loss:		1.702761E-01
  validation loss:		6.245311E-01
Epoch took 0.702s

Epoch 3 of 500
  training loss:		1.062299E-01
  validation loss:		8.499188E-02
Epoch took 0.701s

Epoch 4 of 500
  training loss:		7.954384E-02
  validation loss:		4.307365E-02
Epoch took 0.702s

Epoch 5 of 500
  training loss:		7.542622E-02
  validation loss:		3.835183E-02
Epoch took 0.701s

Epoch 6 of 500
  training loss:		6.681494E-02
  validation loss:		3.216489E-02
Epoch took 0.701s

Epoch 7 of 500
  training loss:		6.602369E-02
  validation loss:		3.849856E-02
Epoch took 0.702s

Epoch 8 of 500
  training loss:		6.115337E-02
  validation loss:		3.744692E-02
Epoch took 0.702s

Epoch 9 of 500
  training loss:		6.363181E-02
  validation loss:		3.358590E-02
Epoch took 0.702s

Epoch 10 of 500
  training loss:		6.091290E-02
  validation loss:		3.386835E-02
Epoch took 0.701s

Epoch 11 of 500
  training loss:		5.889716E-02
  validation loss:		3.118613E-02
Epoch took 0.702s

Epoch 12 of 500
  training loss:		5.582202E-02
  validation loss:		3.717584E-02
Epoch took 0.701s

Epoch 13 of 500
  training loss:		5.651367E-02
  validation loss:		3.491083E-02
Epoch took 0.701s

Epoch 14 of 500
  training loss:		5.319674E-02
  validation loss:		4.003547E-02
Epoch took 0.701s

Epoch 15 of 500
  training loss:		5.546018E-02
  validation loss:		3.177950E-02
Epoch took 0.701s

Epoch 16 of 500
  training loss:		5.371709E-02
  validation loss:		3.206326E-02
Epoch took 0.701s

Epoch 17 of 500
  training loss:		5.387718E-02
  validation loss:		2.579305E-02
Epoch took 0.701s

Epoch 18 of 500
  training loss:		5.014838E-02
  validation loss:		2.687772E-02
Epoch took 0.702s

Epoch 19 of 500
  training loss:		5.147306E-02
  validation loss:		2.892433E-02
Epoch took 0.701s

Epoch 20 of 500
  training loss:		5.134038E-02
  validation loss:		3.630518E-02
Epoch took 0.701s

Epoch 21 of 500
  training loss:		5.157743E-02
  validation loss:		3.153090E-02
Epoch took 0.702s

Epoch 22 of 500
  training loss:		5.023217E-02
  validation loss:		2.299957E-02
Epoch took 0.702s

Epoch 23 of 500
  training loss:		4.861683E-02
  validation loss:		2.974466E-02
Epoch took 0.701s

Epoch 24 of 500
  training loss:		4.957731E-02
  validation loss:		3.389849E-02
Epoch took 0.701s

Epoch 25 of 500
  training loss:		4.883650E-02
  validation loss:		2.914795E-02
Epoch took 0.702s

Epoch 26 of 500
  training loss:		4.702212E-02
  validation loss:		2.100037E-02
Epoch took 0.701s

Epoch 27 of 500
  training loss:		4.862864E-02
  validation loss:		4.315729E-02
Epoch took 0.702s

Epoch 28 of 500
  training loss:		4.901693E-02
  validation loss:		4.218666E-02
Epoch took 0.701s

Epoch 29 of 500
  training loss:		4.782102E-02
  validation loss:		2.370381E-02
Epoch took 0.702s

Epoch 30 of 500
  training loss:		4.683446E-02
  validation loss:		2.264390E-02
Epoch took 0.701s

Epoch 31 of 500
  training loss:		4.585611E-02
  validation loss:		3.205435E-02
Epoch took 0.701s

Epoch 32 of 500
  training loss:		4.759699E-02
  validation loss:		2.024842E-02
Epoch took 0.701s

Epoch 33 of 500
  training loss:		4.576163E-02
  validation loss:		2.494321E-02
Epoch took 0.701s

Epoch 34 of 500
  training loss:		4.561337E-02
  validation loss:		2.224432E-02
Epoch took 0.701s

Epoch 35 of 500
  training loss:		4.629568E-02
  validation loss:		2.395758E-02
Epoch took 0.703s

Epoch 36 of 500
  training loss:		4.470679E-02
  validation loss:		2.428931E-02
Epoch took 0.703s

Epoch 37 of 500
  training loss:		4.585940E-02
  validation loss:		3.526750E-02
Epoch took 0.704s

Epoch 38 of 500
  training loss:		4.397242E-02
  validation loss:		2.656336E-02
Epoch took 0.703s

Epoch 39 of 500
  training loss:		4.572267E-02
  validation loss:		2.732817E-02
Epoch took 0.704s

Epoch 40 of 500
  training loss:		4.413412E-02
  validation loss:		2.655176E-02
Epoch took 0.703s

Epoch 41 of 500
  training loss:		4.372118E-02
  validation loss:		2.933838E-02
Epoch took 0.704s

Epoch 42 of 500
  training loss:		4.306310E-02
  validation loss:		2.062164E-02
Epoch took 0.704s

Epoch 43 of 500
  training loss:		4.238797E-02
  validation loss:		2.796673E-02
Epoch took 0.703s

Epoch 44 of 500
  training loss:		4.621406E-02
  validation loss:		3.259780E-02
Epoch took 0.703s

Epoch 45 of 500
  training loss:		4.306543E-02
  validation loss:		3.349917E-02
Epoch took 0.704s

Epoch 46 of 500
  training loss:		4.257825E-02
  validation loss:		3.047928E-02
Epoch took 0.704s

Epoch 47 of 500
  training loss:		4.222251E-02
  validation loss:		2.639568E-02
Epoch took 0.704s

Epoch 48 of 500
  training loss:		4.229983E-02
  validation loss:		2.716499E-02
Epoch took 0.704s

Epoch 49 of 500
  training loss:		4.113295E-02
  validation loss:		2.369880E-02
Epoch took 0.703s

Epoch 50 of 500
  training loss:		4.146726E-02
  validation loss:		2.127173E-02
Epoch took 0.703s

Epoch 51 of 500
  training loss:		4.046715E-02
  validation loss:		2.296713E-02
Epoch took 0.703s

Epoch 52 of 500
  training loss:		4.214087E-02
  validation loss:		2.293014E-02
Epoch took 0.704s

Epoch 53 of 500
  training loss:		4.217021E-02
  validation loss:		2.235599E-02
Epoch took 0.704s

Epoch 54 of 500
  training loss:		4.053845E-02
  validation loss:		2.198747E-02
Epoch took 0.703s

Epoch 55 of 500
  training loss:		4.175908E-02
  validation loss:		2.630367E-02
Epoch took 0.703s

Epoch 56 of 500
  training loss:		4.014373E-02
  validation loss:		3.027265E-02
Epoch took 0.703s

Epoch 57 of 500
  training loss:		4.130042E-02
  validation loss:		2.224535E-02
Epoch took 0.704s

Epoch 58 of 500
  training loss:		3.983470E-02
  validation loss:		2.558543E-02
Epoch took 0.703s

Epoch 59 of 500
  training loss:		4.038408E-02
  validation loss:		2.175515E-02
Epoch took 0.703s

Epoch 60 of 500
  training loss:		3.899134E-02
  validation loss:		2.553941E-02
Epoch took 0.704s

Epoch 61 of 500
  training loss:		3.967286E-02
  validation loss:		1.979601E-02
Epoch took 0.703s

Epoch 62 of 500
  training loss:		3.947712E-02
  validation loss:		2.105744E-02
Epoch took 0.704s

Epoch 63 of 500
  training loss:		4.103364E-02
  validation loss:		2.356751E-02
Epoch took 0.703s

Epoch 64 of 500
  training loss:		3.962126E-02
  validation loss:		2.340711E-02
Epoch took 0.703s

Epoch 65 of 500
  training loss:		3.822917E-02
  validation loss:		2.534142E-02
Epoch took 0.704s

Epoch 66 of 500
  training loss:		4.011741E-02
  validation loss:		2.692934E-02
Epoch took 0.703s

Epoch 67 of 500
  training loss:		3.930290E-02
  validation loss:		2.513844E-02
Epoch took 0.704s

Epoch 68 of 500
  training loss:		3.859507E-02
  validation loss:		3.605986E-02
Epoch took 0.704s

Epoch 69 of 500
  training loss:		3.934878E-02
  validation loss:		1.915332E-02
Epoch took 0.704s

Epoch 70 of 500
  training loss:		4.077228E-02
  validation loss:		2.083225E-02
Epoch took 0.704s

Epoch 71 of 500
  training loss:		3.883319E-02
  validation loss:		3.171719E-02
Epoch took 0.703s

Epoch 72 of 500
  training loss:		3.870528E-02
  validation loss:		2.374774E-02
Epoch took 0.703s

Epoch 73 of 500
  training loss:		3.874070E-02
  validation loss:		1.865162E-02
Epoch took 0.703s

Epoch 74 of 500
  training loss:		3.962927E-02
  validation loss:		2.131314E-02
Epoch took 0.703s

Epoch 75 of 500
  training loss:		3.814116E-02
  validation loss:		2.352131E-02
Epoch took 0.703s

Epoch 76 of 500
  training loss:		3.792719E-02
  validation loss:		2.730231E-02
Epoch took 0.704s

Epoch 77 of 500
  training loss:		3.716738E-02
  validation loss:		2.089402E-02
Epoch took 0.703s

Epoch 78 of 500
  training loss:		3.750493E-02
  validation loss:		1.997035E-02
Epoch took 0.704s

Epoch 79 of 500
  training loss:		3.779035E-02
  validation loss:		2.630346E-02
Epoch took 0.704s

Epoch 80 of 500
  training loss:		3.653430E-02
  validation loss:		2.064406E-02
Epoch took 0.704s

Epoch 81 of 500
  training loss:		3.639445E-02
  validation loss:		2.462622E-02
Epoch took 0.703s

Epoch 82 of 500
  training loss:		3.605707E-02
  validation loss:		2.084929E-02
Epoch took 0.704s

Epoch 83 of 500
  training loss:		3.570501E-02
  validation loss:		2.455358E-02
Epoch took 0.703s

Epoch 84 of 500
  training loss:		3.721177E-02
  validation loss:		1.728316E-02
Epoch took 0.704s

Epoch 85 of 500
  training loss:		3.663477E-02
  validation loss:		2.225996E-02
Epoch took 0.703s

Epoch 86 of 500
  training loss:		3.760334E-02
  validation loss:		2.168905E-02
Epoch took 0.705s

Epoch 87 of 500
  training loss:		3.674093E-02
  validation loss:		2.260398E-02
Epoch took 0.704s

Epoch 88 of 500
  training loss:		3.639292E-02
  validation loss:		3.216934E-02
Epoch took 0.704s

Epoch 89 of 500
  training loss:		3.721444E-02
  validation loss:		2.276294E-02
Epoch took 0.703s

Epoch 90 of 500
  training loss:		3.481688E-02
  validation loss:		2.317581E-02
Epoch took 0.704s

Epoch 91 of 500
  training loss:		3.581037E-02
  validation loss:		2.981897E-02
Epoch took 0.703s

Epoch 92 of 500
  training loss:		3.596255E-02
  validation loss:		1.781277E-02
Epoch took 0.704s

Epoch 93 of 500
  training loss:		3.399179E-02
  validation loss:		1.569675E-02
Epoch took 0.703s

Epoch 94 of 500
  training loss:		3.604398E-02
  validation loss:		2.004442E-02
Epoch took 0.703s

Epoch 95 of 500
  training loss:		3.578223E-02
  validation loss:		2.271877E-02
Epoch took 0.704s

Epoch 96 of 500
  training loss:		3.626987E-02
  validation loss:		2.001235E-02
Epoch took 0.704s

Epoch 97 of 500
  training loss:		3.526483E-02
  validation loss:		1.729953E-02
Epoch took 0.703s

Epoch 98 of 500
  training loss:		3.459182E-02
  validation loss:		1.793325E-02
Epoch took 0.704s

Epoch 99 of 500
  training loss:		3.402170E-02
  validation loss:		2.345214E-02
Epoch took 0.704s

Epoch 100 of 500
  training loss:		3.547260E-02
  validation loss:		2.173325E-02
Epoch took 0.703s

Epoch 101 of 500
  training loss:		3.366504E-02
  validation loss:		2.001627E-02
Epoch took 0.704s

Epoch 102 of 500
  training loss:		3.483698E-02
  validation loss:		2.683483E-02
Epoch took 0.704s

Epoch 103 of 500
  training loss:		3.449119E-02
  validation loss:		1.888304E-02
Epoch took 0.703s

Epoch 104 of 500
  training loss:		3.478977E-02
  validation loss:		2.726240E-02
Epoch took 0.703s

Epoch 105 of 500
  training loss:		3.477094E-02
  validation loss:		1.918235E-02
Epoch took 0.703s

Epoch 106 of 500
  training loss:		3.269726E-02
  validation loss:		3.147504E-02
Epoch took 0.703s

Epoch 107 of 500
  training loss:		3.447456E-02
  validation loss:		1.874377E-02
Epoch took 0.705s

Epoch 108 of 500
  training loss:		3.573953E-02
  validation loss:		2.569921E-02
Epoch took 0.703s

Epoch 109 of 500
  training loss:		3.382925E-02
  validation loss:		2.845287E-02
Epoch took 0.703s

Epoch 110 of 500
  training loss:		3.506265E-02
  validation loss:		1.901115E-02
Epoch took 0.704s

Epoch 111 of 500
  training loss:		3.498848E-02
  validation loss:		2.204108E-02
Epoch took 0.703s

Epoch 112 of 500
  training loss:		3.377673E-02
  validation loss:		2.451575E-02
Epoch took 0.703s

Epoch 113 of 500
  training loss:		3.438882E-02
  validation loss:		1.632392E-02
Epoch took 0.703s

Epoch 114 of 500
  training loss:		3.423487E-02
  validation loss:		1.715196E-02
Epoch took 0.704s

Epoch 115 of 500
  training loss:		3.499989E-02
  validation loss:		2.319725E-02
Epoch took 0.704s

Epoch 116 of 500
  training loss:		3.451375E-02
  validation loss:		2.879743E-02
Epoch took 0.703s

Epoch 117 of 500
  training loss:		3.423670E-02
  validation loss:		2.195068E-02
Epoch took 0.704s

Epoch 118 of 500
  training loss:		3.395172E-02
  validation loss:		3.245460E-02
Epoch took 0.703s

Epoch 119 of 500
  training loss:		3.368706E-02
  validation loss:		1.616994E-02
Epoch took 0.704s

Epoch 120 of 500
  training loss:		3.372660E-02
  validation loss:		2.264153E-02
Epoch took 0.704s

Early stopping, val-loss increased over the last 20 epochs from 0.0219247754068 to 0.0230402536613
Saving model from epoch 100
Training RMSE: 0.0215989
Validation RMSE: 0.0217629
Test RMSE: 0.0213958993554
Test MSE: 0.000457784539321
Test MAE: 0.0159240942448
Test R2: -4873491017.47 

