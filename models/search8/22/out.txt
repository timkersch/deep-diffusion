Training network with 56340 training samples and 18780 validation samples
Epoch 1 of 500
  training loss:		8.819073E-01
  validation loss:		3.101973E+01
Epoch took 0.750s

Epoch 2 of 500
  training loss:		6.924852E-01
  validation loss:		7.399085E-01
Epoch took 0.700s

Epoch 3 of 500
  training loss:		5.877328E-01
  validation loss:		5.363136E-01
Epoch took 0.700s

Epoch 4 of 500
  training loss:		5.139352E-01
  validation loss:		4.719137E-01
Epoch took 0.700s

Epoch 5 of 500
  training loss:		4.515053E-01
  validation loss:		4.136861E-01
Epoch took 0.700s

Epoch 6 of 500
  training loss:		3.981195E-01
  validation loss:		3.656557E-01
Epoch took 0.700s

Epoch 7 of 500
  training loss:		3.503195E-01
  validation loss:		3.127117E-01
Epoch took 0.700s

Epoch 8 of 500
  training loss:		3.052432E-01
  validation loss:		2.703563E-01
Epoch took 0.700s

Epoch 9 of 500
  training loss:		2.623192E-01
  validation loss:		2.318596E-01
Epoch took 0.700s

Epoch 10 of 500
  training loss:		2.241120E-01
  validation loss:		1.891993E-01
Epoch took 0.700s

Epoch 11 of 500
  training loss:		1.915947E-01
  validation loss:		1.583252E-01
Epoch took 0.700s

Epoch 12 of 500
  training loss:		1.648455E-01
  validation loss:		1.299310E-01
Epoch took 0.699s

Epoch 13 of 500
  training loss:		1.394291E-01
  validation loss:		1.046162E-01
Epoch took 0.700s

Epoch 14 of 500
  training loss:		1.202795E-01
  validation loss:		8.804082E-02
Epoch took 0.700s

Epoch 15 of 500
  training loss:		1.110437E-01
  validation loss:		7.148639E-02
Epoch took 0.700s

Epoch 16 of 500
  training loss:		9.302959E-02
  validation loss:		5.949626E-02
Epoch took 0.700s

Epoch 17 of 500
  training loss:		8.646149E-02
  validation loss:		4.913132E-02
Epoch took 0.700s

Epoch 18 of 500
  training loss:		7.771209E-02
  validation loss:		4.178889E-02
Epoch took 0.699s

Epoch 19 of 500
  training loss:		7.610475E-02
  validation loss:		4.156047E-02
Epoch took 0.700s

Epoch 20 of 500
  training loss:		7.407497E-02
  validation loss:		3.971570E-02
Epoch took 0.700s

Epoch 21 of 500
  training loss:		6.946657E-02
  validation loss:		2.935889E-02
Epoch took 0.700s

Epoch 22 of 500
  training loss:		6.693432E-02
  validation loss:		3.188475E-02
Epoch took 0.700s

Epoch 23 of 500
  training loss:		6.620097E-02
  validation loss:		2.824548E-02
Epoch took 0.700s

Epoch 24 of 500
  training loss:		6.457893E-02
  validation loss:		2.803336E-02
Epoch took 0.700s

Epoch 25 of 500
  training loss:		6.659268E-02
  validation loss:		2.424401E-02
Epoch took 0.700s

Epoch 26 of 500
  training loss:		6.659468E-02
  validation loss:		2.283533E-02
Epoch took 0.700s

Epoch 27 of 500
  training loss:		6.402441E-02
  validation loss:		2.518034E-02
Epoch took 0.700s

Epoch 28 of 500
  training loss:		6.247858E-02
  validation loss:		2.772162E-02
Epoch took 0.700s

Epoch 29 of 500
  training loss:		6.201344E-02
  validation loss:		2.543210E-02
Epoch took 0.699s

Epoch 30 of 500
  training loss:		5.929762E-02
  validation loss:		2.207865E-02
Epoch took 0.700s

Epoch 31 of 500
  training loss:		6.044452E-02
  validation loss:		2.545135E-02
Epoch took 0.700s

Epoch 32 of 500
  training loss:		6.114343E-02
  validation loss:		3.139052E-02
Epoch took 0.700s

Epoch 33 of 500
  training loss:		5.800432E-02
  validation loss:		2.028512E-02
Epoch took 0.700s

Epoch 34 of 500
  training loss:		5.770182E-02
  validation loss:		2.229273E-02
Epoch took 0.700s

Epoch 35 of 500
  training loss:		5.515780E-02
  validation loss:		1.964583E-02
Epoch took 0.700s

Epoch 36 of 500
  training loss:		5.457159E-02
  validation loss:		2.093265E-02
Epoch took 0.700s

Epoch 37 of 500
  training loss:		5.719693E-02
  validation loss:		2.220773E-02
Epoch took 0.700s

Epoch 38 of 500
  training loss:		5.740120E-02
  validation loss:		2.295199E-02
Epoch took 0.700s

Epoch 39 of 500
  training loss:		5.505233E-02
  validation loss:		2.323942E-02
Epoch took 0.700s

Epoch 40 of 500
  training loss:		5.635978E-02
  validation loss:		1.903926E-02
Epoch took 0.701s

Epoch 41 of 500
  training loss:		5.675912E-02
  validation loss:		2.133788E-02
Epoch took 0.700s

Epoch 42 of 500
  training loss:		5.529323E-02
  validation loss:		1.853502E-02
Epoch took 0.700s

Epoch 43 of 500
  training loss:		5.521112E-02
  validation loss:		2.038367E-02
Epoch took 0.700s

Epoch 44 of 500
  training loss:		5.373196E-02
  validation loss:		1.892040E-02
Epoch took 0.700s

Epoch 45 of 500
  training loss:		5.636018E-02
  validation loss:		1.929323E-02
Epoch took 0.700s

Epoch 46 of 500
  training loss:		5.437513E-02
  validation loss:		2.096965E-02
Epoch took 0.700s

Epoch 47 of 500
  training loss:		5.816153E-02
  validation loss:		1.926971E-02
Epoch took 0.700s

Epoch 48 of 500
  training loss:		5.277893E-02
  validation loss:		1.841574E-02
Epoch took 0.701s

Epoch 49 of 500
  training loss:		5.343923E-02
  validation loss:		1.977429E-02
Epoch took 0.700s

Epoch 50 of 500
  training loss:		5.174146E-02
  validation loss:		2.437703E-02
Epoch took 0.700s

Epoch 51 of 500
  training loss:		5.417644E-02
  validation loss:		1.934987E-02
Epoch took 0.701s

Epoch 52 of 500
  training loss:		5.129206E-02
  validation loss:		1.830342E-02
Epoch took 0.701s

Epoch 53 of 500
  training loss:		5.193420E-02
  validation loss:		2.517605E-02
Epoch took 0.700s

Epoch 54 of 500
  training loss:		5.326778E-02
  validation loss:		1.935371E-02
Epoch took 0.702s

Epoch 55 of 500
  training loss:		5.370966E-02
  validation loss:		1.871079E-02
Epoch took 0.702s

Epoch 56 of 500
  training loss:		5.295071E-02
  validation loss:		2.011538E-02
Epoch took 0.702s

Epoch 57 of 500
  training loss:		5.359746E-02
  validation loss:		2.108442E-02
Epoch took 0.702s

Epoch 58 of 500
  training loss:		4.945521E-02
  validation loss:		2.568655E-02
Epoch took 0.702s

Epoch 59 of 500
  training loss:		5.139803E-02
  validation loss:		1.865534E-02
Epoch took 0.702s

Epoch 60 of 500
  training loss:		5.268910E-02
  validation loss:		2.017396E-02
Epoch took 0.702s

Early stopping, val-loss increased over the last 10 epochs from 0.0201276616607 to 0.0206609490224
Saving model from epoch 50
Training RMSE: 0.024503
Validation RMSE: 0.0244214
Test RMSE: 0.0240189004689
Test MSE: 0.000576907536015
Test MAE: 0.0170668568462
Test R2: -6141653648.6 

