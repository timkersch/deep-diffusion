Training network with 56340 training samples and 18780 validation samples
Epoch 1 of 500
  training loss:		6.305952E-01
  validation loss:		1.800925E+02
Epoch took 0.755s

Epoch 2 of 500
  training loss:		4.470316E-01
  validation loss:		5.770177E-01
Epoch took 0.700s

Epoch 3 of 500
  training loss:		3.412804E-01
  validation loss:		3.057429E-01
Epoch took 0.700s

Epoch 4 of 500
  training loss:		2.746652E-01
  validation loss:		2.477072E-01
Epoch took 0.701s

Epoch 5 of 500
  training loss:		2.327121E-01
  validation loss:		2.076265E-01
Epoch took 0.700s

Epoch 6 of 500
  training loss:		1.969089E-01
  validation loss:		1.739194E-01
Epoch took 0.700s

Epoch 7 of 500
  training loss:		1.707836E-01
  validation loss:		1.471682E-01
Epoch took 0.700s

Epoch 8 of 500
  training loss:		1.502109E-01
  validation loss:		1.256353E-01
Epoch took 0.700s

Epoch 9 of 500
  training loss:		1.313094E-01
  validation loss:		1.061578E-01
Epoch took 0.700s

Epoch 10 of 500
  training loss:		1.160389E-01
  validation loss:		9.010602E-02
Epoch took 0.700s

Epoch 11 of 500
  training loss:		1.077899E-01
  validation loss:		8.128876E-02
Epoch took 0.700s

Epoch 12 of 500
  training loss:		9.590314E-02
  validation loss:		6.590203E-02
Epoch took 0.699s

Epoch 13 of 500
  training loss:		8.744824E-02
  validation loss:		5.704797E-02
Epoch took 0.700s

Epoch 14 of 500
  training loss:		8.082884E-02
  validation loss:		4.958384E-02
Epoch took 0.700s

Epoch 15 of 500
  training loss:		7.810195E-02
  validation loss:		4.415735E-02
Epoch took 0.699s

Epoch 16 of 500
  training loss:		7.305143E-02
  validation loss:		3.706389E-02
Epoch took 0.699s

Epoch 17 of 500
  training loss:		6.981734E-02
  validation loss:		3.288857E-02
Epoch took 0.700s

Epoch 18 of 500
  training loss:		6.843454E-02
  validation loss:		3.095152E-02
Epoch took 0.700s

Epoch 19 of 500
  training loss:		6.663686E-02
  validation loss:		2.655273E-02
Epoch took 0.700s

Epoch 20 of 500
  training loss:		6.532775E-02
  validation loss:		2.826641E-02
Epoch took 0.700s

Epoch 21 of 500
  training loss:		6.096022E-02
  validation loss:		3.016041E-02
Epoch took 0.700s

Epoch 22 of 500
  training loss:		5.848485E-02
  validation loss:		2.604451E-02
Epoch took 0.701s

Epoch 23 of 500
  training loss:		5.848323E-02
  validation loss:		2.393604E-02
Epoch took 0.700s

Epoch 24 of 500
  training loss:		6.119160E-02
  validation loss:		2.571200E-02
Epoch took 0.700s

Epoch 25 of 500
  training loss:		6.108058E-02
  validation loss:		2.108182E-02
Epoch took 0.700s

Epoch 26 of 500
  training loss:		5.996031E-02
  validation loss:		1.940765E-02
Epoch took 0.700s

Epoch 27 of 500
  training loss:		5.685770E-02
  validation loss:		1.893205E-02
Epoch took 0.699s

Epoch 28 of 500
  training loss:		5.497580E-02
  validation loss:		2.133756E-02
Epoch took 0.700s

Epoch 29 of 500
  training loss:		5.603508E-02
  validation loss:		2.690152E-02
Epoch took 0.700s

Epoch 30 of 500
  training loss:		5.640807E-02
  validation loss:		1.796888E-02
Epoch took 0.699s

Epoch 31 of 500
  training loss:		5.274391E-02
  validation loss:		2.436288E-02
Epoch took 0.700s

Epoch 32 of 500
  training loss:		5.651707E-02
  validation loss:		1.965924E-02
Epoch took 0.699s

Epoch 33 of 500
  training loss:		5.657046E-02
  validation loss:		1.785075E-02
Epoch took 0.700s

Epoch 34 of 500
  training loss:		5.303254E-02
  validation loss:		1.806784E-02
Epoch took 0.700s

Epoch 35 of 500
  training loss:		5.528341E-02
  validation loss:		1.914261E-02
Epoch took 0.700s

Epoch 36 of 500
  training loss:		5.745333E-02
  validation loss:		2.478877E-02
Epoch took 0.700s

Epoch 37 of 500
  training loss:		5.087507E-02
  validation loss:		1.975157E-02
Epoch took 0.700s

Epoch 38 of 500
  training loss:		5.222114E-02
  validation loss:		1.687543E-02
Epoch took 0.701s

Epoch 39 of 500
  training loss:		5.277812E-02
  validation loss:		1.667008E-02
Epoch took 0.700s

Epoch 40 of 500
  training loss:		4.749102E-02
  validation loss:		1.700078E-02
Epoch took 0.700s

Epoch 41 of 500
  training loss:		5.267835E-02
  validation loss:		1.585218E-02
Epoch took 0.700s

Epoch 42 of 500
  training loss:		4.814582E-02
  validation loss:		2.228232E-02
Epoch took 0.701s

Epoch 43 of 500
  training loss:		5.166438E-02
  validation loss:		1.753949E-02
Epoch took 0.700s

Epoch 44 of 500
  training loss:		5.025948E-02
  validation loss:		1.802332E-02
Epoch took 0.700s

Epoch 45 of 500
  training loss:		5.005383E-02
  validation loss:		1.702547E-02
Epoch took 0.700s

Epoch 46 of 500
  training loss:		5.117116E-02
  validation loss:		2.270959E-02
Epoch took 0.701s

Epoch 47 of 500
  training loss:		5.332225E-02
  validation loss:		1.827700E-02
Epoch took 0.700s

Epoch 48 of 500
  training loss:		4.915722E-02
  validation loss:		1.740576E-02
Epoch took 0.700s

Epoch 49 of 500
  training loss:		5.220126E-02
  validation loss:		1.807876E-02
Epoch took 0.700s

Epoch 50 of 500
  training loss:		4.670972E-02
  validation loss:		1.754043E-02
Epoch took 0.700s

Epoch 51 of 500
  training loss:		4.548410E-02
  validation loss:		2.035194E-02
Epoch took 0.700s

Epoch 52 of 500
  training loss:		4.623492E-02
  validation loss:		1.594510E-02
Epoch took 0.702s

Epoch 53 of 500
  training loss:		4.849837E-02
  validation loss:		1.516774E-02
Epoch took 0.702s

Epoch 54 of 500
  training loss:		4.915341E-02
  validation loss:		2.789424E-02
Epoch took 0.702s

Epoch 55 of 500
  training loss:		4.798141E-02
  validation loss:		1.990447E-02
Epoch took 0.702s

Epoch 56 of 500
  training loss:		4.679735E-02
  validation loss:		1.598371E-02
Epoch took 0.703s

Epoch 57 of 500
  training loss:		4.718228E-02
  validation loss:		1.575438E-02
Epoch took 0.702s

Epoch 58 of 500
  training loss:		4.779351E-02
  validation loss:		1.605878E-02
Epoch took 0.702s

Epoch 59 of 500
  training loss:		4.665624E-02
  validation loss:		1.894971E-02
Epoch took 0.702s

Epoch 60 of 500
  training loss:		4.730540E-02
  validation loss:		1.705746E-02
Epoch took 0.704s

Epoch 61 of 500
  training loss:		4.622466E-02
  validation loss:		1.840381E-02
Epoch took 0.702s

Epoch 62 of 500
  training loss:		4.570222E-02
  validation loss:		1.772562E-02
Epoch took 0.702s

Epoch 63 of 500
  training loss:		4.859908E-02
  validation loss:		1.492385E-02
Epoch took 0.702s

Epoch 64 of 500
  training loss:		4.730746E-02
  validation loss:		1.541711E-02
Epoch took 0.702s

Epoch 65 of 500
  training loss:		4.731953E-02
  validation loss:		1.639265E-02
Epoch took 0.702s

Epoch 66 of 500
  training loss:		4.699644E-02
  validation loss:		1.743333E-02
Epoch took 0.703s

Epoch 67 of 500
  training loss:		4.491585E-02
  validation loss:		1.664927E-02
Epoch took 0.702s

Epoch 68 of 500
  training loss:		4.468939E-02
  validation loss:		1.774789E-02
Epoch took 0.702s

Epoch 69 of 500
  training loss:		4.445591E-02
  validation loss:		1.410749E-02
Epoch took 0.703s

Epoch 70 of 500
  training loss:		4.652104E-02
  validation loss:		1.707660E-02
Epoch took 0.702s

Epoch 71 of 500
  training loss:		4.500883E-02
  validation loss:		1.459475E-02
Epoch took 0.705s

Epoch 72 of 500
  training loss:		4.474909E-02
  validation loss:		1.435416E-02
Epoch took 0.703s

Epoch 73 of 500
  training loss:		4.644718E-02
  validation loss:		1.559985E-02
Epoch took 0.703s

Epoch 74 of 500
  training loss:		4.573083E-02
  validation loss:		1.359211E-02
Epoch took 0.702s

Epoch 75 of 500
  training loss:		4.587641E-02
  validation loss:		1.965198E-02
Epoch took 0.702s

Epoch 76 of 500
  training loss:		4.306951E-02
  validation loss:		1.791939E-02
Epoch took 0.703s

Epoch 77 of 500
  training loss:		4.487927E-02
  validation loss:		1.389818E-02
Epoch took 0.702s

Epoch 78 of 500
  training loss:		4.332656E-02
  validation loss:		1.563783E-02
Epoch took 0.702s

Epoch 79 of 500
  training loss:		4.376042E-02
  validation loss:		1.455567E-02
Epoch took 0.703s

Epoch 80 of 500
  training loss:		4.397520E-02
  validation loss:		1.347886E-02
Epoch took 0.702s

Epoch 81 of 500
  training loss:		4.430261E-02
  validation loss:		1.576547E-02
Epoch took 0.701s

Epoch 82 of 500
  training loss:		4.292811E-02
  validation loss:		1.824306E-02
Epoch took 0.702s

Epoch 83 of 500
  training loss:		4.540321E-02
  validation loss:		2.270892E-02
Epoch took 0.702s

Epoch 84 of 500
  training loss:		4.338274E-02
  validation loss:		1.547892E-02
Epoch took 0.703s

Epoch 85 of 500
  training loss:		4.392200E-02
  validation loss:		1.416267E-02
Epoch took 0.702s

Epoch 86 of 500
  training loss:		4.432858E-02
  validation loss:		1.880209E-02
Epoch took 0.702s

Epoch 87 of 500
  training loss:		4.353031E-02
  validation loss:		1.519580E-02
Epoch took 0.703s

Epoch 88 of 500
  training loss:		4.329424E-02
  validation loss:		1.423145E-02
Epoch took 0.702s

Epoch 89 of 500
  training loss:		4.158777E-02
  validation loss:		1.546491E-02
Epoch took 0.702s

Epoch 90 of 500
  training loss:		4.304353E-02
  validation loss:		1.760915E-02
Epoch took 0.702s

Epoch 91 of 500
  training loss:		4.209941E-02
  validation loss:		1.300050E-02
Epoch took 0.702s

Epoch 92 of 500
  training loss:		4.445404E-02
  validation loss:		1.340817E-02
Epoch took 0.702s

Epoch 93 of 500
  training loss:		4.192625E-02
  validation loss:		1.506658E-02
Epoch took 0.702s

Epoch 94 of 500
  training loss:		4.113570E-02
  validation loss:		1.805243E-02
Epoch took 0.702s

Epoch 95 of 500
  training loss:		4.358595E-02
  validation loss:		1.442024E-02
Epoch took 0.703s

Epoch 96 of 500
  training loss:		4.269079E-02
  validation loss:		1.381537E-02
Epoch took 0.702s

Epoch 97 of 500
  training loss:		4.254841E-02
  validation loss:		1.388664E-02
Epoch took 0.702s

Epoch 98 of 500
  training loss:		4.038557E-02
  validation loss:		1.716489E-02
Epoch took 0.702s

Epoch 99 of 500
  training loss:		4.238819E-02
  validation loss:		1.784156E-02
Epoch took 0.702s

Epoch 100 of 500
  training loss:		4.102556E-02
  validation loss:		1.844415E-02
Epoch took 0.702s

Epoch 101 of 500
  training loss:		4.173661E-02
  validation loss:		1.365611E-02
Epoch took 0.702s

Epoch 102 of 500
  training loss:		4.084381E-02
  validation loss:		1.592692E-02
Epoch took 0.702s

Epoch 103 of 500
  training loss:		4.174227E-02
  validation loss:		2.206297E-02
Epoch took 0.702s

Epoch 104 of 500
  training loss:		4.063065E-02
  validation loss:		1.479494E-02
Epoch took 0.702s

Epoch 105 of 500
  training loss:		4.083984E-02
  validation loss:		1.955846E-02
Epoch took 0.703s

Epoch 106 of 500
  training loss:		4.338957E-02
  validation loss:		1.296267E-02
Epoch took 0.703s

Epoch 107 of 500
  training loss:		4.117556E-02
  validation loss:		1.411101E-02
Epoch took 0.702s

Epoch 108 of 500
  training loss:		4.190806E-02
  validation loss:		1.677682E-02
Epoch took 0.703s

Epoch 109 of 500
  training loss:		4.278132E-02
  validation loss:		1.385369E-02
Epoch took 0.702s

Epoch 110 of 500
  training loss:		4.102828E-02
  validation loss:		1.297476E-02
Epoch took 0.702s

Epoch 111 of 500
  training loss:		4.213939E-02
  validation loss:		1.512829E-02
Epoch took 0.702s

Epoch 112 of 500
  training loss:		4.130324E-02
  validation loss:		1.542631E-02
Epoch took 0.702s

Epoch 113 of 500
  training loss:		4.274424E-02
  validation loss:		1.584573E-02
Epoch took 0.701s

Epoch 114 of 500
  training loss:		4.062940E-02
  validation loss:		1.222567E-02
Epoch took 0.702s

Epoch 115 of 500
  training loss:		4.057606E-02
  validation loss:		1.386266E-02
Epoch took 0.702s

Epoch 116 of 500
  training loss:		4.012602E-02
  validation loss:		1.472415E-02
Epoch took 0.703s

Epoch 117 of 500
  training loss:		4.052497E-02
  validation loss:		1.419089E-02
Epoch took 0.702s

Epoch 118 of 500
  training loss:		3.982502E-02
  validation loss:		1.752353E-02
Epoch took 0.702s

Epoch 119 of 500
  training loss:		4.169695E-02
  validation loss:		1.342118E-02
Epoch took 0.703s

Epoch 120 of 500
  training loss:		3.843913E-02
  validation loss:		1.422939E-02
Epoch took 0.702s

Epoch 121 of 500
  training loss:		4.187161E-02
  validation loss:		2.321731E-02
Epoch took 0.702s

Epoch 122 of 500
  training loss:		4.028893E-02
  validation loss:		1.238498E-02
Epoch took 0.703s

Epoch 123 of 500
  training loss:		4.067286E-02
  validation loss:		1.600833E-02
Epoch took 0.703s

Epoch 124 of 500
  training loss:		4.014410E-02
  validation loss:		1.387180E-02
Epoch took 0.702s

Epoch 125 of 500
  training loss:		4.023930E-02
  validation loss:		1.395537E-02
Epoch took 0.702s

Epoch 126 of 500
  training loss:		3.905874E-02
  validation loss:		1.525480E-02
Epoch took 0.703s

Epoch 127 of 500
  training loss:		4.091961E-02
  validation loss:		1.442891E-02
Epoch took 0.702s

Epoch 128 of 500
  training loss:		4.105238E-02
  validation loss:		1.454385E-02
Epoch took 0.702s

Epoch 129 of 500
  training loss:		4.079154E-02
  validation loss:		1.312645E-02
Epoch took 0.702s

Epoch 130 of 500
  training loss:		3.987583E-02
  validation loss:		1.398557E-02
Epoch took 0.703s

Epoch 131 of 500
  training loss:		3.833967E-02
  validation loss:		1.691477E-02
Epoch took 0.702s

Epoch 132 of 500
  training loss:		4.039228E-02
  validation loss:		1.430222E-02
Epoch took 0.702s

Epoch 133 of 500
  training loss:		3.850222E-02
  validation loss:		1.627080E-02
Epoch took 0.703s

Epoch 134 of 500
  training loss:		3.933598E-02
  validation loss:		1.311041E-02
Epoch took 0.702s

Epoch 135 of 500
  training loss:		3.887113E-02
  validation loss:		1.364242E-02
Epoch took 0.703s

Early stopping, val-loss increased over the last 15 epochs from 0.0144837839943 to 0.0150011983459
Saving model from epoch 120
Training RMSE: 0.0142275
Validation RMSE: 0.0142546
Test RMSE: 0.0142812225968
Test MSE: 0.00020395332831
Test MAE: 0.0107132308185
Test R2: -2171250159.74 

