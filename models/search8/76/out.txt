Training network with 56340 training samples and 18780 validation samples
Epoch 1 of 500
  training loss:		2.002609E-01
  validation loss:		9.226757E-02
Epoch took 0.816s

Epoch 2 of 500
  training loss:		6.288299E-02
  validation loss:		4.389620E-02
Epoch took 0.776s

Epoch 3 of 500
  training loss:		4.840394E-02
  validation loss:		4.537325E-02
Epoch took 0.777s

Epoch 4 of 500
  training loss:		3.927859E-02
  validation loss:		6.179010E-02
Epoch took 0.776s

Epoch 5 of 500
  training loss:		3.685303E-02
  validation loss:		2.399947E-02
Epoch took 0.776s

Epoch 6 of 500
  training loss:		2.954369E-02
  validation loss:		1.834756E-02
Epoch took 0.776s

Epoch 7 of 500
  training loss:		3.170726E-02
  validation loss:		2.760850E-02
Epoch took 0.776s

Epoch 8 of 500
  training loss:		3.172081E-02
  validation loss:		3.902072E-02
Epoch took 0.776s

Epoch 9 of 500
  training loss:		2.981129E-02
  validation loss:		1.770812E-02
Epoch took 0.776s

Epoch 10 of 500
  training loss:		2.328474E-02
  validation loss:		2.596485E-02
Epoch took 0.776s

Epoch 11 of 500
  training loss:		2.170949E-02
  validation loss:		3.727721E-02
Epoch took 0.776s

Epoch 12 of 500
  training loss:		2.411670E-02
  validation loss:		2.192510E-02
Epoch took 0.776s

Epoch 13 of 500
  training loss:		1.917695E-02
  validation loss:		1.811828E-02
Epoch took 0.776s

Epoch 14 of 500
  training loss:		1.879043E-02
  validation loss:		2.857387E-02
Epoch took 0.776s

Epoch 15 of 500
  training loss:		1.774833E-02
  validation loss:		1.647166E-02
Epoch took 0.776s

Epoch 16 of 500
  training loss:		1.676655E-02
  validation loss:		1.675328E-02
Epoch took 0.776s

Epoch 17 of 500
  training loss:		1.781781E-02
  validation loss:		2.601329E-02
Epoch took 0.776s

Epoch 18 of 500
  training loss:		1.739775E-02
  validation loss:		1.751702E-02
Epoch took 0.776s

Epoch 19 of 500
  training loss:		1.502073E-02
  validation loss:		1.159301E-02
Epoch took 0.776s

Epoch 20 of 500
  training loss:		1.676236E-02
  validation loss:		1.303855E-02
Epoch took 0.776s

Epoch 21 of 500
  training loss:		1.797089E-02
  validation loss:		1.497287E-02
Epoch took 0.776s

Epoch 22 of 500
  training loss:		1.618126E-02
  validation loss:		2.114621E-02
Epoch took 0.776s

Epoch 23 of 500
  training loss:		1.608393E-02
  validation loss:		1.894716E-02
Epoch took 0.776s

Epoch 24 of 500
  training loss:		1.555524E-02
  validation loss:		1.361253E-02
Epoch took 0.776s

Epoch 25 of 500
  training loss:		1.544494E-02
  validation loss:		1.375051E-02
Epoch took 0.776s

Epoch 26 of 500
  training loss:		1.314583E-02
  validation loss:		1.871770E-02
Epoch took 0.776s

Epoch 27 of 500
  training loss:		1.458891E-02
  validation loss:		1.889293E-02
Epoch took 0.779s

Epoch 28 of 500
  training loss:		1.559567E-02
  validation loss:		3.671677E-02
Epoch took 0.776s

Epoch 29 of 500
  training loss:		1.362073E-02
  validation loss:		1.897401E-02
Epoch took 0.776s

Epoch 30 of 500
  training loss:		1.415764E-02
  validation loss:		3.174142E-02
Epoch took 0.777s

Epoch 31 of 500
  training loss:		1.458414E-02
  validation loss:		1.352237E-02
Epoch took 0.776s

Epoch 32 of 500
  training loss:		1.275019E-02
  validation loss:		1.016702E-02
Epoch took 0.776s

Epoch 33 of 500
  training loss:		1.221854E-02
  validation loss:		1.692649E-02
Epoch took 0.777s

Epoch 34 of 500
  training loss:		1.351441E-02
  validation loss:		2.450862E-02
Epoch took 0.777s

Epoch 35 of 500
  training loss:		1.832934E-02
  validation loss:		1.330664E-02
Epoch took 0.777s

Epoch 36 of 500
  training loss:		1.405573E-02
  validation loss:		1.742917E-02
Epoch took 0.776s

Epoch 37 of 500
  training loss:		1.271013E-02
  validation loss:		9.651532E-03
Epoch took 0.776s

Epoch 38 of 500
  training loss:		1.221870E-02
  validation loss:		2.987750E-02
Epoch took 0.776s

Epoch 39 of 500
  training loss:		1.305082E-02
  validation loss:		2.094549E-02
Epoch took 0.776s

Epoch 40 of 500
  training loss:		1.234730E-02
  validation loss:		2.017605E-02
Epoch took 0.777s

Epoch 41 of 500
  training loss:		1.185467E-02
  validation loss:		2.041842E-02
Epoch took 0.776s

Epoch 42 of 500
  training loss:		1.038073E-02
  validation loss:		1.190730E-02
Epoch took 0.776s

Epoch 43 of 500
  training loss:		1.094371E-02
  validation loss:		6.074322E-03
Epoch took 0.776s

Epoch 44 of 500
  training loss:		1.148533E-02
  validation loss:		2.894337E-02
Epoch took 0.776s

Epoch 45 of 500
  training loss:		1.284412E-02
  validation loss:		1.091108E-02
Epoch took 0.776s

Epoch 46 of 500
  training loss:		1.203745E-02
  validation loss:		2.341025E-02
Epoch took 0.776s

Epoch 47 of 500
  training loss:		1.102932E-02
  validation loss:		9.242850E-03
Epoch took 0.776s

Epoch 48 of 500
  training loss:		1.225769E-02
  validation loss:		1.018386E-02
Epoch took 0.777s

Epoch 49 of 500
  training loss:		1.385186E-02
  validation loss:		2.806923E-02
Epoch took 0.777s

Epoch 50 of 500
  training loss:		1.171507E-02
  validation loss:		2.104643E-02
Epoch took 0.776s

Epoch 51 of 500
  training loss:		9.709266E-03
  validation loss:		1.332804E-02
Epoch took 0.783s

Epoch 52 of 500
  training loss:		1.162992E-02
  validation loss:		1.782583E-02
Epoch took 0.786s

Epoch 53 of 500
  training loss:		1.089134E-02
  validation loss:		8.702765E-03
Epoch took 0.777s

Epoch 54 of 500
  training loss:		1.043109E-02
  validation loss:		1.558256E-02
Epoch took 0.777s

Epoch 55 of 500
  training loss:		1.287555E-02
  validation loss:		9.404558E-03
Epoch took 0.777s

Epoch 56 of 500
  training loss:		1.087057E-02
  validation loss:		1.806618E-02
Epoch took 0.777s

Epoch 57 of 500
  training loss:		1.082608E-02
  validation loss:		2.464381E-02
Epoch took 0.777s

Epoch 58 of 500
  training loss:		1.138334E-02
  validation loss:		2.316172E-02
Epoch took 0.777s

Epoch 59 of 500
  training loss:		1.151681E-02
  validation loss:		1.411801E-02
Epoch took 0.777s

Epoch 60 of 500
  training loss:		1.121992E-02
  validation loss:		8.713264E-03
Epoch took 0.777s

Epoch 61 of 500
  training loss:		9.881613E-03
  validation loss:		1.627187E-02
Epoch took 0.777s

Epoch 62 of 500
  training loss:		1.033223E-02
  validation loss:		7.800203E-03
Epoch took 0.777s

Epoch 63 of 500
  training loss:		9.866822E-03
  validation loss:		1.729436E-02
Epoch took 0.777s

Epoch 64 of 500
  training loss:		1.023943E-02
  validation loss:		2.704337E-02
Epoch took 0.777s

Epoch 65 of 500
  training loss:		1.048650E-02
  validation loss:		1.468791E-02
Epoch took 0.777s

Epoch 66 of 500
  training loss:		1.014371E-02
  validation loss:		1.292623E-02
Epoch took 0.777s

Epoch 67 of 500
  training loss:		9.992272E-03
  validation loss:		2.114071E-02
Epoch took 0.779s

Epoch 68 of 500
  training loss:		9.273971E-03
  validation loss:		1.465215E-02
Epoch took 0.777s

Epoch 69 of 500
  training loss:		9.932751E-03
  validation loss:		2.032898E-02
Epoch took 0.778s

Epoch 70 of 500
  training loss:		9.810760E-03
  validation loss:		1.163370E-02
Epoch took 0.778s

Epoch 71 of 500
  training loss:		9.068615E-03
  validation loss:		3.103222E-02
Epoch took 0.778s

Epoch 72 of 500
  training loss:		8.906367E-03
  validation loss:		2.060805E-02
Epoch took 0.778s

Epoch 73 of 500
  training loss:		1.088768E-02
  validation loss:		1.901491E-02
Epoch took 0.777s

Epoch 74 of 500
  training loss:		9.537600E-03
  validation loss:		1.745197E-02
Epoch took 0.778s

Epoch 75 of 500
  training loss:		9.997681E-03
  validation loss:		1.067329E-02
Epoch took 0.778s

Epoch 76 of 500
  training loss:		8.953003E-03
  validation loss:		1.439441E-02
Epoch took 0.777s

Epoch 77 of 500
  training loss:		8.761005E-03
  validation loss:		9.963776E-03
Epoch took 0.778s

Epoch 78 of 500
  training loss:		9.480044E-03
  validation loss:		2.860272E-02
Epoch took 0.778s

Epoch 79 of 500
  training loss:		8.066093E-03
  validation loss:		1.565319E-02
Epoch took 0.779s

Epoch 80 of 500
  training loss:		9.424714E-03
  validation loss:		1.952468E-02
Epoch took 0.779s

Early stopping, val-loss increased over the last 20 epochs from 0.0161876927874 to 0.0175349347895
Saving model from epoch 60
Training RMSE: 0.00877007
Validation RMSE: 0.00871441
Test RMSE: 0.00874474458396
Test MSE: 7.64705546317e-05
Test MAE: 0.00730070145801
Test R2: -814091786.533 

