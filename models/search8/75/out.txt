Training network with 56340 training samples and 18780 validation samples
Epoch 1 of 500
  training loss:		1.707411E-01
  validation loss:		3.149021E-01
Epoch took 0.824s

Epoch 2 of 500
  training loss:		7.618352E-02
  validation loss:		8.071833E-02
Epoch took 0.775s

Epoch 3 of 500
  training loss:		6.012113E-02
  validation loss:		1.261980E-01
Epoch took 0.775s

Epoch 4 of 500
  training loss:		4.944362E-02
  validation loss:		4.452779E-02
Epoch took 0.775s

Epoch 5 of 500
  training loss:		4.324175E-02
  validation loss:		4.842361E-02
Epoch took 0.775s

Epoch 6 of 500
  training loss:		3.803117E-02
  validation loss:		3.456516E-02
Epoch took 0.775s

Epoch 7 of 500
  training loss:		3.396513E-02
  validation loss:		2.746318E-02
Epoch took 0.775s

Epoch 8 of 500
  training loss:		3.178729E-02
  validation loss:		3.627189E-02
Epoch took 0.775s

Epoch 9 of 500
  training loss:		3.191598E-02
  validation loss:		6.324367E-02
Epoch took 0.775s

Epoch 10 of 500
  training loss:		2.831722E-02
  validation loss:		2.573335E-02
Epoch took 0.775s

Epoch 11 of 500
  training loss:		2.612415E-02
  validation loss:		3.536695E-02
Epoch took 0.775s

Epoch 12 of 500
  training loss:		2.553828E-02
  validation loss:		2.129931E-02
Epoch took 0.775s

Epoch 13 of 500
  training loss:		2.283672E-02
  validation loss:		4.206990E-02
Epoch took 0.775s

Epoch 14 of 500
  training loss:		2.221810E-02
  validation loss:		2.520982E-02
Epoch took 0.775s

Epoch 15 of 500
  training loss:		2.362634E-02
  validation loss:		1.311546E-02
Epoch took 0.775s

Epoch 16 of 500
  training loss:		2.110884E-02
  validation loss:		1.201757E-02
Epoch took 0.775s

Epoch 17 of 500
  training loss:		2.546236E-02
  validation loss:		1.422090E-02
Epoch took 0.775s

Epoch 18 of 500
  training loss:		2.021789E-02
  validation loss:		1.874671E-02
Epoch took 0.775s

Epoch 19 of 500
  training loss:		1.804681E-02
  validation loss:		1.303735E-02
Epoch took 0.775s

Epoch 20 of 500
  training loss:		1.976705E-02
  validation loss:		1.558068E-02
Epoch took 0.775s

Epoch 21 of 500
  training loss:		2.019011E-02
  validation loss:		2.802658E-02
Epoch took 0.775s

Epoch 22 of 500
  training loss:		1.903750E-02
  validation loss:		1.413962E-02
Epoch took 0.775s

Epoch 23 of 500
  training loss:		1.642881E-02
  validation loss:		1.472447E-02
Epoch took 0.775s

Epoch 24 of 500
  training loss:		1.662699E-02
  validation loss:		1.116298E-02
Epoch took 0.775s

Epoch 25 of 500
  training loss:		1.501059E-02
  validation loss:		1.311293E-02
Epoch took 0.775s

Epoch 26 of 500
  training loss:		1.576719E-02
  validation loss:		1.122747E-02
Epoch took 0.775s

Epoch 27 of 500
  training loss:		1.841847E-02
  validation loss:		2.035460E-02
Epoch took 0.775s

Epoch 28 of 500
  training loss:		1.738704E-02
  validation loss:		1.726079E-02
Epoch took 0.775s

Epoch 29 of 500
  training loss:		1.601529E-02
  validation loss:		8.595709E-03
Epoch took 0.775s

Epoch 30 of 500
  training loss:		1.645179E-02
  validation loss:		1.736662E-02
Epoch took 0.775s

Epoch 31 of 500
  training loss:		1.501130E-02
  validation loss:		8.726953E-03
Epoch took 0.775s

Epoch 32 of 500
  training loss:		1.485510E-02
  validation loss:		1.015608E-02
Epoch took 0.775s

Epoch 33 of 500
  training loss:		1.234838E-02
  validation loss:		9.001671E-03
Epoch took 0.775s

Epoch 34 of 500
  training loss:		1.266920E-02
  validation loss:		9.893667E-03
Epoch took 0.775s

Epoch 35 of 500
  training loss:		1.659015E-02
  validation loss:		1.081407E-02
Epoch took 0.775s

Epoch 36 of 500
  training loss:		1.510201E-02
  validation loss:		8.114714E-03
Epoch took 0.775s

Epoch 37 of 500
  training loss:		1.311246E-02
  validation loss:		1.517105E-02
Epoch took 0.775s

Epoch 38 of 500
  training loss:		1.278632E-02
  validation loss:		1.039422E-02
Epoch took 0.775s

Epoch 39 of 500
  training loss:		1.350236E-02
  validation loss:		2.064072E-02
Epoch took 0.775s

Epoch 40 of 500
  training loss:		1.382970E-02
  validation loss:		1.135576E-02
Epoch took 0.775s

Epoch 41 of 500
  training loss:		1.100731E-02
  validation loss:		7.489495E-03
Epoch took 0.775s

Epoch 42 of 500
  training loss:		1.199101E-02
  validation loss:		7.882178E-03
Epoch took 0.775s

Epoch 43 of 500
  training loss:		1.130345E-02
  validation loss:		1.493923E-02
Epoch took 0.775s

Epoch 44 of 500
  training loss:		1.060389E-02
  validation loss:		7.468017E-03
Epoch took 0.775s

Epoch 45 of 500
  training loss:		1.244682E-02
  validation loss:		9.305839E-03
Epoch took 0.775s

Epoch 46 of 500
  training loss:		1.082800E-02
  validation loss:		1.578771E-02
Epoch took 0.775s

Epoch 47 of 500
  training loss:		1.507203E-02
  validation loss:		1.149156E-02
Epoch took 0.777s

Epoch 48 of 500
  training loss:		1.260152E-02
  validation loss:		1.241819E-02
Epoch took 0.777s

Epoch 49 of 500
  training loss:		1.303260E-02
  validation loss:		7.122558E-03
Epoch took 0.777s

Epoch 50 of 500
  training loss:		1.095656E-02
  validation loss:		1.063036E-02
Epoch took 0.777s

Epoch 51 of 500
  training loss:		1.004607E-02
  validation loss:		6.841774E-03
Epoch took 0.777s

Epoch 52 of 500
  training loss:		1.132524E-02
  validation loss:		8.417767E-03
Epoch took 0.777s

Epoch 53 of 500
  training loss:		1.075462E-02
  validation loss:		8.813810E-03
Epoch took 0.777s

Epoch 54 of 500
  training loss:		1.160355E-02
  validation loss:		8.808875E-03
Epoch took 0.777s

Epoch 55 of 500
  training loss:		1.143672E-02
  validation loss:		1.381481E-02
Epoch took 0.777s

Epoch 56 of 500
  training loss:		1.175286E-02
  validation loss:		1.386855E-02
Epoch took 0.777s

Epoch 57 of 500
  training loss:		1.520141E-02
  validation loss:		1.355189E-02
Epoch took 0.777s

Epoch 58 of 500
  training loss:		1.057581E-02
  validation loss:		1.049564E-02
Epoch took 0.777s

Epoch 59 of 500
  training loss:		1.023950E-02
  validation loss:		1.394321E-02
Epoch took 0.777s

Epoch 60 of 500
  training loss:		9.361348E-03
  validation loss:		1.114430E-02
Epoch took 0.777s

Epoch 61 of 500
  training loss:		9.514814E-03
  validation loss:		1.884431E-02
Epoch took 0.777s

Epoch 62 of 500
  training loss:		1.059477E-02
  validation loss:		1.242436E-02
Epoch took 0.777s

Epoch 63 of 500
  training loss:		9.973448E-03
  validation loss:		1.280308E-02
Epoch took 0.777s

Epoch 64 of 500
  training loss:		1.194266E-02
  validation loss:		1.455542E-02
Epoch took 0.777s

Epoch 65 of 500
  training loss:		9.241551E-03
  validation loss:		1.008569E-02
Epoch took 0.777s

Epoch 66 of 500
  training loss:		9.208423E-03
  validation loss:		1.407781E-02
Epoch took 0.778s

Epoch 67 of 500
  training loss:		8.887045E-03
  validation loss:		1.594219E-02
Epoch took 0.778s

Epoch 68 of 500
  training loss:		9.443637E-03
  validation loss:		6.614351E-03
Epoch took 0.778s

Epoch 69 of 500
  training loss:		8.881658E-03
  validation loss:		9.205141E-03
Epoch took 0.777s

Epoch 70 of 500
  training loss:		1.009502E-02
  validation loss:		9.590725E-03
Epoch took 0.777s

Epoch 71 of 500
  training loss:		8.563707E-03
  validation loss:		6.838022E-03
Epoch took 0.777s

Epoch 72 of 500
  training loss:		1.045439E-02
  validation loss:		1.492217E-02
Epoch took 0.778s

Epoch 73 of 500
  training loss:		8.956167E-03
  validation loss:		5.654826E-03
Epoch took 0.777s

Epoch 74 of 500
  training loss:		7.627865E-03
  validation loss:		7.782526E-03
Epoch took 0.777s

Epoch 75 of 500
  training loss:		8.873863E-03
  validation loss:		9.162198E-03
Epoch took 0.778s

Epoch 76 of 500
  training loss:		1.228304E-02
  validation loss:		1.807985E-02
Epoch took 0.778s

Epoch 77 of 500
  training loss:		1.148980E-02
  validation loss:		1.091132E-02
Epoch took 0.777s

Epoch 78 of 500
  training loss:		9.459154E-03
  validation loss:		7.208621E-03
Epoch took 0.777s

Epoch 79 of 500
  training loss:		8.137639E-03
  validation loss:		7.307141E-03
Epoch took 0.777s

Epoch 80 of 500
  training loss:		8.877656E-03
  validation loss:		7.706182E-03
Epoch took 0.778s

Early stopping, val-loss increased over the last 20 epochs from 0.0107117889863 to 0.0109857964659
Saving model from epoch 60
Training RMSE: 0.0111795
Validation RMSE: 0.0111504
Test RMSE: 0.0109297418967
Test MSE: 0.000119459247799
Test MAE: 0.00852215755731
Test R2: -1271741492.16 

