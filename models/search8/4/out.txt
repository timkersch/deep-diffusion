Training network with 56340 training samples and 18780 validation samples
Epoch 1 of 500
  training loss:		1.277944E-01
  validation loss:		4.623985E-02
Epoch took 2.061s

Epoch 2 of 500
  training loss:		4.117938E-02
  validation loss:		2.341252E-02
Epoch took 2.014s

Epoch 3 of 500
  training loss:		2.730797E-02
  validation loss:		1.464593E-02
Epoch took 2.014s

Epoch 4 of 500
  training loss:		2.228008E-02
  validation loss:		1.793968E-02
Epoch took 2.010s

Epoch 5 of 500
  training loss:		1.872520E-02
  validation loss:		8.167477E-03
Epoch took 2.011s

Epoch 6 of 500
  training loss:		1.625135E-02
  validation loss:		1.421974E-02
Epoch took 2.010s

Epoch 7 of 500
  training loss:		1.514718E-02
  validation loss:		1.750954E-02
Epoch took 2.011s

Epoch 8 of 500
  training loss:		1.354639E-02
  validation loss:		1.436387E-02
Epoch took 2.010s

Epoch 9 of 500
  training loss:		1.342226E-02
  validation loss:		1.068777E-02
Epoch took 2.011s

Epoch 10 of 500
  training loss:		1.172967E-02
  validation loss:		9.814848E-03
Epoch took 2.010s

Epoch 11 of 500
  training loss:		1.108472E-02
  validation loss:		2.147671E-02
Epoch took 2.011s

Epoch 12 of 500
  training loss:		1.004160E-02
  validation loss:		3.847851E-03
Epoch took 2.011s

Epoch 13 of 500
  training loss:		9.808788E-03
  validation loss:		8.124275E-03
Epoch took 2.011s

Epoch 14 of 500
  training loss:		9.341491E-03
  validation loss:		1.567322E-02
Epoch took 2.010s

Epoch 15 of 500
  training loss:		9.228628E-03
  validation loss:		7.384125E-03
Epoch took 2.010s

Epoch 16 of 500
  training loss:		8.828922E-03
  validation loss:		8.247588E-03
Epoch took 2.010s

Epoch 17 of 500
  training loss:		9.017615E-03
  validation loss:		2.414692E-02
Epoch took 2.011s

Epoch 18 of 500
  training loss:		8.010187E-03
  validation loss:		6.442891E-03
Epoch took 2.011s

Epoch 19 of 500
  training loss:		7.471967E-03
  validation loss:		7.787760E-03
Epoch took 2.010s

Epoch 20 of 500
  training loss:		7.489205E-03
  validation loss:		4.011760E-03
Epoch took 2.011s

Epoch 21 of 500
  training loss:		6.846775E-03
  validation loss:		8.064853E-03
Epoch took 2.011s

Epoch 22 of 500
  training loss:		6.861887E-03
  validation loss:		8.607950E-03
Epoch took 2.011s

Epoch 23 of 500
  training loss:		6.298901E-03
  validation loss:		5.776361E-03
Epoch took 2.012s

Epoch 24 of 500
  training loss:		7.092010E-03
  validation loss:		8.346671E-03
Epoch took 2.011s

Epoch 25 of 500
  training loss:		6.157819E-03
  validation loss:		6.179519E-03
Epoch took 2.011s

Epoch 26 of 500
  training loss:		5.958009E-03
  validation loss:		4.580691E-03
Epoch took 2.012s

Epoch 27 of 500
  training loss:		5.911844E-03
  validation loss:		8.808379E-03
Epoch took 2.011s

Epoch 28 of 500
  training loss:		6.230609E-03
  validation loss:		8.726014E-03
Epoch took 2.011s

Epoch 29 of 500
  training loss:		5.040952E-03
  validation loss:		5.123977E-03
Epoch took 2.011s

Epoch 30 of 500
  training loss:		5.565657E-03
  validation loss:		9.417894E-03
Epoch took 2.011s

Epoch 31 of 500
  training loss:		5.589455E-03
  validation loss:		5.297054E-03
Epoch took 2.011s

Epoch 32 of 500
  training loss:		5.105271E-03
  validation loss:		8.270867E-03
Epoch took 2.011s

Epoch 33 of 500
  training loss:		4.931894E-03
  validation loss:		8.054905E-03
Epoch took 2.011s

Epoch 34 of 500
  training loss:		5.403876E-03
  validation loss:		3.119825E-03
Epoch took 2.011s

Epoch 35 of 500
  training loss:		4.576516E-03
  validation loss:		4.199805E-03
Epoch took 2.011s

Epoch 36 of 500
  training loss:		4.358192E-03
  validation loss:		8.729829E-03
Epoch took 2.011s

Epoch 37 of 500
  training loss:		4.894236E-03
  validation loss:		4.898759E-03
Epoch took 2.011s

Epoch 38 of 500
  training loss:		4.438842E-03
  validation loss:		3.207247E-03
Epoch took 2.011s

Epoch 39 of 500
  training loss:		5.124029E-03
  validation loss:		7.507756E-03
Epoch took 2.011s

Epoch 40 of 500
  training loss:		4.380768E-03
  validation loss:		8.686819E-03
Epoch took 2.010s

Epoch 41 of 500
  training loss:		4.304483E-03
  validation loss:		3.546458E-03
Epoch took 2.011s

Epoch 42 of 500
  training loss:		4.255637E-03
  validation loss:		8.251926E-03
Epoch took 2.011s

Epoch 43 of 500
  training loss:		4.441728E-03
  validation loss:		4.817315E-03
Epoch took 2.010s

Epoch 44 of 500
  training loss:		4.340157E-03
  validation loss:		2.102003E-03
Epoch took 2.010s

Epoch 45 of 500
  training loss:		4.288876E-03
  validation loss:		5.083206E-03
Epoch took 2.010s

Epoch 46 of 500
  training loss:		4.293269E-03
  validation loss:		1.378496E-02
Epoch took 2.011s

Epoch 47 of 500
  training loss:		4.153412E-03
  validation loss:		8.980666E-03
Epoch took 2.011s

Epoch 48 of 500
  training loss:		4.011089E-03
  validation loss:		5.988911E-03
Epoch took 2.011s

Epoch 49 of 500
  training loss:		3.835721E-03
  validation loss:		5.061269E-03
Epoch took 2.010s

Epoch 50 of 500
  training loss:		3.908433E-03
  validation loss:		9.959000E-03
Epoch took 2.010s

Early stopping, val-loss increased over the last 10 epochs from 0.00619728664809 to 0.00675757089781
Saving model from epoch 40
Training RMSE: 0.00881086
Validation RMSE: 0.00877851
Test RMSE: 0.00859395880252
Test MSE: 7.38561357139e-05
Test MAE: 0.00567576754838
Test R2: -786258950.314 

