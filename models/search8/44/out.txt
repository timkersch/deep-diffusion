Training network with 56340 training samples and 18780 validation samples
Epoch 1 of 500
  training loss:		9.053873E-01
  validation loss:		1.327207E+00
Epoch took 0.832s

Epoch 2 of 500
  training loss:		4.048064E-01
  validation loss:		2.093038E-01
Epoch took 0.774s

Epoch 3 of 500
  training loss:		1.527081E-01
  validation loss:		7.308092E-02
Epoch took 0.774s

Epoch 4 of 500
  training loss:		1.017012E-01
  validation loss:		5.741813E-02
Epoch took 0.774s

Epoch 5 of 500
  training loss:		8.988597E-02
  validation loss:		5.616929E-02
Epoch took 0.774s

Epoch 6 of 500
  training loss:		8.451821E-02
  validation loss:		6.833299E-02
Epoch took 0.777s

Epoch 7 of 500
  training loss:		7.975906E-02
  validation loss:		3.679627E-02
Epoch took 0.774s

Epoch 8 of 500
  training loss:		7.807782E-02
  validation loss:		4.203619E-02
Epoch took 0.774s

Epoch 9 of 500
  training loss:		7.368414E-02
  validation loss:		3.432207E-02
Epoch took 0.774s

Epoch 10 of 500
  training loss:		7.185120E-02
  validation loss:		4.873217E-02
Epoch took 0.774s

Epoch 11 of 500
  training loss:		7.044930E-02
  validation loss:		3.238939E-02
Epoch took 0.774s

Epoch 12 of 500
  training loss:		6.995816E-02
  validation loss:		4.673558E-02
Epoch took 0.774s

Epoch 13 of 500
  training loss:		7.008693E-02
  validation loss:		3.818469E-02
Epoch took 0.774s

Epoch 14 of 500
  training loss:		6.463858E-02
  validation loss:		2.845647E-02
Epoch took 0.774s

Epoch 15 of 500
  training loss:		6.353021E-02
  validation loss:		4.376814E-02
Epoch took 0.774s

Epoch 16 of 500
  training loss:		6.321750E-02
  validation loss:		3.292954E-02
Epoch took 0.774s

Epoch 17 of 500
  training loss:		6.255929E-02
  validation loss:		4.034470E-02
Epoch took 0.774s

Epoch 18 of 500
  training loss:		6.173130E-02
  validation loss:		3.181189E-02
Epoch took 0.774s

Epoch 19 of 500
  training loss:		6.067717E-02
  validation loss:		2.733769E-02
Epoch took 0.774s

Epoch 20 of 500
  training loss:		5.804977E-02
  validation loss:		3.010753E-02
Epoch took 0.774s

Epoch 21 of 500
  training loss:		5.612682E-02
  validation loss:		3.368244E-02
Epoch took 0.774s

Epoch 22 of 500
  training loss:		5.691064E-02
  validation loss:		2.481790E-02
Epoch took 0.774s

Epoch 23 of 500
  training loss:		5.505657E-02
  validation loss:		2.508735E-02
Epoch took 0.774s

Epoch 24 of 500
  training loss:		5.687653E-02
  validation loss:		3.307245E-02
Epoch took 0.774s

Epoch 25 of 500
  training loss:		5.434514E-02
  validation loss:		3.090425E-02
Epoch took 0.774s

Epoch 26 of 500
  training loss:		5.462748E-02
  validation loss:		2.514658E-02
Epoch took 0.774s

Epoch 27 of 500
  training loss:		5.435574E-02
  validation loss:		2.599278E-02
Epoch took 0.774s

Epoch 28 of 500
  training loss:		5.304064E-02
  validation loss:		2.540903E-02
Epoch took 0.774s

Epoch 29 of 500
  training loss:		5.179546E-02
  validation loss:		2.549512E-02
Epoch took 0.774s

Epoch 30 of 500
  training loss:		5.264598E-02
  validation loss:		4.115118E-02
Epoch took 0.774s

Epoch 31 of 500
  training loss:		5.124799E-02
  validation loss:		2.486079E-02
Epoch took 0.774s

Epoch 32 of 500
  training loss:		4.919448E-02
  validation loss:		2.733350E-02
Epoch took 0.774s

Epoch 33 of 500
  training loss:		4.968276E-02
  validation loss:		4.594283E-02
Epoch took 0.774s

Epoch 34 of 500
  training loss:		5.108676E-02
  validation loss:		2.815113E-02
Epoch took 0.774s

Epoch 35 of 500
  training loss:		4.859917E-02
  validation loss:		3.343126E-02
Epoch took 0.774s

Epoch 36 of 500
  training loss:		4.770203E-02
  validation loss:		2.718153E-02
Epoch took 0.774s

Epoch 37 of 500
  training loss:		4.594713E-02
  validation loss:		2.599647E-02
Epoch took 0.774s

Epoch 38 of 500
  training loss:		4.658736E-02
  validation loss:		2.547399E-02
Epoch took 0.774s

Epoch 39 of 500
  training loss:		4.677220E-02
  validation loss:		2.706612E-02
Epoch took 0.774s

Epoch 40 of 500
  training loss:		4.650967E-02
  validation loss:		2.449081E-02
Epoch took 0.774s

Epoch 41 of 500
  training loss:		4.750459E-02
  validation loss:		2.753033E-02
Epoch took 0.774s

Epoch 42 of 500
  training loss:		4.668711E-02
  validation loss:		2.714330E-02
Epoch took 0.774s

Epoch 43 of 500
  training loss:		4.471153E-02
  validation loss:		2.478924E-02
Epoch took 0.774s

Epoch 44 of 500
  training loss:		4.692413E-02
  validation loss:		2.914539E-02
Epoch took 0.774s

Epoch 45 of 500
  training loss:		4.578288E-02
  validation loss:		3.485637E-02
Epoch took 0.774s

Epoch 46 of 500
  training loss:		4.486983E-02
  validation loss:		2.369617E-02
Epoch took 0.774s

Epoch 47 of 500
  training loss:		4.372604E-02
  validation loss:		2.230426E-02
Epoch took 0.774s

Epoch 48 of 500
  training loss:		4.265826E-02
  validation loss:		3.443013E-02
Epoch took 0.774s

Epoch 49 of 500
  training loss:		4.345927E-02
  validation loss:		2.586223E-02
Epoch took 0.774s

Epoch 50 of 500
  training loss:		4.450224E-02
  validation loss:		2.969234E-02
Epoch took 0.774s

Epoch 51 of 500
  training loss:		4.512326E-02
  validation loss:		1.979875E-02
Epoch took 0.774s

Epoch 52 of 500
  training loss:		4.196787E-02
  validation loss:		2.034258E-02
Epoch took 0.774s

Epoch 53 of 500
  training loss:		4.222722E-02
  validation loss:		2.583075E-02
Epoch took 0.774s

Epoch 54 of 500
  training loss:		4.186130E-02
  validation loss:		2.477588E-02
Epoch took 0.774s

Epoch 55 of 500
  training loss:		4.191062E-02
  validation loss:		2.840303E-02
Epoch took 0.774s

Epoch 56 of 500
  training loss:		4.022094E-02
  validation loss:		2.232256E-02
Epoch took 0.776s

Epoch 57 of 500
  training loss:		4.116466E-02
  validation loss:		2.389857E-02
Epoch took 0.777s

Epoch 58 of 500
  training loss:		3.989111E-02
  validation loss:		2.865922E-02
Epoch took 0.776s

Epoch 59 of 500
  training loss:		4.062217E-02
  validation loss:		3.157008E-02
Epoch took 0.776s

Epoch 60 of 500
  training loss:		4.011900E-02
  validation loss:		2.190659E-02
Epoch took 0.777s

Epoch 61 of 500
  training loss:		4.006599E-02
  validation loss:		2.168712E-02
Epoch took 0.776s

Epoch 62 of 500
  training loss:		4.096731E-02
  validation loss:		2.802607E-02
Epoch took 0.776s

Epoch 63 of 500
  training loss:		4.062389E-02
  validation loss:		1.985473E-02
Epoch took 0.776s

Epoch 64 of 500
  training loss:		3.963708E-02
  validation loss:		2.845515E-02
Epoch took 0.777s

Epoch 65 of 500
  training loss:		3.957295E-02
  validation loss:		1.962384E-02
Epoch took 0.776s

Epoch 66 of 500
  training loss:		3.896145E-02
  validation loss:		2.609911E-02
Epoch took 0.776s

Epoch 67 of 500
  training loss:		3.914952E-02
  validation loss:		2.015311E-02
Epoch took 0.776s

Epoch 68 of 500
  training loss:		3.841514E-02
  validation loss:		2.282898E-02
Epoch took 0.776s

Epoch 69 of 500
  training loss:		3.789670E-02
  validation loss:		1.933944E-02
Epoch took 0.776s

Epoch 70 of 500
  training loss:		3.828684E-02
  validation loss:		3.153816E-02
Epoch took 0.776s

Epoch 71 of 500
  training loss:		3.872589E-02
  validation loss:		2.422415E-02
Epoch took 0.776s

Epoch 72 of 500
  training loss:		3.660063E-02
  validation loss:		1.866161E-02
Epoch took 0.776s

Epoch 73 of 500
  training loss:		3.744241E-02
  validation loss:		2.240370E-02
Epoch took 0.776s

Epoch 74 of 500
  training loss:		3.772293E-02
  validation loss:		2.316929E-02
Epoch took 0.776s

Epoch 75 of 500
  training loss:		3.693547E-02
  validation loss:		2.857134E-02
Epoch took 0.776s

Epoch 76 of 500
  training loss:		3.635823E-02
  validation loss:		2.873018E-02
Epoch took 0.776s

Epoch 77 of 500
  training loss:		3.572556E-02
  validation loss:		2.099762E-02
Epoch took 0.776s

Epoch 78 of 500
  training loss:		3.712497E-02
  validation loss:		2.003884E-02
Epoch took 0.776s

Epoch 79 of 500
  training loss:		3.521830E-02
  validation loss:		2.403741E-02
Epoch took 0.777s

Epoch 80 of 500
  training loss:		3.618058E-02
  validation loss:		2.361386E-02
Epoch took 0.776s

Epoch 81 of 500
  training loss:		3.559847E-02
  validation loss:		2.192404E-02
Epoch took 0.776s

Epoch 82 of 500
  training loss:		3.668576E-02
  validation loss:		2.085361E-02
Epoch took 0.776s

Epoch 83 of 500
  training loss:		3.557003E-02
  validation loss:		1.811824E-02
Epoch took 0.776s

Epoch 84 of 500
  training loss:		3.524705E-02
  validation loss:		2.849234E-02
Epoch took 0.776s

Epoch 85 of 500
  training loss:		3.546342E-02
  validation loss:		1.782354E-02
Epoch took 0.776s

Epoch 86 of 500
  training loss:		3.422799E-02
  validation loss:		2.283632E-02
Epoch took 0.776s

Epoch 87 of 500
  training loss:		3.517755E-02
  validation loss:		2.039807E-02
Epoch took 0.776s

Epoch 88 of 500
  training loss:		3.342350E-02
  validation loss:		1.917967E-02
Epoch took 0.777s

Epoch 89 of 500
  training loss:		3.462651E-02
  validation loss:		1.430640E-02
Epoch took 0.776s

Epoch 90 of 500
  training loss:		3.376282E-02
  validation loss:		1.922848E-02
Epoch took 0.776s

Epoch 91 of 500
  training loss:		3.354750E-02
  validation loss:		2.960200E-02
Epoch took 0.777s

Epoch 92 of 500
  training loss:		3.482779E-02
  validation loss:		1.661617E-02
Epoch took 0.776s

Epoch 93 of 500
  training loss:		3.333767E-02
  validation loss:		2.119955E-02
Epoch took 0.776s

Epoch 94 of 500
  training loss:		3.360699E-02
  validation loss:		2.083006E-02
Epoch took 0.776s

Epoch 95 of 500
  training loss:		3.251359E-02
  validation loss:		1.576825E-02
Epoch took 0.776s

Epoch 96 of 500
  training loss:		3.305950E-02
  validation loss:		1.646486E-02
Epoch took 0.777s

Epoch 97 of 500
  training loss:		3.327510E-02
  validation loss:		1.636155E-02
Epoch took 0.776s

Epoch 98 of 500
  training loss:		3.277903E-02
  validation loss:		1.875232E-02
Epoch took 0.776s

Epoch 99 of 500
  training loss:		3.310655E-02
  validation loss:		1.596802E-02
Epoch took 0.776s

Epoch 100 of 500
  training loss:		3.242677E-02
  validation loss:		2.160637E-02
Epoch took 0.777s

Epoch 101 of 500
  training loss:		3.234969E-02
  validation loss:		1.510955E-02
Epoch took 0.776s

Epoch 102 of 500
  training loss:		3.111614E-02
  validation loss:		1.743895E-02
Epoch took 0.776s

Epoch 103 of 500
  training loss:		3.239931E-02
  validation loss:		1.504437E-02
Epoch took 0.777s

Epoch 104 of 500
  training loss:		3.180468E-02
  validation loss:		1.588901E-02
Epoch took 0.776s

Epoch 105 of 500
  training loss:		3.186574E-02
  validation loss:		1.784980E-02
Epoch took 0.776s

Epoch 106 of 500
  training loss:		3.199835E-02
  validation loss:		1.519835E-02
Epoch took 0.776s

Epoch 107 of 500
  training loss:		3.156783E-02
  validation loss:		1.741333E-02
Epoch took 0.776s

Epoch 108 of 500
  training loss:		3.113948E-02
  validation loss:		2.191409E-02
Epoch took 0.776s

Epoch 109 of 500
  training loss:		3.075121E-02
  validation loss:		1.720130E-02
Epoch took 0.776s

Epoch 110 of 500
  training loss:		3.167901E-02
  validation loss:		2.233467E-02
Epoch took 0.776s

Epoch 111 of 500
  training loss:		3.051676E-02
  validation loss:		1.937645E-02
Epoch took 0.776s

Epoch 112 of 500
  training loss:		3.083418E-02
  validation loss:		1.368562E-02
Epoch took 0.776s

Epoch 113 of 500
  training loss:		3.116727E-02
  validation loss:		1.719546E-02
Epoch took 0.776s

Epoch 114 of 500
  training loss:		3.135600E-02
  validation loss:		1.876074E-02
Epoch took 0.776s

Epoch 115 of 500
  training loss:		3.043393E-02
  validation loss:		1.787173E-02
Epoch took 0.777s

Epoch 116 of 500
  training loss:		3.055000E-02
  validation loss:		1.717853E-02
Epoch took 0.776s

Epoch 117 of 500
  training loss:		2.981113E-02
  validation loss:		1.801066E-02
Epoch took 0.776s

Epoch 118 of 500
  training loss:		3.061516E-02
  validation loss:		1.797117E-02
Epoch took 0.776s

Epoch 119 of 500
  training loss:		2.991921E-02
  validation loss:		1.974248E-02
Epoch took 0.776s

Epoch 120 of 500
  training loss:		2.995210E-02
  validation loss:		1.556917E-02
Epoch took 0.776s

Epoch 121 of 500
  training loss:		2.998677E-02
  validation loss:		1.401811E-02
Epoch took 0.776s

Epoch 122 of 500
  training loss:		3.049576E-02
  validation loss:		1.422756E-02
Epoch took 0.776s

Epoch 123 of 500
  training loss:		2.959752E-02
  validation loss:		1.554311E-02
Epoch took 0.776s

Epoch 124 of 500
  training loss:		3.026592E-02
  validation loss:		1.984261E-02
Epoch took 0.776s

Epoch 125 of 500
  training loss:		2.929876E-02
  validation loss:		1.827678E-02
Epoch took 0.776s

Epoch 126 of 500
  training loss:		2.939251E-02
  validation loss:		1.798548E-02
Epoch took 0.776s

Epoch 127 of 500
  training loss:		2.882655E-02
  validation loss:		1.377725E-02
Epoch took 0.776s

Epoch 128 of 500
  training loss:		2.963255E-02
  validation loss:		1.613814E-02
Epoch took 0.776s

Epoch 129 of 500
  training loss:		2.958050E-02
  validation loss:		1.553694E-02
Epoch took 0.776s

Epoch 130 of 500
  training loss:		2.891364E-02
  validation loss:		1.560633E-02
Epoch took 0.777s

Epoch 131 of 500
  training loss:		2.801546E-02
  validation loss:		1.624051E-02
Epoch took 0.776s

Epoch 132 of 500
  training loss:		2.879481E-02
  validation loss:		1.544926E-02
Epoch took 0.776s

Epoch 133 of 500
  training loss:		2.890351E-02
  validation loss:		1.837790E-02
Epoch took 0.776s

Epoch 134 of 500
  training loss:		2.769211E-02
  validation loss:		2.176327E-02
Epoch took 0.776s

Epoch 135 of 500
  training loss:		2.829489E-02
  validation loss:		1.390745E-02
Epoch took 0.776s

Epoch 136 of 500
  training loss:		2.925972E-02
  validation loss:		1.530208E-02
Epoch took 0.776s

Epoch 137 of 500
  training loss:		2.847571E-02
  validation loss:		1.629328E-02
Epoch took 0.776s

Epoch 138 of 500
  training loss:		2.846264E-02
  validation loss:		1.446991E-02
Epoch took 0.776s

Epoch 139 of 500
  training loss:		2.799833E-02
  validation loss:		1.670882E-02
Epoch took 0.776s

Epoch 140 of 500
  training loss:		2.749543E-02
  validation loss:		1.184218E-02
Epoch took 0.776s

Epoch 141 of 500
  training loss:		2.824513E-02
  validation loss:		1.674413E-02
Epoch took 0.776s

Epoch 142 of 500
  training loss:		2.753375E-02
  validation loss:		1.573440E-02
Epoch took 0.776s

Epoch 143 of 500
  training loss:		2.726470E-02
  validation loss:		1.595329E-02
Epoch took 0.776s

Epoch 144 of 500
  training loss:		2.736760E-02
  validation loss:		1.463501E-02
Epoch took 0.776s

Epoch 145 of 500
  training loss:		2.694204E-02
  validation loss:		1.897035E-02
Epoch took 0.776s

Epoch 146 of 500
  training loss:		2.751151E-02
  validation loss:		1.778002E-02
Epoch took 0.776s

Epoch 147 of 500
  training loss:		2.720987E-02
  validation loss:		1.603502E-02
Epoch took 0.776s

Epoch 148 of 500
  training loss:		2.673748E-02
  validation loss:		1.421119E-02
Epoch took 0.776s

Epoch 149 of 500
  training loss:		2.607732E-02
  validation loss:		1.838973E-02
Epoch took 0.776s

Epoch 150 of 500
  training loss:		2.746323E-02
  validation loss:		1.494979E-02
Epoch took 0.776s

Epoch 151 of 500
  training loss:		2.729112E-02
  validation loss:		1.771878E-02
Epoch took 0.777s

Epoch 152 of 500
  training loss:		2.680841E-02
  validation loss:		2.208303E-02
Epoch took 0.776s

Epoch 153 of 500
  training loss:		2.632103E-02
  validation loss:		1.649124E-02
Epoch took 0.776s

Epoch 154 of 500
  training loss:		2.664662E-02
  validation loss:		1.549720E-02
Epoch took 0.776s

Epoch 155 of 500
  training loss:		2.680467E-02
  validation loss:		1.419409E-02
Epoch took 0.776s

Epoch 156 of 500
  training loss:		2.573444E-02
  validation loss:		1.649105E-02
Epoch took 0.776s

Epoch 157 of 500
  training loss:		2.625728E-02
  validation loss:		1.220216E-02
Epoch took 0.776s

Epoch 158 of 500
  training loss:		2.635981E-02
  validation loss:		1.758380E-02
Epoch took 0.776s

Epoch 159 of 500
  training loss:		2.551114E-02
  validation loss:		1.256729E-02
Epoch took 0.777s

Epoch 160 of 500
  training loss:		2.520374E-02
  validation loss:		1.373608E-02
Epoch took 0.776s

Epoch 161 of 500
  training loss:		2.490460E-02
  validation loss:		1.394858E-02
Epoch took 0.776s

Epoch 162 of 500
  training loss:		2.510674E-02
  validation loss:		1.458071E-02
Epoch took 0.776s

Epoch 163 of 500
  training loss:		2.553325E-02
  validation loss:		1.672526E-02
Epoch took 0.776s

Epoch 164 of 500
  training loss:		2.577367E-02
  validation loss:		1.417294E-02
Epoch took 0.776s

Epoch 165 of 500
  training loss:		2.560050E-02
  validation loss:		1.540170E-02
Epoch took 0.776s

Epoch 166 of 500
  training loss:		2.582737E-02
  validation loss:		1.943800E-02
Epoch took 0.776s

Epoch 167 of 500
  training loss:		2.502744E-02
  validation loss:		1.395814E-02
Epoch took 0.776s

Epoch 168 of 500
  training loss:		2.515249E-02
  validation loss:		1.562250E-02
Epoch took 0.776s

Epoch 169 of 500
  training loss:		2.474742E-02
  validation loss:		1.276013E-02
Epoch took 0.776s

Epoch 170 of 500
  training loss:		2.490419E-02
  validation loss:		2.175109E-02
Epoch took 0.776s

Epoch 171 of 500
  training loss:		2.604522E-02
  validation loss:		1.417206E-02
Epoch took 0.777s

Epoch 172 of 500
  training loss:		2.447489E-02
  validation loss:		1.481058E-02
Epoch took 0.776s

Epoch 173 of 500
  training loss:		2.410418E-02
  validation loss:		1.623170E-02
Epoch took 0.776s

Epoch 174 of 500
  training loss:		2.458119E-02
  validation loss:		1.734585E-02
Epoch took 0.776s

Epoch 175 of 500
  training loss:		2.404755E-02
  validation loss:		1.921065E-02
Epoch took 0.777s

Epoch 176 of 500
  training loss:		2.419054E-02
  validation loss:		1.426714E-02
Epoch took 0.777s

Epoch 177 of 500
  training loss:		2.429679E-02
  validation loss:		1.786136E-02
Epoch took 0.776s

Epoch 178 of 500
  training loss:		2.523144E-02
  validation loss:		1.609720E-02
Epoch took 0.776s

Epoch 179 of 500
  training loss:		2.471933E-02
  validation loss:		2.206575E-02
Epoch took 0.776s

Epoch 180 of 500
  training loss:		2.467867E-02
  validation loss:		1.505806E-02
Epoch took 0.776s

Early stopping, val-loss increased over the last 15 epochs from 0.0155595949489 to 0.0167100136065
Saving model from epoch 165
Training RMSE: 0.0154058
Validation RMSE: 0.0154001
Test RMSE: 0.0150597365573
Test MSE: 0.000226795658818
Test MAE: 0.0111255608499
Test R2: -2414425657.31 

