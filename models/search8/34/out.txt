Training network with 56340 training samples and 18780 validation samples
Epoch 1 of 500
  training loss:		7.717003E-02
  validation loss:		8.133009E-02
Epoch took 2.050s

Epoch 2 of 500
  training loss:		2.666157E-02
  validation loss:		6.009683E-02
Epoch took 2.017s

Epoch 3 of 500
  training loss:		1.977641E-02
  validation loss:		1.715599E-02
Epoch took 2.016s

Epoch 4 of 500
  training loss:		1.910597E-02
  validation loss:		2.124252E-02
Epoch took 2.016s

Epoch 5 of 500
  training loss:		1.621634E-02
  validation loss:		2.698932E-02
Epoch took 2.017s

Epoch 6 of 500
  training loss:		1.436066E-02
  validation loss:		2.604827E-02
Epoch took 2.021s

Epoch 7 of 500
  training loss:		1.429587E-02
  validation loss:		1.288587E-02
Epoch took 2.018s

Epoch 8 of 500
  training loss:		1.339079E-02
  validation loss:		1.715312E-02
Epoch took 2.018s

Epoch 9 of 500
  training loss:		1.169420E-02
  validation loss:		1.527983E-02
Epoch took 2.018s

Epoch 10 of 500
  training loss:		1.298999E-02
  validation loss:		1.990070E-02
Epoch took 2.021s

Epoch 11 of 500
  training loss:		1.167209E-02
  validation loss:		1.962426E-02
Epoch took 2.024s

Epoch 12 of 500
  training loss:		1.061355E-02
  validation loss:		9.000130E-03
Epoch took 2.025s

Epoch 13 of 500
  training loss:		1.020512E-02
  validation loss:		1.932618E-02
Epoch took 2.027s

Epoch 14 of 500
  training loss:		1.083980E-02
  validation loss:		6.914252E-03
Epoch took 2.025s

Epoch 15 of 500
  training loss:		9.825241E-03
  validation loss:		6.123354E-03
Epoch took 2.024s

Epoch 16 of 500
  training loss:		9.121217E-03
  validation loss:		1.957168E-02
Epoch took 2.028s

Epoch 17 of 500
  training loss:		9.209414E-03
  validation loss:		9.805983E-03
Epoch took 2.026s

Epoch 18 of 500
  training loss:		9.286467E-03
  validation loss:		4.050912E-03
Epoch took 2.027s

Epoch 19 of 500
  training loss:		9.078917E-03
  validation loss:		1.480510E-02
Epoch took 2.026s

Epoch 20 of 500
  training loss:		9.375309E-03
  validation loss:		2.172136E-02
Epoch took 2.028s

Epoch 21 of 500
  training loss:		8.565021E-03
  validation loss:		1.872839E-02
Epoch took 2.027s

Epoch 22 of 500
  training loss:		9.447602E-03
  validation loss:		1.622912E-02
Epoch took 2.028s

Epoch 23 of 500
  training loss:		8.336665E-03
  validation loss:		9.708774E-03
Epoch took 2.027s

Epoch 24 of 500
  training loss:		8.523275E-03
  validation loss:		1.182878E-02
Epoch took 2.029s

Epoch 25 of 500
  training loss:		1.003255E-02
  validation loss:		5.513763E-03
Epoch took 2.029s

Epoch 26 of 500
  training loss:		7.864262E-03
  validation loss:		1.080764E-02
Epoch took 2.029s

Epoch 27 of 500
  training loss:		8.814460E-03
  validation loss:		1.432204E-02
Epoch took 2.028s

Epoch 28 of 500
  training loss:		8.276015E-03
  validation loss:		5.859155E-03
Epoch took 2.029s

Epoch 29 of 500
  training loss:		7.739090E-03
  validation loss:		8.015927E-03
Epoch took 2.029s

Epoch 30 of 500
  training loss:		8.561244E-03
  validation loss:		2.920382E-03
Epoch took 2.029s

Epoch 31 of 500
  training loss:		7.751744E-03
  validation loss:		1.160920E-02
Epoch took 2.035s

Epoch 32 of 500
  training loss:		8.806852E-03
  validation loss:		1.606429E-02
Epoch took 2.031s

Epoch 33 of 500
  training loss:		7.422601E-03
  validation loss:		1.195743E-02
Epoch took 2.034s

Epoch 34 of 500
  training loss:		8.013195E-03
  validation loss:		9.330624E-03
Epoch took 2.032s

Epoch 35 of 500
  training loss:		7.507115E-03
  validation loss:		2.699721E-03
Epoch took 2.034s

Epoch 36 of 500
  training loss:		7.521632E-03
  validation loss:		8.192137E-03
Epoch took 2.033s

Epoch 37 of 500
  training loss:		7.217094E-03
  validation loss:		3.189960E-03
Epoch took 2.033s

Epoch 38 of 500
  training loss:		7.148231E-03
  validation loss:		7.779766E-03
Epoch took 2.035s

Epoch 39 of 500
  training loss:		8.369539E-03
  validation loss:		5.837049E-03
Epoch took 2.035s

Epoch 40 of 500
  training loss:		7.787777E-03
  validation loss:		7.267885E-03
Epoch took 2.032s

Epoch 41 of 500
  training loss:		6.988270E-03
  validation loss:		2.375240E-02
Epoch took 2.037s

Epoch 42 of 500
  training loss:		8.118921E-03
  validation loss:		1.249348E-02
Epoch took 2.035s

Epoch 43 of 500
  training loss:		7.101941E-03
  validation loss:		7.045902E-03
Epoch took 2.033s

Epoch 44 of 500
  training loss:		8.278110E-03
  validation loss:		6.637790E-03
Epoch took 2.035s

Epoch 45 of 500
  training loss:		8.249088E-03
  validation loss:		1.148346E-02
Epoch took 2.039s

Epoch 46 of 500
  training loss:		8.475612E-03
  validation loss:		1.134531E-02
Epoch took 2.036s

Epoch 47 of 500
  training loss:		6.998487E-03
  validation loss:		9.525185E-03
Epoch took 2.036s

Epoch 48 of 500
  training loss:		8.200611E-03
  validation loss:		8.145492E-03
Epoch took 2.036s

Epoch 49 of 500
  training loss:		6.593688E-03
  validation loss:		7.305256E-03
Epoch took 2.035s

Epoch 50 of 500
  training loss:		6.626771E-03
  validation loss:		1.378743E-02
Epoch took 2.034s

Epoch 51 of 500
  training loss:		7.790922E-03
  validation loss:		6.277395E-03
Epoch took 2.037s

Epoch 52 of 500
  training loss:		6.590638E-03
  validation loss:		3.059179E-03
Epoch took 2.035s

Epoch 53 of 500
  training loss:		6.596397E-03
  validation loss:		3.784754E-03
Epoch took 2.034s

Epoch 54 of 500
  training loss:		6.781653E-03
  validation loss:		9.100460E-03
Epoch took 2.036s

Epoch 55 of 500
  training loss:		7.005830E-03
  validation loss:		3.426900E-03
Epoch took 2.035s

Epoch 56 of 500
  training loss:		7.195707E-03
  validation loss:		5.016280E-03
Epoch took 2.039s

Epoch 57 of 500
  training loss:		6.062748E-03
  validation loss:		7.799113E-03
Epoch took 2.039s

Epoch 58 of 500
  training loss:		6.908213E-03
  validation loss:		1.243643E-02
Epoch took 2.036s

Epoch 59 of 500
  training loss:		6.965904E-03
  validation loss:		7.398236E-03
Epoch took 2.042s

Epoch 60 of 500
  training loss:		8.349139E-03
  validation loss:		1.300093E-02
Epoch took 2.043s

Epoch 61 of 500
  training loss:		6.369646E-03
  validation loss:		1.120744E-02
Epoch took 2.042s

Epoch 62 of 500
  training loss:		6.425028E-03
  validation loss:		8.749779E-03
Epoch took 2.040s

Epoch 63 of 500
  training loss:		7.040287E-03
  validation loss:		7.804006E-03
Epoch took 2.040s

Epoch 64 of 500
  training loss:		6.950648E-03
  validation loss:		5.341299E-03
Epoch took 2.039s

Epoch 65 of 500
  training loss:		6.336143E-03
  validation loss:		2.776863E-02
Epoch took 2.042s

Epoch 66 of 500
  training loss:		8.471955E-03
  validation loss:		2.200229E-03
Epoch took 2.045s

Epoch 67 of 500
  training loss:		7.268761E-03
  validation loss:		5.600728E-03
Epoch took 2.044s

Epoch 68 of 500
  training loss:		7.025613E-03
  validation loss:		3.573188E-03
Epoch took 2.040s

Epoch 69 of 500
  training loss:		6.383193E-03
  validation loss:		7.503939E-04
Epoch took 2.041s

Epoch 70 of 500
  training loss:		7.255364E-03
  validation loss:		1.178177E-02
Epoch took 2.045s

Epoch 71 of 500
  training loss:		7.691583E-03
  validation loss:		1.168701E-02
Epoch took 2.047s

Epoch 72 of 500
  training loss:		5.742821E-03
  validation loss:		6.620774E-03
Epoch took 2.046s

Epoch 73 of 500
  training loss:		6.279724E-03
  validation loss:		3.917941E-03
Epoch took 2.045s

Epoch 74 of 500
  training loss:		5.969501E-03
  validation loss:		1.121381E-02
Epoch took 2.048s

Epoch 75 of 500
  training loss:		6.096129E-03
  validation loss:		3.961842E-03
Epoch took 2.047s

Early stopping, val-loss increased over the last 15 epochs from 0.00809388942939 to 0.00814525649118
Saving model from epoch 60
Training RMSE: 0.0134982
Validation RMSE: 0.0135578
Test RMSE: 0.0131661258638
Test MSE: 0.000173346881638
Test MAE: 0.00661917962134
Test R2: -1845419256.31 

