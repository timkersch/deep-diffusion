Training network with 56340 training samples and 18780 validation samples
Epoch 1 of 500
  training loss:		3.540288E-01
  validation loss:		3.833795E-01
Epoch took 0.818s

Epoch 2 of 500
  training loss:		1.242820E-01
  validation loss:		8.599200E-02
Epoch took 0.775s

Epoch 3 of 500
  training loss:		9.660998E-02
  validation loss:		4.765113E-02
Epoch took 0.775s

Epoch 4 of 500
  training loss:		8.694354E-02
  validation loss:		4.497682E-02
Epoch took 0.775s

Epoch 5 of 500
  training loss:		7.575489E-02
  validation loss:		4.325430E-02
Epoch took 0.775s

Epoch 6 of 500
  training loss:		7.213376E-02
  validation loss:		3.465794E-02
Epoch took 0.775s

Epoch 7 of 500
  training loss:		7.035128E-02
  validation loss:		4.874734E-02
Epoch took 0.775s

Epoch 8 of 500
  training loss:		6.988006E-02
  validation loss:		3.302650E-02
Epoch took 0.775s

Epoch 9 of 500
  training loss:		6.669457E-02
  validation loss:		7.034846E-02
Epoch took 0.775s

Epoch 10 of 500
  training loss:		6.545137E-02
  validation loss:		4.551895E-02
Epoch took 0.775s

Epoch 11 of 500
  training loss:		6.207321E-02
  validation loss:		5.773851E-02
Epoch took 0.775s

Epoch 12 of 500
  training loss:		6.198605E-02
  validation loss:		5.454206E-02
Epoch took 0.775s

Epoch 13 of 500
  training loss:		6.007407E-02
  validation loss:		4.071069E-02
Epoch took 0.775s

Epoch 14 of 500
  training loss:		5.891851E-02
  validation loss:		4.286757E-02
Epoch took 0.775s

Epoch 15 of 500
  training loss:		5.932678E-02
  validation loss:		3.454748E-02
Epoch took 0.775s

Epoch 16 of 500
  training loss:		5.902174E-02
  validation loss:		2.839790E-02
Epoch took 0.775s

Epoch 17 of 500
  training loss:		5.671104E-02
  validation loss:		3.440580E-02
Epoch took 0.775s

Epoch 18 of 500
  training loss:		5.523379E-02
  validation loss:		4.618474E-02
Epoch took 0.775s

Epoch 19 of 500
  training loss:		5.531889E-02
  validation loss:		2.599328E-02
Epoch took 0.775s

Epoch 20 of 500
  training loss:		5.346223E-02
  validation loss:		2.779528E-02
Epoch took 0.775s

Epoch 21 of 500
  training loss:		5.347326E-02
  validation loss:		4.761178E-02
Epoch took 0.775s

Epoch 22 of 500
  training loss:		5.150158E-02
  validation loss:		2.942356E-02
Epoch took 0.775s

Epoch 23 of 500
  training loss:		5.184202E-02
  validation loss:		2.274138E-02
Epoch took 0.775s

Epoch 24 of 500
  training loss:		5.002505E-02
  validation loss:		2.688592E-02
Epoch took 0.775s

Epoch 25 of 500
  training loss:		5.023374E-02
  validation loss:		4.710703E-02
Epoch took 0.775s

Epoch 26 of 500
  training loss:		4.980286E-02
  validation loss:		2.797872E-02
Epoch took 0.775s

Epoch 27 of 500
  training loss:		4.927311E-02
  validation loss:		2.609568E-02
Epoch took 0.775s

Epoch 28 of 500
  training loss:		4.865763E-02
  validation loss:		3.352411E-02
Epoch took 0.775s

Epoch 29 of 500
  training loss:		4.787836E-02
  validation loss:		2.398687E-02
Epoch took 0.775s

Epoch 30 of 500
  training loss:		4.845091E-02
  validation loss:		2.321517E-02
Epoch took 0.775s

Epoch 31 of 500
  training loss:		4.694442E-02
  validation loss:		2.802017E-02
Epoch took 0.775s

Epoch 32 of 500
  training loss:		4.643056E-02
  validation loss:		4.496571E-02
Epoch took 0.775s

Epoch 33 of 500
  training loss:		4.617799E-02
  validation loss:		4.161094E-02
Epoch took 0.775s

Epoch 34 of 500
  training loss:		4.624821E-02
  validation loss:		3.802454E-02
Epoch took 0.775s

Epoch 35 of 500
  training loss:		4.694489E-02
  validation loss:		2.409862E-02
Epoch took 0.775s

Epoch 36 of 500
  training loss:		4.637530E-02
  validation loss:		2.359735E-02
Epoch took 0.775s

Epoch 37 of 500
  training loss:		4.566307E-02
  validation loss:		2.813616E-02
Epoch took 0.775s

Epoch 38 of 500
  training loss:		4.549684E-02
  validation loss:		2.440234E-02
Epoch took 0.775s

Epoch 39 of 500
  training loss:		4.368826E-02
  validation loss:		2.388440E-02
Epoch took 0.775s

Epoch 40 of 500
  training loss:		4.437906E-02
  validation loss:		2.681855E-02
Epoch took 0.775s

Epoch 41 of 500
  training loss:		4.367589E-02
  validation loss:		2.319250E-02
Epoch took 0.775s

Epoch 42 of 500
  training loss:		4.268217E-02
  validation loss:		2.486155E-02
Epoch took 0.775s

Epoch 43 of 500
  training loss:		4.377827E-02
  validation loss:		2.150149E-02
Epoch took 0.775s

Epoch 44 of 500
  training loss:		4.327463E-02
  validation loss:		2.401695E-02
Epoch took 0.775s

Epoch 45 of 500
  training loss:		4.194188E-02
  validation loss:		2.187414E-02
Epoch took 0.775s

Epoch 46 of 500
  training loss:		4.204336E-02
  validation loss:		3.029613E-02
Epoch took 0.775s

Epoch 47 of 500
  training loss:		4.221502E-02
  validation loss:		1.806811E-02
Epoch took 0.775s

Epoch 48 of 500
  training loss:		4.241683E-02
  validation loss:		1.906157E-02
Epoch took 0.775s

Epoch 49 of 500
  training loss:		4.077504E-02
  validation loss:		3.476679E-02
Epoch took 0.775s

Epoch 50 of 500
  training loss:		4.167527E-02
  validation loss:		2.421079E-02
Epoch took 0.775s

Epoch 51 of 500
  training loss:		4.021263E-02
  validation loss:		2.023610E-02
Epoch took 0.775s

Epoch 52 of 500
  training loss:		4.045897E-02
  validation loss:		2.324662E-02
Epoch took 0.774s

Epoch 53 of 500
  training loss:		3.944736E-02
  validation loss:		2.404610E-02
Epoch took 0.775s

Epoch 54 of 500
  training loss:		3.953405E-02
  validation loss:		1.890607E-02
Epoch took 0.775s

Epoch 55 of 500
  training loss:		3.977308E-02
  validation loss:		2.438573E-02
Epoch took 0.775s

Epoch 56 of 500
  training loss:		3.905497E-02
  validation loss:		2.014568E-02
Epoch took 0.775s

Epoch 57 of 500
  training loss:		3.881924E-02
  validation loss:		1.819398E-02
Epoch took 0.775s

Epoch 58 of 500
  training loss:		3.854946E-02
  validation loss:		2.110763E-02
Epoch took 0.775s

Epoch 59 of 500
  training loss:		3.840078E-02
  validation loss:		1.939063E-02
Epoch took 0.775s

Epoch 60 of 500
  training loss:		3.932663E-02
  validation loss:		1.682025E-02
Epoch took 0.775s

Epoch 61 of 500
  training loss:		3.826023E-02
  validation loss:		2.306725E-02
Epoch took 0.775s

Epoch 62 of 500
  training loss:		3.816345E-02
  validation loss:		2.690801E-02
Epoch took 0.775s

Epoch 63 of 500
  training loss:		3.854329E-02
  validation loss:		2.337180E-02
Epoch took 0.775s

Epoch 64 of 500
  training loss:		3.746865E-02
  validation loss:		2.025139E-02
Epoch took 0.775s

Epoch 65 of 500
  training loss:		3.653755E-02
  validation loss:		2.284617E-02
Epoch took 0.775s

Epoch 66 of 500
  training loss:		3.676018E-02
  validation loss:		1.849040E-02
Epoch took 0.774s

Epoch 67 of 500
  training loss:		3.566973E-02
  validation loss:		1.779356E-02
Epoch took 0.775s

Epoch 68 of 500
  training loss:		3.655768E-02
  validation loss:		1.572944E-02
Epoch took 0.775s

Epoch 69 of 500
  training loss:		3.594289E-02
  validation loss:		2.011023E-02
Epoch took 0.775s

Epoch 70 of 500
  training loss:		3.644214E-02
  validation loss:		2.320169E-02
Epoch took 0.775s

Epoch 71 of 500
  training loss:		3.570111E-02
  validation loss:		1.945735E-02
Epoch took 0.775s

Epoch 72 of 500
  training loss:		3.545327E-02
  validation loss:		2.040720E-02
Epoch took 0.775s

Epoch 73 of 500
  training loss:		3.583201E-02
  validation loss:		2.279099E-02
Epoch took 0.775s

Epoch 74 of 500
  training loss:		3.562558E-02
  validation loss:		2.140371E-02
Epoch took 0.775s

Epoch 75 of 500
  training loss:		3.595284E-02
  validation loss:		3.144177E-02
Epoch took 0.775s

Epoch 76 of 500
  training loss:		3.575362E-02
  validation loss:		2.223880E-02
Epoch took 0.777s

Epoch 77 of 500
  training loss:		3.477615E-02
  validation loss:		2.052575E-02
Epoch took 0.777s

Epoch 78 of 500
  training loss:		3.538110E-02
  validation loss:		1.892672E-02
Epoch took 0.777s

Epoch 79 of 500
  training loss:		3.373396E-02
  validation loss:		1.855634E-02
Epoch took 0.777s

Epoch 80 of 500
  training loss:		3.371893E-02
  validation loss:		1.729951E-02
Epoch took 0.778s

Epoch 81 of 500
  training loss:		3.383599E-02
  validation loss:		3.183140E-02
Epoch took 0.778s

Epoch 82 of 500
  training loss:		3.341712E-02
  validation loss:		1.851918E-02
Epoch took 0.778s

Epoch 83 of 500
  training loss:		3.432276E-02
  validation loss:		2.030164E-02
Epoch took 0.777s

Epoch 84 of 500
  training loss:		3.344506E-02
  validation loss:		1.616420E-02
Epoch took 0.777s

Epoch 85 of 500
  training loss:		3.436590E-02
  validation loss:		3.177463E-02
Epoch took 0.777s

Epoch 86 of 500
  training loss:		3.341114E-02
  validation loss:		1.904544E-02
Epoch took 0.777s

Epoch 87 of 500
  training loss:		3.311465E-02
  validation loss:		1.610277E-02
Epoch took 0.777s

Epoch 88 of 500
  training loss:		3.355668E-02
  validation loss:		1.749473E-02
Epoch took 0.777s

Epoch 89 of 500
  training loss:		3.294227E-02
  validation loss:		2.137782E-02
Epoch took 0.777s

Epoch 90 of 500
  training loss:		3.289750E-02
  validation loss:		2.216156E-02
Epoch took 0.777s

Epoch 91 of 500
  training loss:		3.212539E-02
  validation loss:		2.378214E-02
Epoch took 0.777s

Epoch 92 of 500
  training loss:		3.276689E-02
  validation loss:		1.676746E-02
Epoch took 0.777s

Epoch 93 of 500
  training loss:		3.145648E-02
  validation loss:		1.910191E-02
Epoch took 0.777s

Epoch 94 of 500
  training loss:		3.163749E-02
  validation loss:		1.704643E-02
Epoch took 0.777s

Epoch 95 of 500
  training loss:		3.194962E-02
  validation loss:		1.874188E-02
Epoch took 0.777s

Epoch 96 of 500
  training loss:		3.183414E-02
  validation loss:		2.552249E-02
Epoch took 0.777s

Epoch 97 of 500
  training loss:		3.175663E-02
  validation loss:		2.414185E-02
Epoch took 0.777s

Epoch 98 of 500
  training loss:		3.240646E-02
  validation loss:		1.995149E-02
Epoch took 0.777s

Epoch 99 of 500
  training loss:		3.121249E-02
  validation loss:		2.311730E-02
Epoch took 0.777s

Epoch 100 of 500
  training loss:		3.201127E-02
  validation loss:		1.779961E-02
Epoch took 0.777s

Epoch 101 of 500
  training loss:		3.153053E-02
  validation loss:		1.856480E-02
Epoch took 0.777s

Epoch 102 of 500
  training loss:		3.138742E-02
  validation loss:		1.611023E-02
Epoch took 0.777s

Epoch 103 of 500
  training loss:		3.250711E-02
  validation loss:		2.146121E-02
Epoch took 0.777s

Epoch 104 of 500
  training loss:		3.131590E-02
  validation loss:		1.883339E-02
Epoch took 0.777s

Epoch 105 of 500
  training loss:		2.956700E-02
  validation loss:		1.962818E-02
Epoch took 0.777s

Epoch 106 of 500
  training loss:		3.179658E-02
  validation loss:		2.401630E-02
Epoch took 0.777s

Epoch 107 of 500
  training loss:		3.096875E-02
  validation loss:		1.414370E-02
Epoch took 0.777s

Epoch 108 of 500
  training loss:		2.983024E-02
  validation loss:		1.554279E-02
Epoch took 0.777s

Epoch 109 of 500
  training loss:		3.022318E-02
  validation loss:		1.812914E-02
Epoch took 0.777s

Epoch 110 of 500
  training loss:		3.075842E-02
  validation loss:		2.141088E-02
Epoch took 0.777s

Epoch 111 of 500
  training loss:		2.987385E-02
  validation loss:		2.657703E-02
Epoch took 0.777s

Epoch 112 of 500
  training loss:		3.001806E-02
  validation loss:		1.796351E-02
Epoch took 0.777s

Epoch 113 of 500
  training loss:		2.965270E-02
  validation loss:		2.238260E-02
Epoch took 0.777s

Epoch 114 of 500
  training loss:		2.849699E-02
  validation loss:		1.784360E-02
Epoch took 0.777s

Epoch 115 of 500
  training loss:		2.872872E-02
  validation loss:		1.347016E-02
Epoch took 0.777s

Epoch 116 of 500
  training loss:		2.823358E-02
  validation loss:		1.517843E-02
Epoch took 0.778s

Epoch 117 of 500
  training loss:		3.058663E-02
  validation loss:		2.238381E-02
Epoch took 0.777s

Epoch 118 of 500
  training loss:		2.874560E-02
  validation loss:		1.611287E-02
Epoch took 0.777s

Epoch 119 of 500
  training loss:		2.901154E-02
  validation loss:		1.808021E-02
Epoch took 0.777s

Epoch 120 of 500
  training loss:		2.923634E-02
  validation loss:		1.928519E-02
Epoch took 0.778s

Epoch 121 of 500
  training loss:		2.806068E-02
  validation loss:		1.891098E-02
Epoch took 0.777s

Epoch 122 of 500
  training loss:		2.804582E-02
  validation loss:		1.963700E-02
Epoch took 0.778s

Epoch 123 of 500
  training loss:		2.823664E-02
  validation loss:		2.058989E-02
Epoch took 0.777s

Epoch 124 of 500
  training loss:		2.932040E-02
  validation loss:		1.516418E-02
Epoch took 0.777s

Epoch 125 of 500
  training loss:		2.931486E-02
  validation loss:		1.457163E-02
Epoch took 0.778s

Epoch 126 of 500
  training loss:		2.792532E-02
  validation loss:		2.527195E-02
Epoch took 0.777s

Epoch 127 of 500
  training loss:		2.782787E-02
  validation loss:		1.867218E-02
Epoch took 0.777s

Epoch 128 of 500
  training loss:		2.832185E-02
  validation loss:		1.670420E-02
Epoch took 0.777s

Epoch 129 of 500
  training loss:		2.786538E-02
  validation loss:		2.223412E-02
Epoch took 0.777s

Epoch 130 of 500
  training loss:		2.868524E-02
  validation loss:		1.842203E-02
Epoch took 0.777s

Epoch 131 of 500
  training loss:		2.723340E-02
  validation loss:		2.083524E-02
Epoch took 0.778s

Epoch 132 of 500
  training loss:		2.778461E-02
  validation loss:		1.869187E-02
Epoch took 0.777s

Epoch 133 of 500
  training loss:		2.760884E-02
  validation loss:		2.425031E-02
Epoch took 0.777s

Epoch 134 of 500
  training loss:		2.766977E-02
  validation loss:		1.734149E-02
Epoch took 0.777s

Epoch 135 of 500
  training loss:		2.703447E-02
  validation loss:		1.449208E-02
Epoch took 0.777s

Epoch 136 of 500
  training loss:		2.724548E-02
  validation loss:		1.778571E-02
Epoch took 0.777s

Epoch 137 of 500
  training loss:		2.723382E-02
  validation loss:		1.443888E-02
Epoch took 0.777s

Epoch 138 of 500
  training loss:		2.659759E-02
  validation loss:		1.332902E-02
Epoch took 0.777s

Epoch 139 of 500
  training loss:		2.630355E-02
  validation loss:		1.425673E-02
Epoch took 0.777s

Epoch 140 of 500
  training loss:		2.693544E-02
  validation loss:		2.541659E-02
Epoch took 0.777s

Epoch 141 of 500
  training loss:		2.651764E-02
  validation loss:		1.839730E-02
Epoch took 0.777s

Epoch 142 of 500
  training loss:		2.585553E-02
  validation loss:		1.505708E-02
Epoch took 0.777s

Epoch 143 of 500
  training loss:		2.692438E-02
  validation loss:		1.575416E-02
Epoch took 0.777s

Epoch 144 of 500
  training loss:		2.674469E-02
  validation loss:		1.545120E-02
Epoch took 0.777s

Epoch 145 of 500
  training loss:		2.601292E-02
  validation loss:		1.657665E-02
Epoch took 0.777s

Epoch 146 of 500
  training loss:		2.588363E-02
  validation loss:		2.037355E-02
Epoch took 0.777s

Epoch 147 of 500
  training loss:		2.653111E-02
  validation loss:		1.678712E-02
Epoch took 0.777s

Epoch 148 of 500
  training loss:		2.616953E-02
  validation loss:		1.421444E-02
Epoch took 0.777s

Epoch 149 of 500
  training loss:		2.574981E-02
  validation loss:		1.440234E-02
Epoch took 0.777s

Epoch 150 of 500
  training loss:		2.597538E-02
  validation loss:		1.514335E-02
Epoch took 0.777s

Epoch 151 of 500
  training loss:		2.599279E-02
  validation loss:		1.515981E-02
Epoch took 0.777s

Epoch 152 of 500
  training loss:		2.563577E-02
  validation loss:		2.133719E-02
Epoch took 0.777s

Epoch 153 of 500
  training loss:		2.628909E-02
  validation loss:		1.587387E-02
Epoch took 0.777s

Epoch 154 of 500
  training loss:		2.661709E-02
  validation loss:		1.795756E-02
Epoch took 0.777s

Epoch 155 of 500
  training loss:		2.517910E-02
  validation loss:		1.854157E-02
Epoch took 0.780s

Epoch 156 of 500
  training loss:		2.612201E-02
  validation loss:		1.455299E-02
Epoch took 0.777s

Epoch 157 of 500
  training loss:		2.481934E-02
  validation loss:		1.492761E-02
Epoch took 0.777s

Epoch 158 of 500
  training loss:		2.524889E-02
  validation loss:		1.655550E-02
Epoch took 0.778s

Epoch 159 of 500
  training loss:		2.564056E-02
  validation loss:		1.689202E-02
Epoch took 0.777s

Epoch 160 of 500
  training loss:		2.521290E-02
  validation loss:		1.914255E-02
Epoch took 0.777s

Epoch 161 of 500
  training loss:		2.505931E-02
  validation loss:		1.437926E-02
Epoch took 0.778s

Epoch 162 of 500
  training loss:		2.454122E-02
  validation loss:		1.571398E-02
Epoch took 0.778s

Epoch 163 of 500
  training loss:		2.548269E-02
  validation loss:		1.467348E-02
Epoch took 0.777s

Epoch 164 of 500
  training loss:		2.554255E-02
  validation loss:		1.409166E-02
Epoch took 0.777s

Epoch 165 of 500
  training loss:		2.444186E-02
  validation loss:		1.491785E-02
Epoch took 0.778s

Epoch 166 of 500
  training loss:		2.534165E-02
  validation loss:		1.549132E-02
Epoch took 0.777s

Epoch 167 of 500
  training loss:		2.493969E-02
  validation loss:		1.265657E-02
Epoch took 0.777s

Epoch 168 of 500
  training loss:		2.458542E-02
  validation loss:		1.409604E-02
Epoch took 0.778s

Epoch 169 of 500
  training loss:		2.392925E-02
  validation loss:		1.259139E-02
Epoch took 0.778s

Epoch 170 of 500
  training loss:		2.480998E-02
  validation loss:		1.598972E-02
Epoch took 0.778s

Epoch 171 of 500
  training loss:		2.358050E-02
  validation loss:		1.468464E-02
Epoch took 0.777s

Epoch 172 of 500
  training loss:		2.440112E-02
  validation loss:		2.170631E-02
Epoch took 0.777s

Epoch 173 of 500
  training loss:		2.460659E-02
  validation loss:		1.318229E-02
Epoch took 0.777s

Epoch 174 of 500
  training loss:		2.466440E-02
  validation loss:		1.703438E-02
Epoch took 0.777s

Epoch 175 of 500
  training loss:		2.481076E-02
  validation loss:		1.629657E-02
Epoch took 0.777s

Epoch 176 of 500
  training loss:		2.420946E-02
  validation loss:		1.552724E-02
Epoch took 0.777s

Epoch 177 of 500
  training loss:		2.415184E-02
  validation loss:		1.415627E-02
Epoch took 0.777s

Epoch 178 of 500
  training loss:		2.361492E-02
  validation loss:		1.257983E-02
Epoch took 0.777s

Epoch 179 of 500
  training loss:		2.332694E-02
  validation loss:		1.477237E-02
Epoch took 0.777s

Epoch 180 of 500
  training loss:		2.362753E-02
  validation loss:		1.281755E-02
Epoch took 0.777s

Epoch 181 of 500
  training loss:		2.438674E-02
  validation loss:		1.986043E-02
Epoch took 0.777s

Epoch 182 of 500
  training loss:		2.370050E-02
  validation loss:		1.346532E-02
Epoch took 0.777s

Epoch 183 of 500
  training loss:		2.394827E-02
  validation loss:		1.139817E-02
Epoch took 0.777s

Epoch 184 of 500
  training loss:		2.325385E-02
  validation loss:		1.400198E-02
Epoch took 0.778s

Epoch 185 of 500
  training loss:		2.312757E-02
  validation loss:		1.652586E-02
Epoch took 0.777s

Epoch 186 of 500
  training loss:		2.395438E-02
  validation loss:		1.515072E-02
Epoch took 0.777s

Epoch 187 of 500
  training loss:		2.397105E-02
  validation loss:		1.608530E-02
Epoch took 0.777s

Epoch 188 of 500
  training loss:		2.382665E-02
  validation loss:		1.551997E-02
Epoch took 0.778s

Epoch 189 of 500
  training loss:		2.318756E-02
  validation loss:		1.408389E-02
Epoch took 0.777s

Epoch 190 of 500
  training loss:		2.303592E-02
  validation loss:		1.224282E-02
Epoch took 0.777s

Epoch 191 of 500
  training loss:		2.335075E-02
  validation loss:		1.489455E-02
Epoch took 0.777s

Epoch 192 of 500
  training loss:		2.283016E-02
  validation loss:		1.857887E-02
Epoch took 0.777s

Epoch 193 of 500
  training loss:		2.274157E-02
  validation loss:		1.425530E-02
Epoch took 0.778s

Epoch 194 of 500
  training loss:		2.291400E-02
  validation loss:		1.243892E-02
Epoch took 0.777s

Epoch 195 of 500
  training loss:		2.254382E-02
  validation loss:		1.720314E-02
Epoch took 0.777s

Epoch 196 of 500
  training loss:		2.209222E-02
  validation loss:		1.237036E-02
Epoch took 0.777s

Epoch 197 of 500
  training loss:		2.343977E-02
  validation loss:		1.443910E-02
Epoch took 0.777s

Epoch 198 of 500
  training loss:		2.332450E-02
  validation loss:		1.558091E-02
Epoch took 0.777s

Epoch 199 of 500
  training loss:		2.241412E-02
  validation loss:		1.745805E-02
Epoch took 0.777s

Epoch 200 of 500
  training loss:		2.290548E-02
  validation loss:		1.256297E-02
Epoch took 0.777s

Early stopping, val-loss increased over the last 20 epochs from 0.0148679361122 to 0.0149058306499
Saving model from epoch 180
Training RMSE: 0.0128458
Validation RMSE: 0.0128414
Test RMSE: 0.0127064604312
Test MSE: 0.000161454125191
Test MAE: 0.00972649641335
Test R2: -1718811349.93 

