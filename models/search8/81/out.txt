Training network with 56340 training samples and 18780 validation samples
Epoch 1 of 500
  training loss:		2.136861E-01
  validation loss:		9.098718E+01
Epoch took 0.744s

Epoch 2 of 500
  training loss:		9.019481E-02
  validation loss:		4.200097E-01
Epoch took 0.700s

Epoch 3 of 500
  training loss:		8.168112E-02
  validation loss:		9.441184E-02
Epoch took 0.700s

Epoch 4 of 500
  training loss:		6.881652E-02
  validation loss:		6.877765E-02
Epoch took 0.700s

Epoch 5 of 500
  training loss:		6.997302E-02
  validation loss:		1.022413E-01
Epoch took 0.700s

Epoch 6 of 500
  training loss:		6.536794E-02
  validation loss:		6.395518E-02
Epoch took 0.700s

Epoch 7 of 500
  training loss:		6.145576E-02
  validation loss:		6.826029E-02
Epoch took 0.700s

Epoch 8 of 500
  training loss:		5.522483E-02
  validation loss:		5.391394E-02
Epoch took 0.700s

Epoch 9 of 500
  training loss:		5.352788E-02
  validation loss:		6.341962E-02
Epoch took 0.700s

Epoch 10 of 500
  training loss:		5.199748E-02
  validation loss:		6.410348E-02
Epoch took 0.700s

Epoch 11 of 500
  training loss:		5.075362E-02
  validation loss:		5.920420E-02
Epoch took 0.700s

Epoch 12 of 500
  training loss:		4.715084E-02
  validation loss:		5.089593E-02
Epoch took 0.699s

Epoch 13 of 500
  training loss:		4.673892E-02
  validation loss:		4.410002E-02
Epoch took 0.700s

Epoch 14 of 500
  training loss:		4.625652E-02
  validation loss:		5.119294E-02
Epoch took 0.699s

Epoch 15 of 500
  training loss:		4.740304E-02
  validation loss:		4.145954E-02
Epoch took 0.700s

Epoch 16 of 500
  training loss:		4.137423E-02
  validation loss:		4.134528E-02
Epoch took 0.700s

Epoch 17 of 500
  training loss:		4.357845E-02
  validation loss:		5.250711E-02
Epoch took 0.700s

Epoch 18 of 500
  training loss:		4.108729E-02
  validation loss:		4.461899E-02
Epoch took 0.700s

Epoch 19 of 500
  training loss:		3.893372E-02
  validation loss:		4.534349E-02
Epoch took 0.700s

Epoch 20 of 500
  training loss:		3.959993E-02
  validation loss:		3.326125E-02
Epoch took 0.700s

Epoch 21 of 500
  training loss:		3.514007E-02
  validation loss:		3.337462E-02
Epoch took 0.700s

Epoch 22 of 500
  training loss:		3.875914E-02
  validation loss:		4.321619E-02
Epoch took 0.700s

Epoch 23 of 500
  training loss:		3.609256E-02
  validation loss:		4.354959E-02
Epoch took 0.700s

Epoch 24 of 500
  training loss:		3.618840E-02
  validation loss:		3.803196E-02
Epoch took 0.700s

Epoch 25 of 500
  training loss:		3.659719E-02
  validation loss:		5.486377E-02
Epoch took 0.700s

Epoch 26 of 500
  training loss:		3.429190E-02
  validation loss:		4.266153E-02
Epoch took 0.701s

Epoch 27 of 500
  training loss:		3.376171E-02
  validation loss:		3.232889E-02
Epoch took 0.700s

Epoch 28 of 500
  training loss:		3.218213E-02
  validation loss:		3.490441E-02
Epoch took 0.700s

Epoch 29 of 500
  training loss:		3.105116E-02
  validation loss:		4.004666E-02
Epoch took 0.700s

Epoch 30 of 500
  training loss:		3.432850E-02
  validation loss:		2.482690E-02
Epoch took 0.700s

Epoch 31 of 500
  training loss:		3.235599E-02
  validation loss:		2.913094E-02
Epoch took 0.699s

Epoch 32 of 500
  training loss:		2.996766E-02
  validation loss:		3.511222E-02
Epoch took 0.701s

Epoch 33 of 500
  training loss:		3.066616E-02
  validation loss:		2.927510E-02
Epoch took 0.700s

Epoch 34 of 500
  training loss:		2.983977E-02
  validation loss:		3.370829E-02
Epoch took 0.700s

Epoch 35 of 500
  training loss:		2.860768E-02
  validation loss:		3.886586E-02
Epoch took 0.700s

Epoch 36 of 500
  training loss:		3.156045E-02
  validation loss:		3.020338E-02
Epoch took 0.700s

Epoch 37 of 500
  training loss:		2.981692E-02
  validation loss:		3.030794E-02
Epoch took 0.700s

Epoch 38 of 500
  training loss:		2.738557E-02
  validation loss:		3.838702E-02
Epoch took 0.700s

Epoch 39 of 500
  training loss:		2.671269E-02
  validation loss:		3.093596E-02
Epoch took 0.699s

Epoch 40 of 500
  training loss:		2.728717E-02
  validation loss:		3.750618E-02
Epoch took 0.702s

Epoch 41 of 500
  training loss:		2.688075E-02
  validation loss:		3.262803E-02
Epoch took 0.702s

Epoch 42 of 500
  training loss:		2.823940E-02
  validation loss:		3.059773E-02
Epoch took 0.703s

Epoch 43 of 500
  training loss:		2.522483E-02
  validation loss:		2.135214E-02
Epoch took 0.702s

Epoch 44 of 500
  training loss:		2.473798E-02
  validation loss:		2.358500E-02
Epoch took 0.703s

Epoch 45 of 500
  training loss:		2.631959E-02
  validation loss:		2.850664E-02
Epoch took 0.703s

Epoch 46 of 500
  training loss:		2.586834E-02
  validation loss:		2.833762E-02
Epoch took 0.702s

Epoch 47 of 500
  training loss:		2.518667E-02
  validation loss:		2.915459E-02
Epoch took 0.702s

Epoch 48 of 500
  training loss:		2.670913E-02
  validation loss:		2.518714E-02
Epoch took 0.702s

Epoch 49 of 500
  training loss:		2.393987E-02
  validation loss:		2.314294E-02
Epoch took 0.702s

Epoch 50 of 500
  training loss:		2.459838E-02
  validation loss:		2.143332E-02
Epoch took 0.703s

Epoch 51 of 500
  training loss:		2.510003E-02
  validation loss:		2.119952E-02
Epoch took 0.702s

Epoch 52 of 500
  training loss:		2.516408E-02
  validation loss:		2.953505E-02
Epoch took 0.703s

Epoch 53 of 500
  training loss:		2.449073E-02
  validation loss:		1.876156E-02
Epoch took 0.702s

Epoch 54 of 500
  training loss:		2.276033E-02
  validation loss:		2.266910E-02
Epoch took 0.702s

Epoch 55 of 500
  training loss:		2.223208E-02
  validation loss:		2.034002E-02
Epoch took 0.703s

Epoch 56 of 500
  training loss:		2.124231E-02
  validation loss:		2.105130E-02
Epoch took 0.702s

Epoch 57 of 500
  training loss:		2.170150E-02
  validation loss:		3.070797E-02
Epoch took 0.702s

Epoch 58 of 500
  training loss:		2.176459E-02
  validation loss:		2.302380E-02
Epoch took 0.702s

Epoch 59 of 500
  training loss:		2.177171E-02
  validation loss:		2.098301E-02
Epoch took 0.702s

Epoch 60 of 500
  training loss:		2.002195E-02
  validation loss:		2.205842E-02
Epoch took 0.702s

Epoch 61 of 500
  training loss:		2.227125E-02
  validation loss:		2.435432E-02
Epoch took 0.702s

Epoch 62 of 500
  training loss:		2.082085E-02
  validation loss:		2.118951E-02
Epoch took 0.702s

Epoch 63 of 500
  training loss:		2.049262E-02
  validation loss:		2.296146E-02
Epoch took 0.702s

Epoch 64 of 500
  training loss:		2.375173E-02
  validation loss:		2.853316E-02
Epoch took 0.702s

Epoch 65 of 500
  training loss:		2.348614E-02
  validation loss:		2.186935E-02
Epoch took 0.702s

Epoch 66 of 500
  training loss:		2.172477E-02
  validation loss:		2.809448E-02
Epoch took 0.702s

Epoch 67 of 500
  training loss:		2.121609E-02
  validation loss:		2.195402E-02
Epoch took 0.703s

Epoch 68 of 500
  training loss:		1.983572E-02
  validation loss:		2.211293E-02
Epoch took 0.702s

Epoch 69 of 500
  training loss:		2.011835E-02
  validation loss:		1.996665E-02
Epoch took 0.702s

Epoch 70 of 500
  training loss:		1.859568E-02
  validation loss:		1.579468E-02
Epoch took 0.702s

Epoch 71 of 500
  training loss:		2.198756E-02
  validation loss:		2.095087E-02
Epoch took 0.702s

Epoch 72 of 500
  training loss:		1.799480E-02
  validation loss:		1.890087E-02
Epoch took 0.702s

Epoch 73 of 500
  training loss:		1.985757E-02
  validation loss:		2.029854E-02
Epoch took 0.702s

Epoch 74 of 500
  training loss:		1.810291E-02
  validation loss:		1.841195E-02
Epoch took 0.702s

Epoch 75 of 500
  training loss:		1.911848E-02
  validation loss:		2.019263E-02
Epoch took 0.702s

Epoch 76 of 500
  training loss:		1.798510E-02
  validation loss:		2.018048E-02
Epoch took 0.702s

Epoch 77 of 500
  training loss:		1.804073E-02
  validation loss:		1.951990E-02
Epoch took 0.702s

Epoch 78 of 500
  training loss:		1.814700E-02
  validation loss:		1.609168E-02
Epoch took 0.702s

Epoch 79 of 500
  training loss:		1.744131E-02
  validation loss:		1.827858E-02
Epoch took 0.702s

Epoch 80 of 500
  training loss:		1.883988E-02
  validation loss:		2.501595E-02
Epoch took 0.702s

Epoch 81 of 500
  training loss:		1.819022E-02
  validation loss:		1.946033E-02
Epoch took 0.702s

Epoch 82 of 500
  training loss:		1.894414E-02
  validation loss:		1.612502E-02
Epoch took 0.702s

Epoch 83 of 500
  training loss:		1.768201E-02
  validation loss:		1.697426E-02
Epoch took 0.702s

Epoch 84 of 500
  training loss:		1.984945E-02
  validation loss:		1.842104E-02
Epoch took 0.702s

Epoch 85 of 500
  training loss:		1.641867E-02
  validation loss:		1.522254E-02
Epoch took 0.702s

Epoch 86 of 500
  training loss:		1.678924E-02
  validation loss:		1.785598E-02
Epoch took 0.702s

Epoch 87 of 500
  training loss:		1.697418E-02
  validation loss:		2.258297E-02
Epoch took 0.702s

Epoch 88 of 500
  training loss:		1.615367E-02
  validation loss:		1.492354E-02
Epoch took 0.702s

Epoch 89 of 500
  training loss:		1.821436E-02
  validation loss:		2.207779E-02
Epoch took 0.703s

Epoch 90 of 500
  training loss:		1.586355E-02
  validation loss:		1.465125E-02
Epoch took 0.702s

Epoch 91 of 500
  training loss:		1.580024E-02
  validation loss:		1.359436E-02
Epoch took 0.702s

Epoch 92 of 500
  training loss:		1.662909E-02
  validation loss:		1.463638E-02
Epoch took 0.702s

Epoch 93 of 500
  training loss:		1.566782E-02
  validation loss:		1.895825E-02
Epoch took 0.703s

Epoch 94 of 500
  training loss:		1.961799E-02
  validation loss:		2.612063E-02
Epoch took 0.702s

Epoch 95 of 500
  training loss:		1.921956E-02
  validation loss:		1.928189E-02
Epoch took 0.702s

Epoch 96 of 500
  training loss:		1.579077E-02
  validation loss:		1.603800E-02
Epoch took 0.703s

Epoch 97 of 500
  training loss:		1.531819E-02
  validation loss:		2.114388E-02
Epoch took 0.702s

Epoch 98 of 500
  training loss:		1.579879E-02
  validation loss:		1.449642E-02
Epoch took 0.702s

Epoch 99 of 500
  training loss:		1.595800E-02
  validation loss:		1.794080E-02
Epoch took 0.702s

Epoch 100 of 500
  training loss:		1.588085E-02
  validation loss:		2.256533E-02
Epoch took 0.702s

Epoch 101 of 500
  training loss:		1.536756E-02
  validation loss:		1.509444E-02
Epoch took 0.702s

Epoch 102 of 500
  training loss:		1.552409E-02
  validation loss:		2.111159E-02
Epoch took 0.702s

Epoch 103 of 500
  training loss:		1.601983E-02
  validation loss:		1.749771E-02
Epoch took 0.703s

Epoch 104 of 500
  training loss:		1.456118E-02
  validation loss:		1.543221E-02
Epoch took 0.702s

Epoch 105 of 500
  training loss:		1.647149E-02
  validation loss:		1.222080E-02
Epoch took 0.702s

Epoch 106 of 500
  training loss:		1.600880E-02
  validation loss:		1.664955E-02
Epoch took 0.701s

Epoch 107 of 500
  training loss:		1.491972E-02
  validation loss:		1.526672E-02
Epoch took 0.703s

Epoch 108 of 500
  training loss:		1.449810E-02
  validation loss:		1.731097E-02
Epoch took 0.702s

Epoch 109 of 500
  training loss:		1.651308E-02
  validation loss:		1.593695E-02
Epoch took 0.702s

Epoch 110 of 500
  training loss:		1.519061E-02
  validation loss:		1.787189E-02
Epoch took 0.703s

Epoch 111 of 500
  training loss:		1.525691E-02
  validation loss:		1.579253E-02
Epoch took 0.703s

Epoch 112 of 500
  training loss:		1.385884E-02
  validation loss:		1.325039E-02
Epoch took 0.702s

Epoch 113 of 500
  training loss:		1.481651E-02
  validation loss:		1.340432E-02
Epoch took 0.702s

Epoch 114 of 500
  training loss:		1.464664E-02
  validation loss:		1.953668E-02
Epoch took 0.702s

Epoch 115 of 500
  training loss:		1.411509E-02
  validation loss:		1.254928E-02
Epoch took 0.701s

Epoch 116 of 500
  training loss:		1.652144E-02
  validation loss:		2.170362E-02
Epoch took 0.702s

Epoch 117 of 500
  training loss:		1.334252E-02
  validation loss:		1.454415E-02
Epoch took 0.702s

Epoch 118 of 500
  training loss:		1.328346E-02
  validation loss:		1.334072E-02
Epoch took 0.702s

Epoch 119 of 500
  training loss:		1.413992E-02
  validation loss:		1.689230E-02
Epoch took 0.702s

Epoch 120 of 500
  training loss:		1.703755E-02
  validation loss:		1.463096E-02
Epoch took 0.702s

Epoch 121 of 500
  training loss:		1.309775E-02
  validation loss:		1.839495E-02
Epoch took 0.702s

Epoch 122 of 500
  training loss:		1.247346E-02
  validation loss:		1.866420E-02
Epoch took 0.702s

Epoch 123 of 500
  training loss:		1.328202E-02
  validation loss:		1.337953E-02
Epoch took 0.702s

Epoch 124 of 500
  training loss:		1.384026E-02
  validation loss:		1.730805E-02
Epoch took 0.702s

Epoch 125 of 500
  training loss:		1.417629E-02
  validation loss:		1.440230E-02
Epoch took 0.702s

Epoch 126 of 500
  training loss:		1.650968E-02
  validation loss:		1.208514E-02
Epoch took 0.703s

Epoch 127 of 500
  training loss:		1.167605E-02
  validation loss:		1.733985E-02
Epoch took 0.702s

Epoch 128 of 500
  training loss:		1.824545E-02
  validation loss:		1.974529E-02
Epoch took 0.702s

Epoch 129 of 500
  training loss:		1.399025E-02
  validation loss:		1.487911E-02
Epoch took 0.702s

Epoch 130 of 500
  training loss:		1.302240E-02
  validation loss:		1.123649E-02
Epoch took 0.703s

Epoch 131 of 500
  training loss:		1.211340E-02
  validation loss:		1.464369E-02
Epoch took 0.703s

Epoch 132 of 500
  training loss:		1.232773E-02
  validation loss:		8.997675E-03
Epoch took 0.703s

Epoch 133 of 500
  training loss:		1.421430E-02
  validation loss:		1.959872E-02
Epoch took 0.702s

Epoch 134 of 500
  training loss:		1.253799E-02
  validation loss:		1.401194E-02
Epoch took 0.702s

Epoch 135 of 500
  training loss:		1.326374E-02
  validation loss:		1.148515E-02
Epoch took 0.702s

Epoch 136 of 500
  training loss:		1.315512E-02
  validation loss:		1.092660E-02
Epoch took 0.702s

Epoch 137 of 500
  training loss:		1.149717E-02
  validation loss:		1.250009E-02
Epoch took 0.703s

Epoch 138 of 500
  training loss:		1.291869E-02
  validation loss:		1.777443E-02
Epoch took 0.702s

Epoch 139 of 500
  training loss:		1.395786E-02
  validation loss:		1.385841E-02
Epoch took 0.703s

Epoch 140 of 500
  training loss:		1.380323E-02
  validation loss:		1.274198E-02
Epoch took 0.702s

Epoch 141 of 500
  training loss:		1.163350E-02
  validation loss:		9.411233E-03
Epoch took 0.703s

Epoch 142 of 500
  training loss:		1.280943E-02
  validation loss:		1.062507E-02
Epoch took 0.702s

Epoch 143 of 500
  training loss:		1.198537E-02
  validation loss:		1.224218E-02
Epoch took 0.702s

Epoch 144 of 500
  training loss:		1.343570E-02
  validation loss:		1.332247E-02
Epoch took 0.702s

Epoch 145 of 500
  training loss:		1.308808E-02
  validation loss:		1.369864E-02
Epoch took 0.702s

Epoch 146 of 500
  training loss:		1.221676E-02
  validation loss:		1.076350E-02
Epoch took 0.702s

Epoch 147 of 500
  training loss:		1.206863E-02
  validation loss:		1.308189E-02
Epoch took 0.702s

Epoch 148 of 500
  training loss:		1.216215E-02
  validation loss:		1.006008E-02
Epoch took 0.702s

Epoch 149 of 500
  training loss:		1.254780E-02
  validation loss:		1.426855E-02
Epoch took 0.702s

Epoch 150 of 500
  training loss:		1.139833E-02
  validation loss:		1.083148E-02
Epoch took 0.703s

Epoch 151 of 500
  training loss:		1.176039E-02
  validation loss:		1.058243E-02
Epoch took 0.703s

Epoch 152 of 500
  training loss:		1.057028E-02
  validation loss:		1.157309E-02
Epoch took 0.702s

Epoch 153 of 500
  training loss:		1.100171E-02
  validation loss:		1.237675E-02
Epoch took 0.702s

Epoch 154 of 500
  training loss:		1.094149E-02
  validation loss:		1.051046E-02
Epoch took 0.702s

Epoch 155 of 500
  training loss:		1.139809E-02
  validation loss:		1.216782E-02
Epoch took 0.702s

Epoch 156 of 500
  training loss:		1.175307E-02
  validation loss:		1.203232E-02
Epoch took 0.702s

Epoch 157 of 500
  training loss:		1.316350E-02
  validation loss:		8.895515E-03
Epoch took 0.702s

Epoch 158 of 500
  training loss:		1.124987E-02
  validation loss:		8.853644E-03
Epoch took 0.702s

Epoch 159 of 500
  training loss:		1.164143E-02
  validation loss:		1.110185E-02
Epoch took 0.703s

Epoch 160 of 500
  training loss:		1.062896E-02
  validation loss:		9.640980E-03
Epoch took 0.702s

Epoch 161 of 500
  training loss:		1.245706E-02
  validation loss:		1.414748E-02
Epoch took 0.703s

Epoch 162 of 500
  training loss:		1.158439E-02
  validation loss:		1.070421E-02
Epoch took 0.702s

Epoch 163 of 500
  training loss:		1.130136E-02
  validation loss:		1.004681E-02
Epoch took 0.702s

Epoch 164 of 500
  training loss:		1.161541E-02
  validation loss:		8.357198E-03
Epoch took 0.703s

Epoch 165 of 500
  training loss:		1.055042E-02
  validation loss:		8.672632E-03
Epoch took 0.702s

Epoch 166 of 500
  training loss:		1.062704E-02
  validation loss:		1.347804E-02
Epoch took 0.703s

Epoch 167 of 500
  training loss:		9.775190E-03
  validation loss:		8.640938E-03
Epoch took 0.702s

Epoch 168 of 500
  training loss:		1.041878E-02
  validation loss:		2.347766E-02
Epoch took 0.703s

Epoch 169 of 500
  training loss:		1.051447E-02
  validation loss:		8.923891E-03
Epoch took 0.702s

Epoch 170 of 500
  training loss:		1.032274E-02
  validation loss:		1.882660E-02
Epoch took 0.702s

Epoch 171 of 500
  training loss:		1.072956E-02
  validation loss:		7.185354E-03
Epoch took 0.702s

Epoch 172 of 500
  training loss:		1.193780E-02
  validation loss:		1.029002E-02
Epoch took 0.702s

Epoch 173 of 500
  training loss:		1.092359E-02
  validation loss:		1.099146E-02
Epoch took 0.703s

Epoch 174 of 500
  training loss:		1.024292E-02
  validation loss:		1.372129E-02
Epoch took 0.702s

Epoch 175 of 500
  training loss:		1.258773E-02
  validation loss:		1.318413E-02
Epoch took 0.702s

Epoch 176 of 500
  training loss:		1.057060E-02
  validation loss:		8.022126E-03
Epoch took 0.703s

Epoch 177 of 500
  training loss:		9.929769E-03
  validation loss:		8.815026E-03
Epoch took 0.702s

Epoch 178 of 500
  training loss:		1.068039E-02
  validation loss:		1.734177E-02
Epoch took 0.703s

Epoch 179 of 500
  training loss:		1.136803E-02
  validation loss:		1.048215E-02
Epoch took 0.702s

Epoch 180 of 500
  training loss:		1.010939E-02
  validation loss:		7.313520E-03
Epoch took 0.702s

Early stopping, val-loss increased over the last 20 epochs from 0.0113019977412 to 0.0116311150487
Saving model from epoch 160
Training RMSE: 0.00955268
Validation RMSE: 0.00964494
Test RMSE: 0.00960914418101
Test MSE: 9.23356492422e-05
Test MAE: 0.00787155050784
Test R2: -982988623.922 

