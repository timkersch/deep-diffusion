Training network with 56340 training samples and 18780 validation samples
Epoch 1 of 500
  training loss:		1.698327E-01
  validation loss:		1.155593E+02
Epoch took 0.750s

Epoch 2 of 500
  training loss:		7.073033E-02
  validation loss:		4.195287E-01
Epoch took 0.700s

Epoch 3 of 500
  training loss:		6.066218E-02
  validation loss:		1.195650E-01
Epoch took 0.701s

Epoch 4 of 500
  training loss:		5.828978E-02
  validation loss:		7.924719E-02
Epoch took 0.700s

Epoch 5 of 500
  training loss:		5.576349E-02
  validation loss:		6.746737E-02
Epoch took 0.700s

Epoch 6 of 500
  training loss:		5.048316E-02
  validation loss:		8.056355E-02
Epoch took 0.700s

Epoch 7 of 500
  training loss:		5.051180E-02
  validation loss:		7.070839E-02
Epoch took 0.700s

Epoch 8 of 500
  training loss:		4.599011E-02
  validation loss:		6.513638E-02
Epoch took 0.700s

Epoch 9 of 500
  training loss:		4.267293E-02
  validation loss:		5.292236E-02
Epoch took 0.699s

Epoch 10 of 500
  training loss:		4.214528E-02
  validation loss:		5.302666E-02
Epoch took 0.701s

Epoch 11 of 500
  training loss:		4.085489E-02
  validation loss:		5.039992E-02
Epoch took 0.700s

Epoch 12 of 500
  training loss:		3.804703E-02
  validation loss:		4.698710E-02
Epoch took 0.700s

Epoch 13 of 500
  training loss:		3.974832E-02
  validation loss:		4.328193E-02
Epoch took 0.700s

Epoch 14 of 500
  training loss:		3.601198E-02
  validation loss:		3.801005E-02
Epoch took 0.700s

Epoch 15 of 500
  training loss:		3.430354E-02
  validation loss:		4.460034E-02
Epoch took 0.701s

Epoch 16 of 500
  training loss:		3.451123E-02
  validation loss:		4.241160E-02
Epoch took 0.700s

Epoch 17 of 500
  training loss:		3.337238E-02
  validation loss:		4.532735E-02
Epoch took 0.701s

Epoch 18 of 500
  training loss:		3.206764E-02
  validation loss:		4.084698E-02
Epoch took 0.700s

Epoch 19 of 500
  training loss:		3.134334E-02
  validation loss:		4.628117E-02
Epoch took 0.700s

Epoch 20 of 500
  training loss:		3.192239E-02
  validation loss:		5.261580E-02
Epoch took 0.701s

Epoch 21 of 500
  training loss:		2.977788E-02
  validation loss:		2.936062E-02
Epoch took 0.700s

Epoch 22 of 500
  training loss:		2.931011E-02
  validation loss:		3.307813E-02
Epoch took 0.700s

Epoch 23 of 500
  training loss:		3.016123E-02
  validation loss:		2.994336E-02
Epoch took 0.700s

Epoch 24 of 500
  training loss:		3.230910E-02
  validation loss:		4.964345E-02
Epoch took 0.700s

Epoch 25 of 500
  training loss:		2.620819E-02
  validation loss:		2.531367E-02
Epoch took 0.700s

Epoch 26 of 500
  training loss:		2.690509E-02
  validation loss:		2.543722E-02
Epoch took 0.700s

Epoch 27 of 500
  training loss:		2.789549E-02
  validation loss:		3.337352E-02
Epoch took 0.700s

Epoch 28 of 500
  training loss:		2.656952E-02
  validation loss:		2.639737E-02
Epoch took 0.700s

Epoch 29 of 500
  training loss:		2.638740E-02
  validation loss:		3.246330E-02
Epoch took 0.700s

Epoch 30 of 500
  training loss:		2.556822E-02
  validation loss:		2.818716E-02
Epoch took 0.700s

Epoch 31 of 500
  training loss:		2.569747E-02
  validation loss:		2.692978E-02
Epoch took 0.701s

Epoch 32 of 500
  training loss:		2.398721E-02
  validation loss:		2.750808E-02
Epoch took 0.700s

Epoch 33 of 500
  training loss:		2.405312E-02
  validation loss:		2.412667E-02
Epoch took 0.700s

Epoch 34 of 500
  training loss:		2.371989E-02
  validation loss:		2.730521E-02
Epoch took 0.701s

Epoch 35 of 500
  training loss:		2.464502E-02
  validation loss:		1.894200E-02
Epoch took 0.700s

Epoch 36 of 500
  training loss:		2.382163E-02
  validation loss:		2.264413E-02
Epoch took 0.700s

Epoch 37 of 500
  training loss:		2.481654E-02
  validation loss:		2.147321E-02
Epoch took 0.700s

Epoch 38 of 500
  training loss:		2.581446E-02
  validation loss:		2.990210E-02
Epoch took 0.700s

Epoch 39 of 500
  training loss:		2.652797E-02
  validation loss:		1.953370E-02
Epoch took 0.701s

Epoch 40 of 500
  training loss:		2.367485E-02
  validation loss:		2.555026E-02
Epoch took 0.700s

Epoch 41 of 500
  training loss:		2.217076E-02
  validation loss:		2.538223E-02
Epoch took 0.700s

Epoch 42 of 500
  training loss:		2.217934E-02
  validation loss:		2.261465E-02
Epoch took 0.700s

Epoch 43 of 500
  training loss:		2.012325E-02
  validation loss:		2.330945E-02
Epoch took 0.700s

Epoch 44 of 500
  training loss:		2.097973E-02
  validation loss:		2.841440E-02
Epoch took 0.700s

Epoch 45 of 500
  training loss:		1.924422E-02
  validation loss:		1.850641E-02
Epoch took 0.700s

Epoch 46 of 500
  training loss:		2.023297E-02
  validation loss:		2.052538E-02
Epoch took 0.701s

Epoch 47 of 500
  training loss:		2.077356E-02
  validation loss:		2.526563E-02
Epoch took 0.700s

Epoch 48 of 500
  training loss:		2.010074E-02
  validation loss:		2.432453E-02
Epoch took 0.701s

Epoch 49 of 500
  training loss:		2.236983E-02
  validation loss:		2.046532E-02
Epoch took 0.700s

Epoch 50 of 500
  training loss:		1.844052E-02
  validation loss:		2.144319E-02
Epoch took 0.700s

Epoch 51 of 500
  training loss:		1.852569E-02
  validation loss:		1.989261E-02
Epoch took 0.700s

Epoch 52 of 500
  training loss:		1.888006E-02
  validation loss:		1.884760E-02
Epoch took 0.700s

Epoch 53 of 500
  training loss:		1.974508E-02
  validation loss:		1.906255E-02
Epoch took 0.700s

Epoch 54 of 500
  training loss:		1.968235E-02
  validation loss:		2.664740E-02
Epoch took 0.700s

Epoch 55 of 500
  training loss:		1.854195E-02
  validation loss:		2.117486E-02
Epoch took 0.700s

Epoch 56 of 500
  training loss:		1.633095E-02
  validation loss:		1.921800E-02
Epoch took 0.700s

Epoch 57 of 500
  training loss:		1.735157E-02
  validation loss:		2.161308E-02
Epoch took 0.700s

Epoch 58 of 500
  training loss:		1.807142E-02
  validation loss:		1.415256E-02
Epoch took 0.700s

Epoch 59 of 500
  training loss:		1.521859E-02
  validation loss:		1.995712E-02
Epoch took 0.700s

Epoch 60 of 500
  training loss:		1.658976E-02
  validation loss:		1.572341E-02
Epoch took 0.700s

Epoch 61 of 500
  training loss:		1.606783E-02
  validation loss:		1.841958E-02
Epoch took 0.701s

Epoch 62 of 500
  training loss:		1.582383E-02
  validation loss:		1.520039E-02
Epoch took 0.700s

Epoch 63 of 500
  training loss:		1.504047E-02
  validation loss:		1.601532E-02
Epoch took 0.700s

Epoch 64 of 500
  training loss:		1.780078E-02
  validation loss:		1.758708E-02
Epoch took 0.700s

Epoch 65 of 500
  training loss:		1.720249E-02
  validation loss:		1.559703E-02
Epoch took 0.700s

Epoch 66 of 500
  training loss:		1.685523E-02
  validation loss:		2.531078E-02
Epoch took 0.700s

Epoch 67 of 500
  training loss:		2.000036E-02
  validation loss:		2.796051E-02
Epoch took 0.701s

Epoch 68 of 500
  training loss:		1.582208E-02
  validation loss:		1.458297E-02
Epoch took 0.701s

Epoch 69 of 500
  training loss:		1.525880E-02
  validation loss:		2.179318E-02
Epoch took 0.700s

Epoch 70 of 500
  training loss:		1.552364E-02
  validation loss:		1.312541E-02
Epoch took 0.700s

Epoch 71 of 500
  training loss:		1.500643E-02
  validation loss:		1.448014E-02
Epoch took 0.700s

Epoch 72 of 500
  training loss:		1.433069E-02
  validation loss:		1.603176E-02
Epoch took 0.700s

Epoch 73 of 500
  training loss:		1.553125E-02
  validation loss:		1.649808E-02
Epoch took 0.700s

Epoch 74 of 500
  training loss:		1.492089E-02
  validation loss:		1.286201E-02
Epoch took 0.701s

Epoch 75 of 500
  training loss:		1.426153E-02
  validation loss:		1.731142E-02
Epoch took 0.700s

Epoch 76 of 500
  training loss:		1.442310E-02
  validation loss:		1.430041E-02
Epoch took 0.700s

Epoch 77 of 500
  training loss:		1.370690E-02
  validation loss:		1.668595E-02
Epoch took 0.701s

Epoch 78 of 500
  training loss:		1.272752E-02
  validation loss:		1.419652E-02
Epoch took 0.700s

Epoch 79 of 500
  training loss:		1.581667E-02
  validation loss:		1.476945E-02
Epoch took 0.700s

Epoch 80 of 500
  training loss:		1.507554E-02
  validation loss:		1.408331E-02
Epoch took 0.700s

Epoch 81 of 500
  training loss:		1.328193E-02
  validation loss:		1.641954E-02
Epoch took 0.700s

Epoch 82 of 500
  training loss:		1.265000E-02
  validation loss:		1.415407E-02
Epoch took 0.700s

Epoch 83 of 500
  training loss:		1.313822E-02
  validation loss:		1.202208E-02
Epoch took 0.700s

Epoch 84 of 500
  training loss:		1.285109E-02
  validation loss:		1.280341E-02
Epoch took 0.700s

Epoch 85 of 500
  training loss:		1.284091E-02
  validation loss:		1.023483E-02
Epoch took 0.703s

Epoch 86 of 500
  training loss:		1.451421E-02
  validation loss:		9.692818E-03
Epoch took 0.703s

Epoch 87 of 500
  training loss:		1.243351E-02
  validation loss:		1.006526E-02
Epoch took 0.703s

Epoch 88 of 500
  training loss:		1.316754E-02
  validation loss:		1.612789E-02
Epoch took 0.703s

Epoch 89 of 500
  training loss:		1.232728E-02
  validation loss:		1.038767E-02
Epoch took 0.702s

Epoch 90 of 500
  training loss:		1.256812E-02
  validation loss:		1.450085E-02
Epoch took 0.703s

Epoch 91 of 500
  training loss:		1.178213E-02
  validation loss:		1.879032E-02
Epoch took 0.703s

Epoch 92 of 500
  training loss:		1.241261E-02
  validation loss:		1.130493E-02
Epoch took 0.703s

Epoch 93 of 500
  training loss:		1.336134E-02
  validation loss:		1.402395E-02
Epoch took 0.703s

Epoch 94 of 500
  training loss:		1.214289E-02
  validation loss:		1.440717E-02
Epoch took 0.702s

Epoch 95 of 500
  training loss:		1.216541E-02
  validation loss:		1.075885E-02
Epoch took 0.703s

Epoch 96 of 500
  training loss:		1.180206E-02
  validation loss:		9.442069E-03
Epoch took 0.703s

Epoch 97 of 500
  training loss:		1.199936E-02
  validation loss:		1.239530E-02
Epoch took 0.703s

Epoch 98 of 500
  training loss:		1.202730E-02
  validation loss:		1.077035E-02
Epoch took 0.703s

Epoch 99 of 500
  training loss:		1.320738E-02
  validation loss:		1.595773E-02
Epoch took 0.703s

Epoch 100 of 500
  training loss:		1.237676E-02
  validation loss:		1.107682E-02
Epoch took 0.703s

Epoch 101 of 500
  training loss:		1.145321E-02
  validation loss:		1.213906E-02
Epoch took 0.703s

Epoch 102 of 500
  training loss:		1.246112E-02
  validation loss:		1.677837E-02
Epoch took 0.703s

Epoch 103 of 500
  training loss:		1.450019E-02
  validation loss:		9.795633E-03
Epoch took 0.703s

Epoch 104 of 500
  training loss:		1.320949E-02
  validation loss:		1.311874E-02
Epoch took 0.703s

Epoch 105 of 500
  training loss:		1.228808E-02
  validation loss:		1.071854E-02
Epoch took 0.703s

Epoch 106 of 500
  training loss:		1.117515E-02
  validation loss:		1.145302E-02
Epoch took 0.702s

Epoch 107 of 500
  training loss:		1.054729E-02
  validation loss:		1.096532E-02
Epoch took 0.702s

Epoch 108 of 500
  training loss:		1.056746E-02
  validation loss:		1.221554E-02
Epoch took 0.703s

Epoch 109 of 500
  training loss:		1.129834E-02
  validation loss:		6.503660E-03
Epoch took 0.703s

Epoch 110 of 500
  training loss:		1.144722E-02
  validation loss:		1.165847E-02
Epoch took 0.702s

Epoch 111 of 500
  training loss:		1.074184E-02
  validation loss:		9.451527E-03
Epoch took 0.703s

Epoch 112 of 500
  training loss:		1.318172E-02
  validation loss:		2.035054E-02
Epoch took 0.703s

Epoch 113 of 500
  training loss:		1.127390E-02
  validation loss:		1.110518E-02
Epoch took 0.702s

Epoch 114 of 500
  training loss:		1.195738E-02
  validation loss:		1.499633E-02
Epoch took 0.703s

Epoch 115 of 500
  training loss:		1.152865E-02
  validation loss:		1.197520E-02
Epoch took 0.703s

Epoch 116 of 500
  training loss:		1.163860E-02
  validation loss:		1.119596E-02
Epoch took 0.702s

Epoch 117 of 500
  training loss:		1.058387E-02
  validation loss:		1.554706E-02
Epoch took 0.703s

Epoch 118 of 500
  training loss:		1.087260E-02
  validation loss:		1.018179E-02
Epoch took 0.703s

Epoch 119 of 500
  training loss:		9.548909E-03
  validation loss:		1.074807E-02
Epoch took 0.703s

Epoch 120 of 500
  training loss:		1.181266E-02
  validation loss:		1.006490E-02
Epoch took 0.703s

Epoch 121 of 500
  training loss:		1.206246E-02
  validation loss:		1.867778E-02
Epoch took 0.703s

Epoch 122 of 500
  training loss:		1.140846E-02
  validation loss:		1.087687E-02
Epoch took 0.702s

Epoch 123 of 500
  training loss:		1.190629E-02
  validation loss:		1.124888E-02
Epoch took 0.703s

Epoch 124 of 500
  training loss:		9.705528E-03
  validation loss:		1.094179E-02
Epoch took 0.703s

Epoch 125 of 500
  training loss:		1.087629E-02
  validation loss:		1.313950E-02
Epoch took 0.703s

Epoch 126 of 500
  training loss:		9.851009E-03
  validation loss:		1.070608E-02
Epoch took 0.703s

Epoch 127 of 500
  training loss:		1.025317E-02
  validation loss:		1.442608E-02
Epoch took 0.702s

Epoch 128 of 500
  training loss:		1.109345E-02
  validation loss:		1.715467E-02
Epoch took 0.703s

Epoch 129 of 500
  training loss:		9.745958E-03
  validation loss:		8.219222E-03
Epoch took 0.702s

Epoch 130 of 500
  training loss:		1.006111E-02
  validation loss:		1.188667E-02
Epoch took 0.703s

Epoch 131 of 500
  training loss:		1.038641E-02
  validation loss:		1.147016E-02
Epoch took 0.703s

Epoch 132 of 500
  training loss:		8.979333E-03
  validation loss:		7.552405E-03
Epoch took 0.703s

Epoch 133 of 500
  training loss:		1.018439E-02
  validation loss:		8.519355E-03
Epoch took 0.703s

Epoch 134 of 500
  training loss:		9.180901E-03
  validation loss:		8.037457E-03
Epoch took 0.703s

Epoch 135 of 500
  training loss:		9.645714E-03
  validation loss:		1.081262E-02
Epoch took 0.705s

Epoch 136 of 500
  training loss:		1.033853E-02
  validation loss:		8.401755E-03
Epoch took 0.703s

Epoch 137 of 500
  training loss:		9.510163E-03
  validation loss:		7.682262E-03
Epoch took 0.704s

Epoch 138 of 500
  training loss:		8.906026E-03
  validation loss:		7.879395E-03
Epoch took 0.703s

Epoch 139 of 500
  training loss:		9.379787E-03
  validation loss:		1.222859E-02
Epoch took 0.703s

Epoch 140 of 500
  training loss:		1.004453E-02
  validation loss:		7.506362E-03
Epoch took 0.703s

Epoch 141 of 500
  training loss:		1.054587E-02
  validation loss:		1.175122E-02
Epoch took 0.704s

Epoch 142 of 500
  training loss:		1.061417E-02
  validation loss:		8.132594E-03
Epoch took 0.703s

Epoch 143 of 500
  training loss:		9.340662E-03
  validation loss:		7.975781E-03
Epoch took 0.703s

Epoch 144 of 500
  training loss:		9.318355E-03
  validation loss:		7.010191E-03
Epoch took 0.703s

Epoch 145 of 500
  training loss:		9.547690E-03
  validation loss:		7.433416E-03
Epoch took 0.703s

Epoch 146 of 500
  training loss:		8.701014E-03
  validation loss:		6.549891E-03
Epoch took 0.703s

Epoch 147 of 500
  training loss:		9.331444E-03
  validation loss:		7.040730E-03
Epoch took 0.703s

Epoch 148 of 500
  training loss:		8.915321E-03
  validation loss:		8.904917E-03
Epoch took 0.703s

Epoch 149 of 500
  training loss:		1.106144E-02
  validation loss:		1.248554E-02
Epoch took 0.703s

Epoch 150 of 500
  training loss:		9.866440E-03
  validation loss:		8.260146E-03
Epoch took 0.703s

Epoch 151 of 500
  training loss:		8.534670E-03
  validation loss:		1.049521E-02
Epoch took 0.703s

Epoch 152 of 500
  training loss:		8.917480E-03
  validation loss:		1.482467E-02
Epoch took 0.703s

Epoch 153 of 500
  training loss:		9.151823E-03
  validation loss:		7.738941E-03
Epoch took 0.703s

Epoch 154 of 500
  training loss:		8.586565E-03
  validation loss:		8.182685E-03
Epoch took 0.703s

Epoch 155 of 500
  training loss:		8.906453E-03
  validation loss:		1.335975E-02
Epoch took 0.703s

Epoch 156 of 500
  training loss:		9.004904E-03
  validation loss:		8.425238E-03
Epoch took 0.703s

Epoch 157 of 500
  training loss:		1.142488E-02
  validation loss:		6.951334E-03
Epoch took 0.703s

Epoch 158 of 500
  training loss:		9.008786E-03
  validation loss:		7.120397E-03
Epoch took 0.702s

Epoch 159 of 500
  training loss:		7.952362E-03
  validation loss:		1.131576E-02
Epoch took 0.702s

Epoch 160 of 500
  training loss:		9.537147E-03
  validation loss:		1.526029E-02
Epoch took 0.703s

Epoch 161 of 500
  training loss:		8.480387E-03
  validation loss:		7.534342E-03
Epoch took 0.702s

Epoch 162 of 500
  training loss:		8.147635E-03
  validation loss:		9.188812E-03
Epoch took 0.703s

Epoch 163 of 500
  training loss:		8.207018E-03
  validation loss:		5.472875E-03
Epoch took 0.702s

Epoch 164 of 500
  training loss:		8.953730E-03
  validation loss:		8.674219E-03
Epoch took 0.703s

Epoch 165 of 500
  training loss:		8.147887E-03
  validation loss:		1.261220E-02
Epoch took 0.703s

Early stopping, val-loss increased over the last 15 epochs from 0.00861618643868 to 0.00981044869318
Saving model from epoch 150
Training RMSE: 0.00824285
Validation RMSE: 0.0082588
Test RMSE: 0.00822596624494
Test MSE: 6.76665222272e-05
Test MAE: 0.00615674024448
Test R2: -720365447.847 

