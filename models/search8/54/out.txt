Training network with 56340 training samples and 18780 validation samples
Epoch 1 of 500
  training loss:		2.161910E-01
  validation loss:		1.012408E+02
Epoch took 0.731s

Epoch 2 of 500
  training loss:		8.260866E-02
  validation loss:		3.124114E-01
Epoch took 0.701s

Epoch 3 of 500
  training loss:		7.074184E-02
  validation loss:		9.606555E-02
Epoch took 0.702s

Epoch 4 of 500
  training loss:		6.139529E-02
  validation loss:		7.852615E-02
Epoch took 0.701s

Epoch 5 of 500
  training loss:		6.021255E-02
  validation loss:		8.691306E-02
Epoch took 0.702s

Epoch 6 of 500
  training loss:		5.385852E-02
  validation loss:		5.328499E-02
Epoch took 0.701s

Epoch 7 of 500
  training loss:		4.821930E-02
  validation loss:		5.546094E-02
Epoch took 0.701s

Epoch 8 of 500
  training loss:		4.834913E-02
  validation loss:		4.758263E-02
Epoch took 0.702s

Epoch 9 of 500
  training loss:		4.034885E-02
  validation loss:		3.781602E-02
Epoch took 0.701s

Epoch 10 of 500
  training loss:		3.950234E-02
  validation loss:		3.554772E-02
Epoch took 0.701s

Epoch 11 of 500
  training loss:		3.763681E-02
  validation loss:		3.778074E-02
Epoch took 0.701s

Epoch 12 of 500
  training loss:		3.778377E-02
  validation loss:		3.086418E-02
Epoch took 0.701s

Epoch 13 of 500
  training loss:		3.908065E-02
  validation loss:		3.027014E-02
Epoch took 0.701s

Epoch 14 of 500
  training loss:		3.622136E-02
  validation loss:		3.391333E-02
Epoch took 0.701s

Epoch 15 of 500
  training loss:		3.391139E-02
  validation loss:		3.783809E-02
Epoch took 0.701s

Epoch 16 of 500
  training loss:		3.554371E-02
  validation loss:		4.801495E-02
Epoch took 0.702s

Epoch 17 of 500
  training loss:		3.173170E-02
  validation loss:		2.900051E-02
Epoch took 0.701s

Epoch 18 of 500
  training loss:		2.887433E-02
  validation loss:		2.807934E-02
Epoch took 0.701s

Epoch 19 of 500
  training loss:		3.150260E-02
  validation loss:		5.201335E-02
Epoch took 0.702s

Epoch 20 of 500
  training loss:		3.668107E-02
  validation loss:		3.119444E-02
Epoch took 0.701s

Epoch 21 of 500
  training loss:		2.967954E-02
  validation loss:		5.017679E-02
Epoch took 0.702s

Epoch 22 of 500
  training loss:		2.733860E-02
  validation loss:		2.565781E-02
Epoch took 0.702s

Epoch 23 of 500
  training loss:		2.666581E-02
  validation loss:		2.519498E-02
Epoch took 0.701s

Epoch 24 of 500
  training loss:		2.431970E-02
  validation loss:		2.055837E-02
Epoch took 0.701s

Epoch 25 of 500
  training loss:		2.466849E-02
  validation loss:		2.435338E-02
Epoch took 0.702s

Epoch 26 of 500
  training loss:		2.420432E-02
  validation loss:		1.910757E-02
Epoch took 0.701s

Epoch 27 of 500
  training loss:		2.657463E-02
  validation loss:		2.333318E-02
Epoch took 0.701s

Epoch 28 of 500
  training loss:		2.122024E-02
  validation loss:		2.297891E-02
Epoch took 0.702s

Epoch 29 of 500
  training loss:		2.496964E-02
  validation loss:		2.526219E-02
Epoch took 0.702s

Epoch 30 of 500
  training loss:		2.500195E-02
  validation loss:		1.596790E-02
Epoch took 0.702s

Epoch 31 of 500
  training loss:		2.825868E-02
  validation loss:		1.942685E-02
Epoch took 0.702s

Epoch 32 of 500
  training loss:		2.091624E-02
  validation loss:		1.912551E-02
Epoch took 0.702s

Epoch 33 of 500
  training loss:		2.137437E-02
  validation loss:		1.797379E-02
Epoch took 0.701s

Epoch 34 of 500
  training loss:		2.253396E-02
  validation loss:		1.819708E-02
Epoch took 0.702s

Epoch 35 of 500
  training loss:		2.058114E-02
  validation loss:		3.054633E-02
Epoch took 0.701s

Epoch 36 of 500
  training loss:		1.814749E-02
  validation loss:		1.584381E-02
Epoch took 0.701s

Epoch 37 of 500
  training loss:		2.044331E-02
  validation loss:		2.315766E-02
Epoch took 0.701s

Epoch 38 of 500
  training loss:		1.838708E-02
  validation loss:		2.150579E-02
Epoch took 0.702s

Epoch 39 of 500
  training loss:		1.886447E-02
  validation loss:		1.483811E-02
Epoch took 0.702s

Epoch 40 of 500
  training loss:		1.783288E-02
  validation loss:		1.521958E-02
Epoch took 0.702s

Epoch 41 of 500
  training loss:		1.765008E-02
  validation loss:		1.061838E-02
Epoch took 0.701s

Epoch 42 of 500
  training loss:		1.675425E-02
  validation loss:		1.462506E-02
Epoch took 0.702s

Epoch 43 of 500
  training loss:		1.841180E-02
  validation loss:		1.491983E-02
Epoch took 0.701s

Epoch 44 of 500
  training loss:		1.850863E-02
  validation loss:		3.070306E-02
Epoch took 0.701s

Epoch 45 of 500
  training loss:		1.924217E-02
  validation loss:		1.412495E-02
Epoch took 0.701s

Epoch 46 of 500
  training loss:		1.747800E-02
  validation loss:		1.100906E-02
Epoch took 0.701s

Epoch 47 of 500
  training loss:		1.720945E-02
  validation loss:		1.259780E-02
Epoch took 0.701s

Epoch 48 of 500
  training loss:		1.842098E-02
  validation loss:		1.257070E-02
Epoch took 0.701s

Epoch 49 of 500
  training loss:		1.703105E-02
  validation loss:		1.512148E-02
Epoch took 0.701s

Epoch 50 of 500
  training loss:		1.621026E-02
  validation loss:		1.606828E-02
Epoch took 0.701s

Epoch 51 of 500
  training loss:		1.600702E-02
  validation loss:		1.544492E-02
Epoch took 0.702s

Epoch 52 of 500
  training loss:		1.611037E-02
  validation loss:		1.543994E-02
Epoch took 0.701s

Epoch 53 of 500
  training loss:		1.621967E-02
  validation loss:		1.590287E-02
Epoch took 0.702s

Epoch 54 of 500
  training loss:		1.578876E-02
  validation loss:		1.697061E-02
Epoch took 0.702s

Epoch 55 of 500
  training loss:		1.593175E-02
  validation loss:		2.606230E-02
Epoch took 0.701s

Epoch 56 of 500
  training loss:		1.805898E-02
  validation loss:		1.436606E-02
Epoch took 0.701s

Epoch 57 of 500
  training loss:		1.514609E-02
  validation loss:		9.842624E-03
Epoch took 0.701s

Epoch 58 of 500
  training loss:		1.330356E-02
  validation loss:		1.152282E-02
Epoch took 0.702s

Epoch 59 of 500
  training loss:		1.491081E-02
  validation loss:		1.631858E-02
Epoch took 0.702s

Epoch 60 of 500
  training loss:		1.451503E-02
  validation loss:		9.673765E-03
Epoch took 0.702s

Epoch 61 of 500
  training loss:		1.481513E-02
  validation loss:		1.870581E-02
Epoch took 0.701s

Epoch 62 of 500
  training loss:		1.502366E-02
  validation loss:		1.527187E-02
Epoch took 0.701s

Epoch 63 of 500
  training loss:		1.553632E-02
  validation loss:		2.568959E-02
Epoch took 0.701s

Epoch 64 of 500
  training loss:		1.577551E-02
  validation loss:		1.700650E-02
Epoch took 0.701s

Epoch 65 of 500
  training loss:		1.406887E-02
  validation loss:		1.276464E-02
Epoch took 0.701s

Epoch 66 of 500
  training loss:		1.409420E-02
  validation loss:		9.114771E-03
Epoch took 0.701s

Epoch 67 of 500
  training loss:		1.356083E-02
  validation loss:		1.419913E-02
Epoch took 0.701s

Epoch 68 of 500
  training loss:		1.466029E-02
  validation loss:		2.419659E-02
Epoch took 0.701s

Epoch 69 of 500
  training loss:		1.384854E-02
  validation loss:		1.487512E-02
Epoch took 0.702s

Epoch 70 of 500
  training loss:		1.333432E-02
  validation loss:		1.290875E-02
Epoch took 0.701s

Epoch 71 of 500
  training loss:		1.661546E-02
  validation loss:		1.178063E-02
Epoch took 0.702s

Epoch 72 of 500
  training loss:		1.293948E-02
  validation loss:		1.048595E-02
Epoch took 0.704s

Epoch 73 of 500
  training loss:		1.280841E-02
  validation loss:		1.222619E-02
Epoch took 0.705s

Epoch 74 of 500
  training loss:		1.370972E-02
  validation loss:		1.188923E-02
Epoch took 0.704s

Epoch 75 of 500
  training loss:		1.252070E-02
  validation loss:		1.164795E-02
Epoch took 0.704s

Early stopping, val-loss increased over the last 15 epochs from 0.0145941206771 to 0.0148508481716
Saving model from epoch 60
Training RMSE: 0.00961005
Validation RMSE: 0.00967517
Test RMSE: 0.00973487831652
Test MSE: 9.47678636294e-05
Test MAE: 0.00762323988602
Test R2: -1008881476.62 

