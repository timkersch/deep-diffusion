Training network with 56340 training samples and 18780 validation samples
Epoch 1 of 500
  training loss:		4.983691E-01
  validation loss:		5.032105E-01
Epoch took 0.824s

Epoch 2 of 500
  training loss:		1.718835E-01
  validation loss:		8.484352E-02
Epoch took 0.775s

Epoch 3 of 500
  training loss:		1.044336E-01
  validation loss:		6.659415E-02
Epoch took 0.775s

Epoch 4 of 500
  training loss:		9.249383E-02
  validation loss:		4.579705E-02
Epoch took 0.775s

Epoch 5 of 500
  training loss:		8.542849E-02
  validation loss:		3.515090E-02
Epoch took 0.775s

Epoch 6 of 500
  training loss:		7.820745E-02
  validation loss:		4.558049E-02
Epoch took 0.775s

Epoch 7 of 500
  training loss:		7.846195E-02
  validation loss:		5.713523E-02
Epoch took 0.775s

Epoch 8 of 500
  training loss:		7.519166E-02
  validation loss:		5.268134E-02
Epoch took 0.775s

Epoch 9 of 500
  training loss:		7.230449E-02
  validation loss:		3.657698E-02
Epoch took 0.775s

Epoch 10 of 500
  training loss:		7.089812E-02
  validation loss:		4.122409E-02
Epoch took 0.775s

Epoch 11 of 500
  training loss:		6.797744E-02
  validation loss:		3.938990E-02
Epoch took 0.775s

Epoch 12 of 500
  training loss:		6.737186E-02
  validation loss:		4.970826E-02
Epoch took 0.775s

Epoch 13 of 500
  training loss:		6.460098E-02
  validation loss:		3.422203E-02
Epoch took 0.775s

Epoch 14 of 500
  training loss:		6.516280E-02
  validation loss:		3.482144E-02
Epoch took 0.778s

Epoch 15 of 500
  training loss:		6.533974E-02
  validation loss:		5.789126E-02
Epoch took 0.775s

Epoch 16 of 500
  training loss:		6.362367E-02
  validation loss:		3.495687E-02
Epoch took 0.776s

Epoch 17 of 500
  training loss:		6.018423E-02
  validation loss:		3.095133E-02
Epoch took 0.774s

Epoch 18 of 500
  training loss:		6.115142E-02
  validation loss:		3.526148E-02
Epoch took 0.775s

Epoch 19 of 500
  training loss:		5.859555E-02
  validation loss:		3.422306E-02
Epoch took 0.775s

Epoch 20 of 500
  training loss:		5.853166E-02
  validation loss:		3.464278E-02
Epoch took 0.775s

Epoch 21 of 500
  training loss:		5.759688E-02
  validation loss:		3.151064E-02
Epoch took 0.775s

Epoch 22 of 500
  training loss:		5.800776E-02
  validation loss:		3.118102E-02
Epoch took 0.775s

Epoch 23 of 500
  training loss:		5.690588E-02
  validation loss:		3.217276E-02
Epoch took 0.775s

Epoch 24 of 500
  training loss:		5.507651E-02
  validation loss:		3.340774E-02
Epoch took 0.775s

Epoch 25 of 500
  training loss:		5.634583E-02
  validation loss:		2.549473E-02
Epoch took 0.776s

Epoch 26 of 500
  training loss:		5.380062E-02
  validation loss:		2.623381E-02
Epoch took 0.775s

Epoch 27 of 500
  training loss:		5.369999E-02
  validation loss:		2.150896E-02
Epoch took 0.775s

Epoch 28 of 500
  training loss:		5.388373E-02
  validation loss:		2.668499E-02
Epoch took 0.775s

Epoch 29 of 500
  training loss:		5.483772E-02
  validation loss:		2.723548E-02
Epoch took 0.775s

Epoch 30 of 500
  training loss:		5.323085E-02
  validation loss:		2.386929E-02
Epoch took 0.775s

Epoch 31 of 500
  training loss:		5.200731E-02
  validation loss:		2.778128E-02
Epoch took 0.775s

Epoch 32 of 500
  training loss:		5.187469E-02
  validation loss:		3.245286E-02
Epoch took 0.775s

Epoch 33 of 500
  training loss:		5.049942E-02
  validation loss:		3.211948E-02
Epoch took 0.775s

Epoch 34 of 500
  training loss:		4.990945E-02
  validation loss:		2.625260E-02
Epoch took 0.775s

Epoch 35 of 500
  training loss:		4.894169E-02
  validation loss:		2.988538E-02
Epoch took 0.775s

Epoch 36 of 500
  training loss:		5.114000E-02
  validation loss:		2.914667E-02
Epoch took 0.775s

Epoch 37 of 500
  training loss:		5.117348E-02
  validation loss:		2.547413E-02
Epoch took 0.775s

Epoch 38 of 500
  training loss:		4.903935E-02
  validation loss:		2.975034E-02
Epoch took 0.775s

Epoch 39 of 500
  training loss:		4.849747E-02
  validation loss:		2.692744E-02
Epoch took 0.775s

Epoch 40 of 500
  training loss:		4.925509E-02
  validation loss:		2.698028E-02
Epoch took 0.775s

Early stopping, val-loss increased over the last 10 epochs from 0.0279299423512 to 0.0286770462421
Saving model from epoch 30
Training RMSE: 0.0236726
Validation RMSE: 0.0239077
Test RMSE: 0.0236143395305
Test MSE: 0.000557637016755
Test MAE: 0.0182965658605
Test R2: -5936503307.57 

