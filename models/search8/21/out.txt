Training network with 56340 training samples and 18780 validation samples
Epoch 1 of 500
  training loss:		1.813530E-01
  validation loss:		1.262765E-01
Epoch took 0.780s

Epoch 2 of 500
  training loss:		5.876523E-02
  validation loss:		2.954664E-02
Epoch took 0.774s

Epoch 3 of 500
  training loss:		5.026035E-02
  validation loss:		6.366201E-02
Epoch took 0.774s

Epoch 4 of 500
  training loss:		3.881516E-02
  validation loss:		3.892639E-02
Epoch took 0.774s

Epoch 5 of 500
  training loss:		3.344566E-02
  validation loss:		5.564724E-02
Epoch took 0.774s

Epoch 6 of 500
  training loss:		3.343891E-02
  validation loss:		3.614025E-02
Epoch took 0.774s

Epoch 7 of 500
  training loss:		2.773111E-02
  validation loss:		3.999187E-02
Epoch took 0.774s

Epoch 8 of 500
  training loss:		2.538455E-02
  validation loss:		3.413235E-02
Epoch took 0.774s

Epoch 9 of 500
  training loss:		3.159204E-02
  validation loss:		2.092861E-02
Epoch took 0.774s

Epoch 10 of 500
  training loss:		2.728755E-02
  validation loss:		2.214079E-02
Epoch took 0.774s

Epoch 11 of 500
  training loss:		2.222453E-02
  validation loss:		2.261503E-02
Epoch took 0.774s

Epoch 12 of 500
  training loss:		2.030183E-02
  validation loss:		3.664027E-02
Epoch took 0.774s

Epoch 13 of 500
  training loss:		2.687714E-02
  validation loss:		2.089755E-02
Epoch took 0.774s

Epoch 14 of 500
  training loss:		2.043977E-02
  validation loss:		3.284831E-02
Epoch took 0.774s

Epoch 15 of 500
  training loss:		2.155426E-02
  validation loss:		1.585511E-02
Epoch took 0.774s

Epoch 16 of 500
  training loss:		2.091800E-02
  validation loss:		4.081775E-02
Epoch took 0.774s

Epoch 17 of 500
  training loss:		2.634992E-02
  validation loss:		1.983902E-02
Epoch took 0.774s

Epoch 18 of 500
  training loss:		1.907537E-02
  validation loss:		1.566849E-02
Epoch took 0.774s

Epoch 19 of 500
  training loss:		1.691183E-02
  validation loss:		1.344704E-02
Epoch took 0.774s

Epoch 20 of 500
  training loss:		1.746887E-02
  validation loss:		2.144424E-02
Epoch took 0.774s

Epoch 21 of 500
  training loss:		1.588631E-02
  validation loss:		2.910536E-02
Epoch took 0.775s

Epoch 22 of 500
  training loss:		1.615822E-02
  validation loss:		2.903803E-02
Epoch took 0.774s

Epoch 23 of 500
  training loss:		3.019762E-02
  validation loss:		2.784224E-02
Epoch took 0.774s

Epoch 24 of 500
  training loss:		1.627413E-02
  validation loss:		2.358623E-02
Epoch took 0.774s

Epoch 25 of 500
  training loss:		2.141498E-02
  validation loss:		2.814075E-02
Epoch took 0.775s

Epoch 26 of 500
  training loss:		1.889327E-02
  validation loss:		2.500543E-02
Epoch took 0.775s

Epoch 27 of 500
  training loss:		1.668424E-02
  validation loss:		1.438684E-02
Epoch took 0.774s

Epoch 28 of 500
  training loss:		1.524461E-02
  validation loss:		1.690464E-02
Epoch took 0.774s

Epoch 29 of 500
  training loss:		2.410309E-02
  validation loss:		1.915848E-02
Epoch took 0.775s

Epoch 30 of 500
  training loss:		2.101976E-02
  validation loss:		1.487667E-02
Epoch took 0.775s

Epoch 31 of 500
  training loss:		1.955226E-02
  validation loss:		1.326650E-02
Epoch took 0.775s

Epoch 32 of 500
  training loss:		1.634842E-02
  validation loss:		1.981189E-02
Epoch took 0.775s

Epoch 33 of 500
  training loss:		1.639167E-02
  validation loss:		2.464637E-02
Epoch took 0.775s

Epoch 34 of 500
  training loss:		1.939976E-02
  validation loss:		2.274293E-02
Epoch took 0.775s

Epoch 35 of 500
  training loss:		1.499329E-02
  validation loss:		1.600642E-02
Epoch took 0.775s

Epoch 36 of 500
  training loss:		1.611288E-02
  validation loss:		9.230140E-03
Epoch took 0.775s

Epoch 37 of 500
  training loss:		1.294305E-02
  validation loss:		1.282302E-02
Epoch took 0.775s

Epoch 38 of 500
  training loss:		1.945635E-02
  validation loss:		9.965360E-03
Epoch took 0.775s

Epoch 39 of 500
  training loss:		1.638443E-02
  validation loss:		2.064109E-02
Epoch took 0.776s

Epoch 40 of 500
  training loss:		1.493099E-02
  validation loss:		9.430379E-03
Epoch took 0.775s

Epoch 41 of 500
  training loss:		1.526297E-02
  validation loss:		1.384255E-02
Epoch took 0.775s

Epoch 42 of 500
  training loss:		1.499006E-02
  validation loss:		9.584215E-03
Epoch took 0.776s

Epoch 43 of 500
  training loss:		1.775254E-02
  validation loss:		2.474949E-02
Epoch took 0.776s

Epoch 44 of 500
  training loss:		1.386135E-02
  validation loss:		2.696530E-02
Epoch took 0.776s

Epoch 45 of 500
  training loss:		1.234567E-02
  validation loss:		2.040621E-02
Epoch took 0.776s

Epoch 46 of 500
  training loss:		1.623596E-02
  validation loss:		1.103853E-02
Epoch took 0.775s

Epoch 47 of 500
  training loss:		1.398918E-02
  validation loss:		1.240818E-02
Epoch took 0.776s

Epoch 48 of 500
  training loss:		1.778270E-02
  validation loss:		2.386387E-02
Epoch took 0.776s

Epoch 49 of 500
  training loss:		1.512254E-02
  validation loss:		1.966169E-02
Epoch took 0.777s

Epoch 50 of 500
  training loss:		1.382314E-02
  validation loss:		1.735965E-02
Epoch took 0.777s

Early stopping, val-loss increased over the last 10 epochs from 0.0158564093031 to 0.0179879669773
Saving model from epoch 40
Training RMSE: 0.00945513
Validation RMSE: 0.00943719
Test RMSE: 0.0092162694782
Test MSE: 8.49396164995e-05
Test MAE: 0.00573842227459
Test R2: -904251744.913 

