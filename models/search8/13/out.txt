Training network with 56340 training samples and 18780 validation samples
Epoch 1 of 500
  training loss:		1.290107E-01
  validation loss:		2.933070E-02
Epoch took 0.923s

Epoch 2 of 500
  training loss:		4.511386E-02
  validation loss:		3.950698E-02
Epoch took 0.871s

Epoch 3 of 500
  training loss:		3.424945E-02
  validation loss:		2.115084E-02
Epoch took 0.871s

Epoch 4 of 500
  training loss:		2.819923E-02
  validation loss:		1.953288E-02
Epoch took 0.871s

Epoch 5 of 500
  training loss:		2.656170E-02
  validation loss:		2.357440E-02
Epoch took 0.871s

Epoch 6 of 500
  training loss:		2.262146E-02
  validation loss:		2.865644E-02
Epoch took 0.871s

Epoch 7 of 500
  training loss:		1.940445E-02
  validation loss:		1.540480E-02
Epoch took 0.871s

Epoch 8 of 500
  training loss:		1.825567E-02
  validation loss:		2.056385E-02
Epoch took 0.871s

Epoch 9 of 500
  training loss:		1.791776E-02
  validation loss:		2.578272E-02
Epoch took 0.871s

Epoch 10 of 500
  training loss:		1.714009E-02
  validation loss:		1.299690E-02
Epoch took 0.871s

Epoch 11 of 500
  training loss:		1.810500E-02
  validation loss:		2.447243E-02
Epoch took 0.871s

Epoch 12 of 500
  training loss:		1.413903E-02
  validation loss:		2.672513E-02
Epoch took 0.877s

Epoch 13 of 500
  training loss:		1.433730E-02
  validation loss:		1.939136E-02
Epoch took 0.877s

Epoch 14 of 500
  training loss:		1.826460E-02
  validation loss:		1.289547E-02
Epoch took 0.877s

Epoch 15 of 500
  training loss:		1.296408E-02
  validation loss:		7.922261E-03
Epoch took 0.877s

Epoch 16 of 500
  training loss:		1.464682E-02
  validation loss:		1.451512E-02
Epoch took 0.877s

Epoch 17 of 500
  training loss:		1.492170E-02
  validation loss:		1.851869E-02
Epoch took 0.877s

Epoch 18 of 500
  training loss:		1.428502E-02
  validation loss:		1.363821E-02
Epoch took 0.877s

Epoch 19 of 500
  training loss:		1.532974E-02
  validation loss:		2.310269E-02
Epoch took 0.877s

Epoch 20 of 500
  training loss:		1.286505E-02
  validation loss:		1.576983E-02
Epoch took 0.877s

Epoch 21 of 500
  training loss:		1.277711E-02
  validation loss:		2.423429E-02
Epoch took 0.877s

Epoch 22 of 500
  training loss:		1.269859E-02
  validation loss:		1.832590E-02
Epoch took 0.877s

Epoch 23 of 500
  training loss:		1.099544E-02
  validation loss:		3.917022E-02
Epoch took 0.877s

Epoch 24 of 500
  training loss:		1.192265E-02
  validation loss:		3.093505E-02
Epoch took 0.877s

Epoch 25 of 500
  training loss:		1.360121E-02
  validation loss:		1.966414E-02
Epoch took 0.877s

Epoch 26 of 500
  training loss:		1.172883E-02
  validation loss:		9.432683E-03
Epoch took 0.877s

Epoch 27 of 500
  training loss:		1.090816E-02
  validation loss:		2.966651E-02
Epoch took 0.877s

Epoch 28 of 500
  training loss:		1.070448E-02
  validation loss:		1.320143E-02
Epoch took 0.878s

Epoch 29 of 500
  training loss:		1.020672E-02
  validation loss:		3.823014E-02
Epoch took 0.878s

Epoch 30 of 500
  training loss:		1.106697E-02
  validation loss:		3.455092E-02
Epoch took 0.878s

Early stopping, val-loss increased over the last 10 epochs from 0.0176951202672 to 0.0257411283862
Saving model from epoch 20
Training RMSE: 0.0157453
Validation RMSE: 0.0157782
Test RMSE: 0.0156459957361
Test MSE: 0.000244797178311
Test MAE: 0.014219969511
Test R2: -2606066117.24 

