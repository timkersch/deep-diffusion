Training network with 56340 training samples and 18780 validation samples
Epoch 1 of 500
  training loss:		6.538601E-01
  validation loss:		7.454399E-01
Epoch took 0.770s

Epoch 2 of 500
  training loss:		4.353484E-01
  validation loss:		3.507272E-01
Epoch took 0.773s

Epoch 3 of 500
  training loss:		3.156540E-01
  validation loss:		2.602478E-01
Epoch took 0.774s

Epoch 4 of 500
  training loss:		2.357918E-01
  validation loss:		1.809485E-01
Epoch took 0.774s

Epoch 5 of 500
  training loss:		1.848521E-01
  validation loss:		1.356598E-01
Epoch took 0.774s

Epoch 6 of 500
  training loss:		1.522896E-01
  validation loss:		1.028793E-01
Epoch took 0.774s

Epoch 7 of 500
  training loss:		1.338127E-01
  validation loss:		8.557643E-02
Epoch took 0.774s

Epoch 8 of 500
  training loss:		1.210769E-01
  validation loss:		7.194176E-02
Epoch took 0.774s

Epoch 9 of 500
  training loss:		1.069576E-01
  validation loss:		6.073066E-02
Epoch took 0.773s

Epoch 10 of 500
  training loss:		9.896476E-02
  validation loss:		5.051048E-02
Epoch took 0.773s

Epoch 11 of 500
  training loss:		9.890571E-02
  validation loss:		4.560844E-02
Epoch took 0.773s

Epoch 12 of 500
  training loss:		9.462426E-02
  validation loss:		4.474323E-02
Epoch took 0.773s

Epoch 13 of 500
  training loss:		9.297529E-02
  validation loss:		3.919004E-02
Epoch took 0.774s

Epoch 14 of 500
  training loss:		8.695483E-02
  validation loss:		3.506740E-02
Epoch took 0.774s

Epoch 15 of 500
  training loss:		8.752888E-02
  validation loss:		3.403554E-02
Epoch took 0.774s

Epoch 16 of 500
  training loss:		8.660009E-02
  validation loss:		3.446122E-02
Epoch took 0.774s

Epoch 17 of 500
  training loss:		8.243625E-02
  validation loss:		3.191369E-02
Epoch took 0.773s

Epoch 18 of 500
  training loss:		8.210100E-02
  validation loss:		2.906883E-02
Epoch took 0.774s

Epoch 19 of 500
  training loss:		8.003320E-02
  validation loss:		2.889079E-02
Epoch took 0.773s

Epoch 20 of 500
  training loss:		7.986069E-02
  validation loss:		2.801620E-02
Epoch took 0.773s

Epoch 21 of 500
  training loss:		7.755400E-02
  validation loss:		3.107308E-02
Epoch took 0.773s

Epoch 22 of 500
  training loss:		7.646078E-02
  validation loss:		2.847063E-02
Epoch took 0.774s

Epoch 23 of 500
  training loss:		7.772185E-02
  validation loss:		3.494127E-02
Epoch took 0.773s

Epoch 24 of 500
  training loss:		7.519155E-02
  validation loss:		2.544504E-02
Epoch took 0.773s

Epoch 25 of 500
  training loss:		7.464061E-02
  validation loss:		3.008405E-02
Epoch took 0.774s

Epoch 26 of 500
  training loss:		7.373026E-02
  validation loss:		2.777206E-02
Epoch took 0.773s

Epoch 27 of 500
  training loss:		7.223237E-02
  validation loss:		2.573015E-02
Epoch took 0.773s

Epoch 28 of 500
  training loss:		7.359704E-02
  validation loss:		2.858236E-02
Epoch took 0.774s

Epoch 29 of 500
  training loss:		7.279942E-02
  validation loss:		2.604274E-02
Epoch took 0.773s

Epoch 30 of 500
  training loss:		7.195913E-02
  validation loss:		2.953399E-02
Epoch took 0.773s

Epoch 31 of 500
  training loss:		7.284227E-02
  validation loss:		2.686433E-02
Epoch took 0.774s

Epoch 32 of 500
  training loss:		6.911438E-02
  validation loss:		2.825984E-02
Epoch took 0.774s

Epoch 33 of 500
  training loss:		6.979557E-02
  validation loss:		2.643844E-02
Epoch took 0.774s

Epoch 34 of 500
  training loss:		6.887543E-02
  validation loss:		2.256472E-02
Epoch took 0.774s

Epoch 35 of 500
  training loss:		6.938346E-02
  validation loss:		2.503092E-02
Epoch took 0.774s

Epoch 36 of 500
  training loss:		7.116986E-02
  validation loss:		2.728304E-02
Epoch took 0.774s

Epoch 37 of 500
  training loss:		6.994974E-02
  validation loss:		2.527975E-02
Epoch took 0.773s

Epoch 38 of 500
  training loss:		6.627209E-02
  validation loss:		2.546635E-02
Epoch took 0.773s

Epoch 39 of 500
  training loss:		6.770395E-02
  validation loss:		2.445462E-02
Epoch took 0.774s

Epoch 40 of 500
  training loss:		6.887193E-02
  validation loss:		2.350056E-02
Epoch took 0.774s

Epoch 41 of 500
  training loss:		6.407672E-02
  validation loss:		2.621202E-02
Epoch took 0.774s

Epoch 42 of 500
  training loss:		6.424383E-02
  validation loss:		2.134784E-02
Epoch took 0.774s

Epoch 43 of 500
  training loss:		6.815156E-02
  validation loss:		2.312451E-02
Epoch took 0.773s

Epoch 44 of 500
  training loss:		6.729974E-02
  validation loss:		2.628826E-02
Epoch took 0.774s

Epoch 45 of 500
  training loss:		6.277742E-02
  validation loss:		2.351545E-02
Epoch took 0.774s

Epoch 46 of 500
  training loss:		6.428625E-02
  validation loss:		2.027991E-02
Epoch took 0.773s

Epoch 47 of 500
  training loss:		6.527263E-02
  validation loss:		2.541369E-02
Epoch took 0.773s

Epoch 48 of 500
  training loss:		6.441087E-02
  validation loss:		2.317074E-02
Epoch took 0.774s

Epoch 49 of 500
  training loss:		6.487448E-02
  validation loss:		2.291632E-02
Epoch took 0.773s

Epoch 50 of 500
  training loss:		6.382393E-02
  validation loss:		2.082126E-02
Epoch took 0.773s

Epoch 51 of 500
  training loss:		6.229805E-02
  validation loss:		2.380332E-02
Epoch took 0.773s

Epoch 52 of 500
  training loss:		6.302025E-02
  validation loss:		2.695671E-02
Epoch took 0.774s

Epoch 53 of 500
  training loss:		6.348548E-02
  validation loss:		2.458349E-02
Epoch took 0.774s

Epoch 54 of 500
  training loss:		6.290938E-02
  validation loss:		2.308820E-02
Epoch took 0.773s

Epoch 55 of 500
  training loss:		6.266151E-02
  validation loss:		2.308350E-02
Epoch took 0.774s

Epoch 56 of 500
  training loss:		6.236704E-02
  validation loss:		2.310611E-02
Epoch took 0.773s

Epoch 57 of 500
  training loss:		6.107489E-02
  validation loss:		2.354933E-02
Epoch took 0.774s

Epoch 58 of 500
  training loss:		6.259144E-02
  validation loss:		2.272432E-02
Epoch took 0.773s

Epoch 59 of 500
  training loss:		6.064493E-02
  validation loss:		2.272497E-02
Epoch took 0.773s

Epoch 60 of 500
  training loss:		6.194768E-02
  validation loss:		2.195857E-02
Epoch took 0.773s

Epoch 61 of 500
  training loss:		6.076364E-02
  validation loss:		2.204401E-02
Epoch took 0.774s

Epoch 62 of 500
  training loss:		6.054406E-02
  validation loss:		2.465980E-02
Epoch took 0.774s

Epoch 63 of 500
  training loss:		6.102627E-02
  validation loss:		2.329275E-02
Epoch took 0.774s

Epoch 64 of 500
  training loss:		5.854504E-02
  validation loss:		1.971522E-02
Epoch took 0.774s

Epoch 65 of 500
  training loss:		5.854738E-02
  validation loss:		2.131245E-02
Epoch took 0.773s

Epoch 66 of 500
  training loss:		5.688360E-02
  validation loss:		2.114730E-02
Epoch took 0.773s

Epoch 67 of 500
  training loss:		5.829408E-02
  validation loss:		2.199826E-02
Epoch took 0.773s

Epoch 68 of 500
  training loss:		5.951898E-02
  validation loss:		2.067748E-02
Epoch took 0.773s

Epoch 69 of 500
  training loss:		5.902821E-02
  validation loss:		2.003532E-02
Epoch took 0.776s

Epoch 70 of 500
  training loss:		5.702302E-02
  validation loss:		2.082641E-02
Epoch took 0.775s

Epoch 71 of 500
  training loss:		5.923557E-02
  validation loss:		2.143245E-02
Epoch took 0.776s

Epoch 72 of 500
  training loss:		5.723922E-02
  validation loss:		2.563046E-02
Epoch took 0.776s

Epoch 73 of 500
  training loss:		5.773886E-02
  validation loss:		1.926224E-02
Epoch took 0.776s

Epoch 74 of 500
  training loss:		5.722329E-02
  validation loss:		2.013926E-02
Epoch took 0.776s

Epoch 75 of 500
  training loss:		5.856361E-02
  validation loss:		1.925283E-02
Epoch took 0.776s

Epoch 76 of 500
  training loss:		5.683562E-02
  validation loss:		2.069615E-02
Epoch took 0.776s

Epoch 77 of 500
  training loss:		5.812448E-02
  validation loss:		2.138703E-02
Epoch took 0.776s

Epoch 78 of 500
  training loss:		5.716128E-02
  validation loss:		1.826406E-02
Epoch took 0.776s

Epoch 79 of 500
  training loss:		5.676203E-02
  validation loss:		1.999761E-02
Epoch took 0.776s

Epoch 80 of 500
  training loss:		5.547584E-02
  validation loss:		1.991565E-02
Epoch took 0.776s

Epoch 81 of 500
  training loss:		5.664286E-02
  validation loss:		2.088877E-02
Epoch took 0.776s

Epoch 82 of 500
  training loss:		5.687471E-02
  validation loss:		2.043294E-02
Epoch took 0.776s

Epoch 83 of 500
  training loss:		5.472877E-02
  validation loss:		1.863028E-02
Epoch took 0.776s

Epoch 84 of 500
  training loss:		5.732846E-02
  validation loss:		2.310353E-02
Epoch took 0.776s

Epoch 85 of 500
  training loss:		5.484624E-02
  validation loss:		1.909405E-02
Epoch took 0.776s

Epoch 86 of 500
  training loss:		5.467763E-02
  validation loss:		1.931785E-02
Epoch took 0.776s

Epoch 87 of 500
  training loss:		5.646224E-02
  validation loss:		2.129460E-02
Epoch took 0.776s

Epoch 88 of 500
  training loss:		5.419658E-02
  validation loss:		2.139790E-02
Epoch took 0.776s

Epoch 89 of 500
  training loss:		5.317596E-02
  validation loss:		2.084960E-02
Epoch took 0.776s

Epoch 90 of 500
  training loss:		5.371898E-02
  validation loss:		1.834989E-02
Epoch took 0.776s

Epoch 91 of 500
  training loss:		5.407941E-02
  validation loss:		2.145152E-02
Epoch took 0.776s

Epoch 92 of 500
  training loss:		5.452797E-02
  validation loss:		2.006775E-02
Epoch took 0.776s

Epoch 93 of 500
  training loss:		5.381978E-02
  validation loss:		1.837212E-02
Epoch took 0.776s

Epoch 94 of 500
  training loss:		5.405496E-02
  validation loss:		1.942173E-02
Epoch took 0.776s

Epoch 95 of 500
  training loss:		5.338531E-02
  validation loss:		1.962794E-02
Epoch took 0.776s

Epoch 96 of 500
  training loss:		5.418445E-02
  validation loss:		2.027518E-02
Epoch took 0.776s

Epoch 97 of 500
  training loss:		5.328194E-02
  validation loss:		2.040063E-02
Epoch took 0.776s

Epoch 98 of 500
  training loss:		5.385979E-02
  validation loss:		1.825914E-02
Epoch took 0.776s

Epoch 99 of 500
  training loss:		5.231900E-02
  validation loss:		1.756961E-02
Epoch took 0.776s

Epoch 100 of 500
  training loss:		5.191142E-02
  validation loss:		1.860903E-02
Epoch took 0.776s

Epoch 101 of 500
  training loss:		5.259622E-02
  validation loss:		1.946575E-02
Epoch took 0.776s

Epoch 102 of 500
  training loss:		5.401172E-02
  validation loss:		1.745699E-02
Epoch took 0.775s

Epoch 103 of 500
  training loss:		5.264008E-02
  validation loss:		2.051235E-02
Epoch took 0.776s

Epoch 104 of 500
  training loss:		5.371024E-02
  validation loss:		2.056976E-02
Epoch took 0.776s

Epoch 105 of 500
  training loss:		5.383827E-02
  validation loss:		1.890835E-02
Epoch took 0.776s

Epoch 106 of 500
  training loss:		5.122310E-02
  validation loss:		1.734078E-02
Epoch took 0.776s

Epoch 107 of 500
  training loss:		5.239338E-02
  validation loss:		2.019543E-02
Epoch took 0.776s

Epoch 108 of 500
  training loss:		5.332146E-02
  validation loss:		2.022477E-02
Epoch took 0.776s

Epoch 109 of 500
  training loss:		5.162371E-02
  validation loss:		1.998983E-02
Epoch took 0.776s

Epoch 110 of 500
  training loss:		5.355930E-02
  validation loss:		1.800311E-02
Epoch took 0.776s

Epoch 111 of 500
  training loss:		5.217545E-02
  validation loss:		2.085825E-02
Epoch took 0.776s

Epoch 112 of 500
  training loss:		5.248391E-02
  validation loss:		1.904770E-02
Epoch took 0.776s

Epoch 113 of 500
  training loss:		5.216196E-02
  validation loss:		1.697388E-02
Epoch took 0.776s

Epoch 114 of 500
  training loss:		5.170516E-02
  validation loss:		2.672796E-02
Epoch took 0.776s

Epoch 115 of 500
  training loss:		5.196114E-02
  validation loss:		2.024223E-02
Epoch took 0.776s

Epoch 116 of 500
  training loss:		5.038822E-02
  validation loss:		1.804052E-02
Epoch took 0.776s

Epoch 117 of 500
  training loss:		5.037037E-02
  validation loss:		1.848314E-02
Epoch took 0.776s

Epoch 118 of 500
  training loss:		4.964765E-02
  validation loss:		1.960053E-02
Epoch took 0.776s

Epoch 119 of 500
  training loss:		5.110962E-02
  validation loss:		1.825924E-02
Epoch took 0.776s

Epoch 120 of 500
  training loss:		4.920723E-02
  validation loss:		2.055046E-02
Epoch took 0.776s

Early stopping, val-loss increased over the last 15 epochs from 0.0193978575058 to 0.0196358557032
Saving model from epoch 105
Training RMSE: 0.018989
Validation RMSE: 0.0189506
Test RMSE: 0.0188696142286
Test MSE: 0.000356062315404
Test MAE: 0.0138970455155
Test R2: -3790575176.4 

