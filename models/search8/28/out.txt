Training network with 56340 training samples and 18780 validation samples
Epoch 1 of 500
  training loss:		2.840381E-01
  validation loss:		1.273868E+01
Epoch took 0.758s

Epoch 2 of 500
  training loss:		6.710299E-02
  validation loss:		4.917104E-02
Epoch took 0.697s

Epoch 3 of 500
  training loss:		5.113936E-02
  validation loss:		4.908147E-02
Epoch took 0.698s

Epoch 4 of 500
  training loss:		3.848138E-02
  validation loss:		4.815428E-02
Epoch took 0.701s

Epoch 5 of 500
  training loss:		4.480868E-02
  validation loss:		7.556892E-02
Epoch took 0.701s

Epoch 6 of 500
  training loss:		4.033734E-02
  validation loss:		3.787250E-02
Epoch took 0.701s

Epoch 7 of 500
  training loss:		3.681914E-02
  validation loss:		4.321473E-02
Epoch took 0.701s

Epoch 8 of 500
  training loss:		3.132831E-02
  validation loss:		2.661002E-02
Epoch took 0.701s

Epoch 9 of 500
  training loss:		2.690544E-02
  validation loss:		3.320337E-02
Epoch took 0.701s

Epoch 10 of 500
  training loss:		2.792135E-02
  validation loss:		3.945602E-02
Epoch took 0.701s

Epoch 11 of 500
  training loss:		3.053333E-02
  validation loss:		2.060129E-02
Epoch took 0.701s

Epoch 12 of 500
  training loss:		2.666632E-02
  validation loss:		2.348970E-02
Epoch took 0.701s

Epoch 13 of 500
  training loss:		2.469194E-02
  validation loss:		1.963050E-02
Epoch took 0.701s

Epoch 14 of 500
  training loss:		2.279467E-02
  validation loss:		3.080004E-02
Epoch took 0.701s

Epoch 15 of 500
  training loss:		2.746752E-02
  validation loss:		2.632184E-02
Epoch took 0.701s

Epoch 16 of 500
  training loss:		2.671014E-02
  validation loss:		2.428721E-02
Epoch took 0.701s

Epoch 17 of 500
  training loss:		2.287992E-02
  validation loss:		2.377207E-02
Epoch took 0.702s

Epoch 18 of 500
  training loss:		2.302180E-02
  validation loss:		2.200940E-02
Epoch took 0.701s

Epoch 19 of 500
  training loss:		2.118521E-02
  validation loss:		2.451448E-02
Epoch took 0.701s

Epoch 20 of 500
  training loss:		2.084969E-02
  validation loss:		1.233634E-02
Epoch took 0.701s

Epoch 21 of 500
  training loss:		1.834716E-02
  validation loss:		2.233377E-02
Epoch took 0.702s

Epoch 22 of 500
  training loss:		2.124210E-02
  validation loss:		1.811971E-02
Epoch took 0.701s

Epoch 23 of 500
  training loss:		1.929610E-02
  validation loss:		2.954593E-02
Epoch took 0.701s

Epoch 24 of 500
  training loss:		1.711705E-02
  validation loss:		1.619084E-02
Epoch took 0.702s

Epoch 25 of 500
  training loss:		2.146891E-02
  validation loss:		2.023515E-02
Epoch took 0.701s

Epoch 26 of 500
  training loss:		2.012356E-02
  validation loss:		1.656056E-02
Epoch took 0.701s

Epoch 27 of 500
  training loss:		1.891920E-02
  validation loss:		3.058590E-02
Epoch took 0.702s

Epoch 28 of 500
  training loss:		1.735794E-02
  validation loss:		2.742194E-02
Epoch took 0.701s

Epoch 29 of 500
  training loss:		1.750232E-02
  validation loss:		8.190817E-03
Epoch took 0.701s

Epoch 30 of 500
  training loss:		1.656915E-02
  validation loss:		3.235982E-02
Epoch took 0.701s

Epoch 31 of 500
  training loss:		2.273473E-02
  validation loss:		1.196532E-02
Epoch took 0.701s

Epoch 32 of 500
  training loss:		1.755293E-02
  validation loss:		1.970087E-02
Epoch took 0.701s

Epoch 33 of 500
  training loss:		1.807175E-02
  validation loss:		3.241165E-02
Epoch took 0.701s

Epoch 34 of 500
  training loss:		1.947603E-02
  validation loss:		2.835003E-02
Epoch took 0.701s

Epoch 35 of 500
  training loss:		1.728205E-02
  validation loss:		2.144509E-02
Epoch took 0.701s

Epoch 36 of 500
  training loss:		1.649171E-02
  validation loss:		1.785279E-02
Epoch took 0.701s

Epoch 37 of 500
  training loss:		2.133312E-02
  validation loss:		4.483754E-02
Epoch took 0.701s

Epoch 38 of 500
  training loss:		1.990620E-02
  validation loss:		2.288633E-02
Epoch took 0.701s

Epoch 39 of 500
  training loss:		1.523390E-02
  validation loss:		1.867645E-02
Epoch took 0.701s

Epoch 40 of 500
  training loss:		1.710487E-02
  validation loss:		2.819848E-02
Epoch took 0.701s

Early stopping, val-loss increased over the last 10 epochs from 0.022154445941 to 0.0246324561019
Saving model from epoch 30
Training RMSE: 0.0323513
Validation RMSE: 0.0323603
Test RMSE: 0.0321232900023
Test MSE: 0.00103190587834
Test MAE: 0.030549351126
Test R2: -10985483048.4 

