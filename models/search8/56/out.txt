Training network with 56340 training samples and 18780 validation samples
Epoch 1 of 500
  training loss:		2.989808E-01
  validation loss:		7.062202E+01
Epoch took 0.759s

Epoch 2 of 500
  training loss:		7.556025E-02
  validation loss:		6.633463E-02
Epoch took 0.701s

Epoch 3 of 500
  training loss:		5.570117E-02
  validation loss:		4.663681E-02
Epoch took 0.702s

Epoch 4 of 500
  training loss:		4.037529E-02
  validation loss:		4.414341E-02
Epoch took 0.701s

Epoch 5 of 500
  training loss:		4.285102E-02
  validation loss:		3.478836E-02
Epoch took 0.701s

Epoch 6 of 500
  training loss:		3.761521E-02
  validation loss:		3.184259E-02
Epoch took 0.702s

Epoch 7 of 500
  training loss:		3.660557E-02
  validation loss:		4.183776E-02
Epoch took 0.701s

Epoch 8 of 500
  training loss:		3.490576E-02
  validation loss:		5.059850E-02
Epoch took 0.702s

Epoch 9 of 500
  training loss:		2.781652E-02
  validation loss:		3.606554E-02
Epoch took 0.701s

Epoch 10 of 500
  training loss:		3.175230E-02
  validation loss:		4.557754E-02
Epoch took 0.701s

Epoch 11 of 500
  training loss:		2.591878E-02
  validation loss:		3.806769E-02
Epoch took 0.701s

Epoch 12 of 500
  training loss:		2.709101E-02
  validation loss:		5.648518E-02
Epoch took 0.701s

Epoch 13 of 500
  training loss:		2.756076E-02
  validation loss:		3.480205E-02
Epoch took 0.701s

Epoch 14 of 500
  training loss:		2.681462E-02
  validation loss:		2.123481E-02
Epoch took 0.701s

Epoch 15 of 500
  training loss:		2.125492E-02
  validation loss:		1.750932E-02
Epoch took 0.701s

Epoch 16 of 500
  training loss:		2.484739E-02
  validation loss:		2.856777E-02
Epoch took 0.701s

Epoch 17 of 500
  training loss:		2.405808E-02
  validation loss:		2.776514E-02
Epoch took 0.701s

Epoch 18 of 500
  training loss:		2.265229E-02
  validation loss:		3.258814E-02
Epoch took 0.701s

Epoch 19 of 500
  training loss:		2.546762E-02
  validation loss:		2.483726E-02
Epoch took 0.701s

Epoch 20 of 500
  training loss:		2.063483E-02
  validation loss:		2.033114E-02
Epoch took 0.701s

Epoch 21 of 500
  training loss:		2.202961E-02
  validation loss:		3.027207E-02
Epoch took 0.701s

Epoch 22 of 500
  training loss:		2.080618E-02
  validation loss:		2.469647E-02
Epoch took 0.701s

Epoch 23 of 500
  training loss:		2.048692E-02
  validation loss:		1.917720E-02
Epoch took 0.702s

Epoch 24 of 500
  training loss:		2.270683E-02
  validation loss:		5.307532E-02
Epoch took 0.701s

Epoch 25 of 500
  training loss:		1.958207E-02
  validation loss:		1.307656E-02
Epoch took 0.701s

Epoch 26 of 500
  training loss:		1.775447E-02
  validation loss:		2.027858E-02
Epoch took 0.701s

Epoch 27 of 500
  training loss:		2.354380E-02
  validation loss:		1.973759E-02
Epoch took 0.701s

Epoch 28 of 500
  training loss:		1.972379E-02
  validation loss:		1.351503E-02
Epoch took 0.702s

Epoch 29 of 500
  training loss:		1.791666E-02
  validation loss:		2.724144E-02
Epoch took 0.702s

Epoch 30 of 500
  training loss:		2.215667E-02
  validation loss:		2.792254E-02
Epoch took 0.701s

Epoch 31 of 500
  training loss:		1.741291E-02
  validation loss:		4.553998E-02
Epoch took 0.701s

Epoch 32 of 500
  training loss:		1.960793E-02
  validation loss:		3.490711E-02
Epoch took 0.702s

Epoch 33 of 500
  training loss:		2.002260E-02
  validation loss:		3.283199E-02
Epoch took 0.701s

Epoch 34 of 500
  training loss:		1.960958E-02
  validation loss:		2.076059E-02
Epoch took 0.701s

Epoch 35 of 500
  training loss:		1.811138E-02
  validation loss:		2.304638E-02
Epoch took 0.701s

Epoch 36 of 500
  training loss:		1.733548E-02
  validation loss:		1.693334E-02
Epoch took 0.701s

Epoch 37 of 500
  training loss:		1.617250E-02
  validation loss:		2.187853E-02
Epoch took 0.702s

Epoch 38 of 500
  training loss:		2.024694E-02
  validation loss:		2.728830E-02
Epoch took 0.701s

Epoch 39 of 500
  training loss:		1.847934E-02
  validation loss:		1.560910E-02
Epoch took 0.702s

Epoch 40 of 500
  training loss:		1.722795E-02
  validation loss:		2.833198E-02
Epoch took 0.701s

Epoch 41 of 500
  training loss:		1.699192E-02
  validation loss:		2.426733E-02
Epoch took 0.702s

Epoch 42 of 500
  training loss:		1.743531E-02
  validation loss:		2.139912E-02
Epoch took 0.702s

Epoch 43 of 500
  training loss:		2.693543E-02
  validation loss:		1.589748E-02
Epoch took 0.703s

Epoch 44 of 500
  training loss:		1.539936E-02
  validation loss:		3.651508E-02
Epoch took 0.703s

Epoch 45 of 500
  training loss:		1.804818E-02
  validation loss:		2.103303E-02
Epoch took 0.702s

Early stopping, val-loss increased over the last 15 epochs from 0.0255388157304 to 0.025749289203
Saving model from epoch 30
Training RMSE: 0.027932
Validation RMSE: 0.0279134
Test RMSE: 0.0273300092667
Test MSE: 0.000746929377783
Test MAE: 0.0203465893865
Test R2: -7951674249.39 

