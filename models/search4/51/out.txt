Training network with 75120 training samples and 14085 validation samples
Epoch 1 of 100
  training loss:		7.981150E-02
  validation loss:		4.666541E-02
Epoch took 15.994s

Epoch 2 of 100
  training loss:		4.188813E-02
  validation loss:		3.833751E-02
Epoch took 18.072s

Epoch 3 of 100
  training loss:		3.639361E-02
  validation loss:		3.752867E-02
Epoch took 15.187s

Epoch 4 of 100
  training loss:		3.401674E-02
  validation loss:		3.110340E-02
Epoch took 16.061s

Epoch 5 of 100
  training loss:		3.208447E-02
  validation loss:		3.046535E-02
Epoch took 17.035s

Epoch 6 of 100
  training loss:		3.111124E-02
  validation loss:		3.371517E-02
Epoch took 16.338s

Epoch 7 of 100
  training loss:		3.024480E-02
  validation loss:		2.861109E-02
Epoch took 16.361s

Epoch 8 of 100
  training loss:		2.992539E-02
  validation loss:		3.154656E-02
Epoch took 15.888s

Epoch 9 of 100
  training loss:		2.924199E-02
  validation loss:		2.848505E-02
Epoch took 17.223s

Epoch 10 of 100
  training loss:		2.895458E-02
  validation loss:		2.868653E-02
Epoch took 17.370s

Epoch 11 of 100
  training loss:		2.952279E-02
  validation loss:		3.037095E-02
Epoch took 16.182s

Epoch 12 of 100
  training loss:		2.886586E-02
  validation loss:		2.855039E-02
Epoch took 16.444s

Epoch 13 of 100
  training loss:		2.852126E-02
  validation loss:		2.797445E-02
Epoch took 16.678s

Epoch 14 of 100
  training loss:		2.810354E-02
  validation loss:		2.839774E-02
Epoch took 17.583s

Epoch 15 of 100
  training loss:		2.827886E-02
  validation loss:		2.759679E-02
Epoch took 15.391s

Epoch 16 of 100
  training loss:		2.812924E-02
  validation loss:		2.751174E-02
Epoch took 16.464s

Epoch 17 of 100
  training loss:		2.779744E-02
  validation loss:		2.820135E-02
Epoch took 17.645s

Epoch 18 of 100
  training loss:		2.788494E-02
  validation loss:		2.730646E-02
Epoch took 17.409s

Epoch 19 of 100
  training loss:		2.787240E-02
  validation loss:		2.706934E-02
Epoch took 15.697s

Epoch 20 of 100
  training loss:		2.764123E-02
  validation loss:		2.750138E-02
Epoch took 17.335s

Epoch 21 of 100
  training loss:		2.787569E-02
  validation loss:		2.854143E-02
Epoch took 14.454s

Epoch 22 of 100
  training loss:		2.774105E-02
  validation loss:		2.684117E-02
Epoch took 15.589s

Epoch 23 of 100
  training loss:		2.758129E-02
  validation loss:		2.718560E-02
Epoch took 16.080s

Epoch 24 of 100
  training loss:		2.759886E-02
  validation loss:		2.724142E-02
Epoch took 17.478s

Epoch 25 of 100
  training loss:		2.761824E-02
  validation loss:		2.706107E-02
Epoch took 16.833s

Epoch 26 of 100
  training loss:		2.726813E-02
  validation loss:		2.681182E-02
Epoch took 17.567s

Epoch 27 of 100
  training loss:		2.729912E-02
  validation loss:		2.836293E-02
Epoch took 16.200s

Epoch 28 of 100
  training loss:		2.739419E-02
  validation loss:		2.733353E-02
Epoch took 18.688s

Epoch 29 of 100
  training loss:		2.725788E-02
  validation loss:		2.780352E-02
Epoch took 15.355s

Epoch 30 of 100
  training loss:		2.732368E-02
  validation loss:		2.724482E-02
Epoch took 18.076s

Early stopping, val-loss increased over the last 5 epochs from 0.0273741366951 to 0.0275113241168
Training RMSE: 1.60676599388e-07
Validation RMSE: 1.61337232685e-07
