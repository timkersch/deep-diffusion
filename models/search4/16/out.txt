Training network with 75120 training samples and 14085 validation samples
Epoch 1 of 100
  training loss:		2.570063E-03
  validation loss:		2.415647E-04
Epoch took 12.146s

Epoch 2 of 100
  training loss:		2.623677E-05
  validation loss:		5.419365E-05
Epoch took 11.137s

Epoch 3 of 100
  training loss:		4.879714E-04
  validation loss:		1.423079E-04
Epoch took 10.881s

Epoch 4 of 100
  training loss:		3.411659E-05
  validation loss:		5.216037E-06
Epoch took 11.123s

Epoch 5 of 100
  training loss:		1.571125E-05
  validation loss:		3.949096E-06
Epoch took 11.252s

Epoch 6 of 100
  training loss:		1.745874E-04
  validation loss:		1.638337E-05
Epoch took 12.372s

Epoch 7 of 100
  training loss:		5.728352E-05
  validation loss:		3.826659E-06
Epoch took 11.571s

Epoch 8 of 100
  training loss:		3.185750E-04
  validation loss:		7.946152E-06
Epoch took 11.910s

Epoch 9 of 100
  training loss:		5.135073E-06
  validation loss:		5.607540E-06
Epoch took 11.427s

Epoch 10 of 100
  training loss:		1.638960E-05
  validation loss:		2.053652E-05
Epoch took 11.614s

Epoch 11 of 100
  training loss:		1.268881E-04
  validation loss:		7.710233E-06
Epoch took 11.878s

Epoch 12 of 100
  training loss:		3.119062E-06
  validation loss:		8.541573E-06
Epoch took 11.343s

Epoch 13 of 100
  training loss:		9.458811E-05
  validation loss:		8.802230E-06
Epoch took 11.896s

Epoch 14 of 100
  training loss:		2.464632E-05
  validation loss:		7.595712E-05
Epoch took 12.247s

Epoch 15 of 100
  training loss:		1.513525E-05
  validation loss:		8.512251E-06
Epoch took 12.107s

Early stopping, val-loss increased over the last 5 epochs from 1.08600487769e-05 to 2.19046815674e-05
Training RMSE: 0.00450330975746
Validation RMSE: 0.00453105456242
