Training network with 75120 training samples and 14085 validation samples
Epoch 1 of 100
  training loss:		1.875104E-01
  validation loss:		6.240410E-02
Epoch took 13.362s

Epoch 2 of 100
  training loss:		5.333262E-02
  validation loss:		4.794429E-02
Epoch took 14.411s

Epoch 3 of 100
  training loss:		4.399621E-02
  validation loss:		4.224295E-02
Epoch took 14.098s

Epoch 4 of 100
  training loss:		3.921272E-02
  validation loss:		3.808513E-02
Epoch took 14.891s

Epoch 5 of 100
  training loss:		3.656384E-02
  validation loss:		3.630900E-02
Epoch took 14.731s

Epoch 6 of 100
  training loss:		3.548925E-02
  validation loss:		3.635559E-02
Epoch took 14.829s

Epoch 7 of 100
  training loss:		3.361073E-02
  validation loss:		3.391832E-02
Epoch took 14.508s

Epoch 8 of 100
  training loss:		3.235689E-02
  validation loss:		3.311865E-02
Epoch took 14.890s

Epoch 9 of 100
  training loss:		3.191003E-02
  validation loss:		3.129803E-02
Epoch took 13.821s

Epoch 10 of 100
  training loss:		3.133298E-02
  validation loss:		3.043955E-02
Epoch took 14.973s

Epoch 11 of 100
  training loss:		3.056562E-02
  validation loss:		3.048650E-02
Epoch took 14.137s

Epoch 12 of 100
  training loss:		3.028530E-02
  validation loss:		3.205667E-02
Epoch took 13.173s

Epoch 13 of 100
  training loss:		2.973134E-02
  validation loss:		3.114953E-02
Epoch took 14.475s

Epoch 14 of 100
  training loss:		2.973560E-02
  validation loss:		2.995448E-02
Epoch took 13.821s

Epoch 15 of 100
  training loss:		2.940155E-02
  validation loss:		2.961727E-02
Epoch took 13.895s

Epoch 16 of 100
  training loss:		2.925012E-02
  validation loss:		3.386608E-02
Epoch took 14.439s

Epoch 17 of 100
  training loss:		2.892646E-02
  validation loss:		2.946857E-02
Epoch took 12.890s

Epoch 18 of 100
  training loss:		2.905934E-02
  validation loss:		2.910799E-02
Epoch took 14.333s

Epoch 19 of 100
  training loss:		2.863769E-02
  validation loss:		2.981780E-02
Epoch took 14.373s

Epoch 20 of 100
  training loss:		2.842013E-02
  validation loss:		2.823089E-02
Epoch took 14.953s

Epoch 21 of 100
  training loss:		2.817263E-02
  validation loss:		2.777883E-02
Epoch took 14.031s

Epoch 22 of 100
  training loss:		2.798890E-02
  validation loss:		2.863242E-02
Epoch took 14.324s

Epoch 23 of 100
  training loss:		2.804466E-02
  validation loss:		2.848359E-02
Epoch took 14.305s

Epoch 24 of 100
  training loss:		2.814734E-02
  validation loss:		2.718357E-02
Epoch took 14.137s

Epoch 25 of 100
  training loss:		2.793480E-02
  validation loss:		2.904240E-02
Epoch took 14.009s

Epoch 26 of 100
  training loss:		2.761324E-02
  validation loss:		2.764158E-02
Epoch took 13.907s

Epoch 27 of 100
  training loss:		2.781473E-02
  validation loss:		2.764415E-02
Epoch took 15.194s

Epoch 28 of 100
  training loss:		2.768293E-02
  validation loss:		2.916075E-02
Epoch took 15.531s

Epoch 29 of 100
  training loss:		2.759021E-02
  validation loss:		2.738698E-02
Epoch took 13.983s

Epoch 30 of 100
  training loss:		2.771456E-02
  validation loss:		2.773252E-02
Epoch took 14.613s

Epoch 31 of 100
  training loss:		2.791676E-02
  validation loss:		2.756974E-02
Epoch took 14.887s

Epoch 32 of 100
  training loss:		2.794455E-02
  validation loss:		2.664660E-02
Epoch took 14.623s

Epoch 33 of 100
  training loss:		2.747903E-02
  validation loss:		2.771053E-02
Epoch took 13.258s

Epoch 34 of 100
  training loss:		2.736490E-02
  validation loss:		2.681687E-02
Epoch took 13.974s

Epoch 35 of 100
  training loss:		2.736867E-02
  validation loss:		2.793858E-02
Epoch took 14.709s

Epoch 36 of 100
  training loss:		2.719566E-02
  validation loss:		2.716285E-02
Epoch took 15.143s

Epoch 37 of 100
  training loss:		2.741393E-02
  validation loss:		2.706991E-02
Epoch took 14.404s

Epoch 38 of 100
  training loss:		2.702175E-02
  validation loss:		2.805934E-02
Epoch took 13.839s

Epoch 39 of 100
  training loss:		2.733571E-02
  validation loss:		2.798860E-02
Epoch took 13.083s

Epoch 40 of 100
  training loss:		2.751237E-02
  validation loss:		2.901632E-02
Epoch took 14.396s

Early stopping, val-loss increased over the last 5 epochs from 0.0273364635562 to 0.0278594018439
Training RMSE: 1.62917029641e-07
Validation RMSE: 1.63775227353e-07
