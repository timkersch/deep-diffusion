Training network with 75120 training samples and 14085 validation samples
Epoch 1 of 100
  training loss:		5.116470E-02
  validation loss:		1.635086E-02
Epoch took 9.698s

Epoch 2 of 100
  training loss:		1.088130E-02
  validation loss:		7.204501E-03
Epoch took 9.080s

Epoch 3 of 100
  training loss:		5.626296E-03
  validation loss:		4.319059E-03
Epoch took 9.540s

Epoch 4 of 100
  training loss:		3.597592E-03
  validation loss:		2.922460E-03
Epoch took 9.650s

Epoch 5 of 100
  training loss:		2.481045E-03
  validation loss:		2.044370E-03
Epoch took 9.312s

Epoch 6 of 100
  training loss:		1.770982E-03
  validation loss:		1.485924E-03
Epoch took 9.098s

Epoch 7 of 100
  training loss:		1.299445E-03
  validation loss:		1.111773E-03
Epoch took 10.294s

Epoch 8 of 100
  training loss:		9.761116E-04
  validation loss:		8.402189E-04
Epoch took 8.886s

Epoch 9 of 100
  training loss:		7.481863E-04
  validation loss:		6.435977E-04
Epoch took 9.238s

Epoch 10 of 100
  training loss:		5.735752E-04
  validation loss:		4.998211E-04
Epoch took 9.174s

Epoch 11 of 100
  training loss:		4.446246E-04
  validation loss:		3.895591E-04
Epoch took 10.006s

Epoch 12 of 100
  training loss:		3.463423E-04
  validation loss:		3.054533E-04
Epoch took 9.221s

Epoch 13 of 100
  training loss:		2.717036E-04
  validation loss:		2.407069E-04
Epoch took 10.138s

Epoch 14 of 100
  training loss:		2.123212E-04
  validation loss:		1.875671E-04
Epoch took 10.046s

Epoch 15 of 100
  training loss:		1.672060E-04
  validation loss:		1.487140E-04
Epoch took 10.110s

Epoch 16 of 100
  training loss:		1.322983E-04
  validation loss:		1.201066E-04
Epoch took 9.580s

Epoch 17 of 100
  training loss:		1.059016E-04
  validation loss:		9.536810E-05
Epoch took 9.267s

Epoch 18 of 100
  training loss:		8.495418E-05
  validation loss:		7.967333E-05
Epoch took 9.802s

Epoch 19 of 100
  training loss:		6.942615E-05
  validation loss:		6.329935E-05
Epoch took 9.047s

Epoch 20 of 100
  training loss:		5.577715E-05
  validation loss:		5.243096E-05
Epoch took 9.333s

Epoch 21 of 100
  training loss:		4.609651E-05
  validation loss:		4.201015E-05
Epoch took 10.180s

Epoch 22 of 100
  training loss:		3.839704E-05
  validation loss:		3.504769E-05
Epoch took 9.258s

Epoch 23 of 100
  training loss:		3.247462E-05
  validation loss:		3.027511E-05
Epoch took 8.986s

Epoch 24 of 100
  training loss:		2.644517E-05
  validation loss:		2.865712E-05
Epoch took 8.855s

Epoch 25 of 100
  training loss:		2.161066E-05
  validation loss:		2.036914E-05
Epoch took 8.845s

Epoch 26 of 100
  training loss:		1.796749E-05
  validation loss:		1.735672E-05
Epoch took 10.205s

Epoch 27 of 100
  training loss:		1.585054E-05
  validation loss:		1.455460E-05
Epoch took 9.972s

Epoch 28 of 100
  training loss:		1.253430E-05
  validation loss:		1.188206E-05
Epoch took 10.768s

Epoch 29 of 100
  training loss:		1.064007E-05
  validation loss:		9.997223E-06
Epoch took 9.443s

Epoch 30 of 100
  training loss:		8.756117E-06
  validation loss:		8.054026E-06
Epoch took 9.568s

Epoch 31 of 100
  training loss:		7.522769E-06
  validation loss:		1.145588E-05
Epoch took 9.660s

Epoch 32 of 100
  training loss:		6.564644E-06
  validation loss:		7.055834E-06
Epoch took 8.432s

Epoch 33 of 100
  training loss:		5.216508E-06
  validation loss:		4.613221E-06
Epoch took 8.350s

Epoch 34 of 100
  training loss:		4.550790E-06
  validation loss:		4.261335E-06
Epoch took 8.768s

Epoch 35 of 100
  training loss:		3.703353E-06
  validation loss:		4.950088E-06
Epoch took 9.103s

Epoch 36 of 100
  training loss:		3.168776E-06
  validation loss:		2.962883E-06
Epoch took 9.794s

Epoch 37 of 100
  training loss:		2.562056E-06
  validation loss:		2.390071E-06
Epoch took 8.633s

Epoch 38 of 100
  training loss:		2.292943E-06
  validation loss:		2.077882E-06
Epoch took 10.589s

Epoch 39 of 100
  training loss:		2.140499E-06
  validation loss:		1.932206E-06
Epoch took 8.901s

Epoch 40 of 100
  training loss:		1.867144E-06
  validation loss:		1.651972E-06
Epoch took 8.506s

Epoch 41 of 100
  training loss:		1.566577E-06
  validation loss:		1.370648E-06
Epoch took 9.158s

Epoch 42 of 100
  training loss:		1.317028E-06
  validation loss:		3.064227E-06
Epoch took 9.157s

Epoch 43 of 100
  training loss:		1.133552E-06
  validation loss:		9.783341E-07
Epoch took 9.207s

Epoch 44 of 100
  training loss:		1.816178E-06
  validation loss:		8.201807E-07
Epoch took 9.368s

Epoch 45 of 100
  training loss:		9.853650E-07
  validation loss:		7.772030E-07
Epoch took 9.831s

Epoch 46 of 100
  training loss:		1.425713E-06
  validation loss:		9.345278E-07
Epoch took 9.032s

Epoch 47 of 100
  training loss:		1.494204E-06
  validation loss:		1.325640E-06
Epoch took 10.030s

Epoch 48 of 100
  training loss:		8.225198E-07
  validation loss:		6.875648E-07
Epoch took 10.123s

Epoch 49 of 100
  training loss:		7.199896E-07
  validation loss:		7.275183E-07
Epoch took 10.228s

Epoch 50 of 100
  training loss:		1.279767E-06
  validation loss:		2.984683E-06
Epoch took 9.193s

Epoch 51 of 100
  training loss:		5.798377E-07
  validation loss:		6.335789E-07
Epoch took 9.879s

Epoch 52 of 100
  training loss:		9.322705E-07
  validation loss:		6.021491E-06
Epoch took 8.665s

Epoch 53 of 100
  training loss:		1.179328E-06
  validation loss:		2.114552E-07
Epoch took 10.039s

Epoch 54 of 100
  training loss:		2.648190E-07
  validation loss:		3.137697E-07
Epoch took 8.984s

Epoch 55 of 100
  training loss:		1.146062E-06
  validation loss:		1.027881E-06
Epoch took 8.726s

Early stopping, val-loss increased over the last 5 epochs from 1.33198681294e-06 to 1.64163513183e-06
Training RMSE: 0.00169864636665
Validation RMSE: 0.00172561125334
