Training network with 75120 training samples and 14085 validation samples
Epoch 1 of 100
  training loss:		2.027413E-02
  validation loss:		2.832514E-03
Epoch took 9.478s

Epoch 2 of 100
  training loss:		1.752943E-03
  validation loss:		1.040914E-03
Epoch took 9.810s

Epoch 3 of 100
  training loss:		7.180136E-04
  validation loss:		4.947622E-04
Epoch took 10.850s

Epoch 4 of 100
  training loss:		3.708500E-04
  validation loss:		2.784397E-04
Epoch took 10.707s

Epoch 5 of 100
  training loss:		2.066828E-04
  validation loss:		1.662222E-04
Epoch took 9.696s

Epoch 6 of 100
  training loss:		1.176895E-04
  validation loss:		9.539782E-05
Epoch took 10.561s

Epoch 7 of 100
  training loss:		7.099891E-05
  validation loss:		5.684868E-05
Epoch took 10.245s

Epoch 8 of 100
  training loss:		4.382783E-05
  validation loss:		3.945033E-05
Epoch took 10.750s

Epoch 9 of 100
  training loss:		2.923109E-05
  validation loss:		2.561645E-05
Epoch took 10.798s

Epoch 10 of 100
  training loss:		1.941334E-05
  validation loss:		1.756590E-05
Epoch took 11.045s

Epoch 11 of 100
  training loss:		1.343125E-05
  validation loss:		1.349904E-05
Epoch took 10.509s

Epoch 12 of 100
  training loss:		9.743718E-06
  validation loss:		9.187530E-06
Epoch took 10.257s

Epoch 13 of 100
  training loss:		7.649488E-06
  validation loss:		9.137339E-06
Epoch took 9.721s

Epoch 14 of 100
  training loss:		5.664624E-06
  validation loss:		7.054523E-06
Epoch took 10.128s

Epoch 15 of 100
  training loss:		4.608549E-06
  validation loss:		4.095678E-06
Epoch took 9.935s

Epoch 16 of 100
  training loss:		3.270726E-06
  validation loss:		2.868043E-06
Epoch took 9.807s

Epoch 17 of 100
  training loss:		2.778881E-06
  validation loss:		3.430531E-06
Epoch took 10.000s

Epoch 18 of 100
  training loss:		1.951622E-06
  validation loss:		2.107333E-06
Epoch took 10.131s

Epoch 19 of 100
  training loss:		2.399568E-06
  validation loss:		1.654342E-06
Epoch took 9.333s

Epoch 20 of 100
  training loss:		2.744143E-06
  validation loss:		4.301429E-06
Epoch took 10.692s

Epoch 21 of 100
  training loss:		2.231320E-06
  validation loss:		2.009712E-06
Epoch took 9.434s

Epoch 22 of 100
  training loss:		3.250882E-06
  validation loss:		1.650326E-06
Epoch took 9.593s

Epoch 23 of 100
  training loss:		3.889962E-06
  validation loss:		3.808849E-06
Epoch took 10.558s

Epoch 24 of 100
  training loss:		4.685571E-06
  validation loss:		5.782519E-07
Epoch took 9.507s

Epoch 25 of 100
  training loss:		2.886753E-06
  validation loss:		6.582829E-06
Epoch took 9.899s

Epoch 26 of 100
  training loss:		1.781466E-05
  validation loss:		2.632990E-07
Epoch took 11.237s

Epoch 27 of 100
  training loss:		1.429545E-07
  validation loss:		3.335209E-08
Epoch took 11.452s

Epoch 28 of 100
  training loss:		1.352817E-07
  validation loss:		3.110964E-08
Epoch took 10.580s

Epoch 29 of 100
  training loss:		1.531695E-05
  validation loss:		9.780173E-08
Epoch took 10.209s

Epoch 30 of 100
  training loss:		4.257588E-08
  validation loss:		6.859266E-08
Epoch took 10.402s

Epoch 31 of 100
  training loss:		1.916527E-07
  validation loss:		2.362459E-06
Epoch took 9.846s

Epoch 32 of 100
  training loss:		6.734342E-06
  validation loss:		5.898399E-08
Epoch took 10.058s

Epoch 33 of 100
  training loss:		3.536171E-05
  validation loss:		3.940857E-07
Epoch took 10.739s

Epoch 34 of 100
  training loss:		8.783947E-08
  validation loss:		1.458974E-08
Epoch took 10.811s

Epoch 35 of 100
  training loss:		1.151383E-08
  validation loss:		4.388573E-09
Epoch took 10.001s

Epoch 36 of 100
  training loss:		7.233366E-09
  validation loss:		1.392336E-09
Epoch took 10.346s

Epoch 37 of 100
  training loss:		1.119179E-08
  validation loss:		1.903946E-08
Epoch took 10.383s

Epoch 38 of 100
  training loss:		4.784951E-06
  validation loss:		8.134755E-06
Epoch took 10.188s

Epoch 39 of 100
  training loss:		8.459820E-06
  validation loss:		5.802113E-08
Epoch took 10.265s

Epoch 40 of 100
  training loss:		5.504196E-08
  validation loss:		2.253686E-07
Epoch took 11.783s

Epoch 41 of 100
  training loss:		1.408814E-05
  validation loss:		3.691134E-06
Epoch took 9.721s

Epoch 42 of 100
  training loss:		2.304608E-07
  validation loss:		7.227297E-08
Epoch took 10.570s

Epoch 43 of 100
  training loss:		1.878164E-06
  validation loss:		2.121854E-05
Epoch took 9.832s

Epoch 44 of 100
  training loss:		2.456574E-05
  validation loss:		4.404662E-08
Epoch took 10.053s

Epoch 45 of 100
  training loss:		2.443878E-08
  validation loss:		6.403985E-09
Epoch took 10.671s

Epoch 46 of 100
  training loss:		1.191323E-08
  validation loss:		2.667238E-08
Epoch took 10.093s

Epoch 47 of 100
  training loss:		1.109436E-05
  validation loss:		1.027434E-06
Epoch took 10.004s

Epoch 48 of 100
  training loss:		1.160521E-07
  validation loss:		7.506103E-09
Epoch took 9.696s

Epoch 49 of 100
  training loss:		7.445526E-06
  validation loss:		3.201439E-07
Epoch took 10.241s

Epoch 50 of 100
  training loss:		4.769603E-06
  validation loss:		1.004551E-05
Epoch took 10.355s

Early stopping, val-loss increased over the last 10 epochs from 1.12730842727e-06 to 3.64596685319e-06
Training RMSE: 0.000472091635619
Validation RMSE: 0.000474835331384
