Training network with 75120 training samples and 14085 validation samples
Epoch 1 of 100
  training loss:		1.809800E-01
  validation loss:		6.981543E-02
Epoch took 12.739s

Epoch 2 of 100
  training loss:		5.989517E-02
  validation loss:		5.454031E-02
Epoch took 12.598s

Epoch 3 of 100
  training loss:		4.899683E-02
  validation loss:		4.633970E-02
Epoch took 12.794s

Epoch 4 of 100
  training loss:		4.307316E-02
  validation loss:		4.171178E-02
Epoch took 12.768s

Epoch 5 of 100
  training loss:		3.945102E-02
  validation loss:		3.880942E-02
Epoch took 10.810s

Epoch 6 of 100
  training loss:		3.679273E-02
  validation loss:		3.619487E-02
Epoch took 12.016s

Epoch 7 of 100
  training loss:		3.475171E-02
  validation loss:		3.435299E-02
Epoch took 12.727s

Epoch 8 of 100
  training loss:		3.326143E-02
  validation loss:		3.335475E-02
Epoch took 11.975s

Epoch 9 of 100
  training loss:		3.236780E-02
  validation loss:		3.221641E-02
Epoch took 13.177s

Epoch 10 of 100
  training loss:		3.151274E-02
  validation loss:		3.346068E-02
Epoch took 12.154s

Epoch 11 of 100
  training loss:		3.071347E-02
  validation loss:		3.050767E-02
Epoch took 12.009s

Epoch 12 of 100
  training loss:		3.016768E-02
  validation loss:		3.028920E-02
Epoch took 11.688s

Epoch 13 of 100
  training loss:		2.945636E-02
  validation loss:		2.957353E-02
Epoch took 12.492s

Epoch 14 of 100
  training loss:		2.922736E-02
  validation loss:		2.897007E-02
Epoch took 12.827s

Epoch 15 of 100
  training loss:		2.899167E-02
  validation loss:		2.955479E-02
Epoch took 12.558s

Epoch 16 of 100
  training loss:		2.878210E-02
  validation loss:		2.899439E-02
Epoch took 12.086s

Epoch 17 of 100
  training loss:		2.857004E-02
  validation loss:		2.834097E-02
Epoch took 12.937s

Epoch 18 of 100
  training loss:		2.814749E-02
  validation loss:		2.891484E-02
Epoch took 12.151s

Epoch 19 of 100
  training loss:		2.801887E-02
  validation loss:		2.809595E-02
Epoch took 12.445s

Epoch 20 of 100
  training loss:		2.783030E-02
  validation loss:		2.873439E-02
Epoch took 12.306s

Epoch 21 of 100
  training loss:		2.782151E-02
  validation loss:		2.837168E-02
Epoch took 11.793s

Epoch 22 of 100
  training loss:		2.752467E-02
  validation loss:		2.848605E-02
Epoch took 13.606s

Epoch 23 of 100
  training loss:		2.753208E-02
  validation loss:		2.784512E-02
Epoch took 11.664s

Epoch 24 of 100
  training loss:		2.749800E-02
  validation loss:		2.755325E-02
Epoch took 11.812s

Epoch 25 of 100
  training loss:		2.769258E-02
  validation loss:		2.863056E-02
Epoch took 12.665s

Epoch 26 of 100
  training loss:		2.715594E-02
  validation loss:		2.737164E-02
Epoch took 11.592s

Epoch 27 of 100
  training loss:		2.759580E-02
  validation loss:		2.696501E-02
Epoch took 12.784s

Epoch 28 of 100
  training loss:		2.722510E-02
  validation loss:		2.712132E-02
Epoch took 12.885s

Epoch 29 of 100
  training loss:		2.707451E-02
  validation loss:		2.685510E-02
Epoch took 13.027s

Epoch 30 of 100
  training loss:		2.723178E-02
  validation loss:		2.717055E-02
Epoch took 12.482s

Epoch 31 of 100
  training loss:		2.708070E-02
  validation loss:		2.695636E-02
Epoch took 13.414s

Epoch 32 of 100
  training loss:		2.693908E-02
  validation loss:		2.720479E-02
Epoch took 12.506s

Epoch 33 of 100
  training loss:		2.700333E-02
  validation loss:		2.672481E-02
Epoch took 12.594s

Epoch 34 of 100
  training loss:		2.702254E-02
  validation loss:		2.719216E-02
Epoch took 13.826s

Epoch 35 of 100
  training loss:		2.693499E-02
  validation loss:		2.916396E-02
Epoch took 12.605s

Early stopping, val-loss increased over the last 5 epochs from 0.0270967234627 to 0.0274484145652
Training RMSE: 1.60328745652e-07
Validation RMSE: 1.61601421232e-07
