Training network with 75120 training samples and 14085 validation samples
Epoch 1 of 100
  training loss:		4.762386E-03
  validation loss:		3.946972E-04
Epoch took 9.362s

Epoch 2 of 100
  training loss:		1.892886E-04
  validation loss:		8.851681E-05
Epoch took 9.709s

Epoch 3 of 100
  training loss:		4.968197E-05
  validation loss:		3.336881E-05
Epoch took 9.121s

Epoch 4 of 100
  training loss:		1.948278E-05
  validation loss:		1.291746E-05
Epoch took 10.816s

Epoch 5 of 100
  training loss:		9.115927E-06
  validation loss:		6.920765E-06
Epoch took 10.067s

Epoch 6 of 100
  training loss:		8.127636E-06
  validation loss:		1.276744E-05
Epoch took 8.014s

Epoch 7 of 100
  training loss:		6.936973E-06
  validation loss:		3.052384E-06
Epoch took 9.838s

Epoch 8 of 100
  training loss:		5.674585E-06
  validation loss:		2.210694E-06
Epoch took 10.316s

Epoch 9 of 100
  training loss:		4.812228E-05
  validation loss:		3.506054E-05
Epoch took 9.501s

Epoch 10 of 100
  training loss:		1.232377E-05
  validation loss:		3.228266E-06
Epoch took 10.194s

Epoch 11 of 100
  training loss:		8.023842E-05
  validation loss:		2.170299E-06
Epoch took 9.292s

Epoch 12 of 100
  training loss:		2.179364E-06
  validation loss:		1.004737E-06
Epoch took 9.369s

Epoch 13 of 100
  training loss:		1.571476E-05
  validation loss:		6.357118E-05
Epoch took 9.110s

Epoch 14 of 100
  training loss:		6.382577E-05
  validation loss:		2.805418E-06
Epoch took 10.473s

Epoch 15 of 100
  training loss:		3.866389E-05
  validation loss:		1.674674E-06
Epoch took 9.440s

Early stopping, val-loss increased over the last 5 epochs from 1.1263864646e-05 to 1.42452621999e-05
Training RMSE: 0.00172857441548
Validation RMSE: 0.00179527116327
