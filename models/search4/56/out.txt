Training network with 75120 training samples and 14085 validation samples
Epoch 1 of 100
  training loss:		6.502756E-02
  validation loss:		4.992346E-02
Epoch took 13.327s

Epoch 2 of 100
  training loss:		3.951156E-02
  validation loss:		3.816790E-02
Epoch took 15.030s

Epoch 3 of 100
  training loss:		3.644866E-02
  validation loss:		3.317760E-02
Epoch took 14.158s

Epoch 4 of 100
  training loss:		3.267389E-02
  validation loss:		3.115442E-02
Epoch took 13.924s

Epoch 5 of 100
  training loss:		3.262285E-02
  validation loss:		3.141083E-02
Epoch took 13.611s

Epoch 6 of 100
  training loss:		3.152436E-02
  validation loss:		3.057628E-02
Epoch took 15.390s

Epoch 7 of 100
  training loss:		3.004852E-02
  validation loss:		3.135363E-02
Epoch took 15.166s

Epoch 8 of 100
  training loss:		3.002356E-02
  validation loss:		3.031442E-02
Epoch took 13.859s

Epoch 9 of 100
  training loss:		2.968921E-02
  validation loss:		3.114542E-02
Epoch took 14.468s

Epoch 10 of 100
  training loss:		2.985347E-02
  validation loss:		2.865766E-02
Epoch took 14.381s

Epoch 11 of 100
  training loss:		2.907228E-02
  validation loss:		2.864147E-02
Epoch took 12.691s

Epoch 12 of 100
  training loss:		2.889363E-02
  validation loss:		2.893799E-02
Epoch took 13.571s

Epoch 13 of 100
  training loss:		2.910657E-02
  validation loss:		3.047431E-02
Epoch took 15.397s

Epoch 14 of 100
  training loss:		2.826433E-02
  validation loss:		2.899276E-02
Epoch took 13.930s

Epoch 15 of 100
  training loss:		2.840938E-02
  validation loss:		2.890411E-02
Epoch took 13.996s

Epoch 16 of 100
  training loss:		2.806225E-02
  validation loss:		3.047726E-02
Epoch took 12.862s

Epoch 17 of 100
  training loss:		2.805070E-02
  validation loss:		2.904902E-02
Epoch took 13.627s

Epoch 18 of 100
  training loss:		2.795210E-02
  validation loss:		2.863367E-02
Epoch took 14.199s

Epoch 19 of 100
  training loss:		2.792866E-02
  validation loss:		2.852493E-02
Epoch took 14.265s

Epoch 20 of 100
  training loss:		2.796786E-02
  validation loss:		2.900721E-02
Epoch took 14.679s

Epoch 21 of 100
  training loss:		2.774416E-02
  validation loss:		2.728030E-02
Epoch took 14.465s

Epoch 22 of 100
  training loss:		2.759921E-02
  validation loss:		2.668473E-02
Epoch took 13.954s

Epoch 23 of 100
  training loss:		2.758295E-02
  validation loss:		2.685358E-02
Epoch took 14.495s

Epoch 24 of 100
  training loss:		2.750804E-02
  validation loss:		2.704571E-02
Epoch took 13.156s

Epoch 25 of 100
  training loss:		2.744099E-02
  validation loss:		2.710777E-02
Epoch took 14.950s

Epoch 26 of 100
  training loss:		2.764406E-02
  validation loss:		2.877790E-02
Epoch took 12.954s

Epoch 27 of 100
  training loss:		2.731213E-02
  validation loss:		2.922038E-02
Epoch took 15.836s

Epoch 28 of 100
  training loss:		2.728553E-02
  validation loss:		2.701027E-02
Epoch took 14.126s

Epoch 29 of 100
  training loss:		2.727088E-02
  validation loss:		2.764037E-02
Epoch took 14.579s

Epoch 30 of 100
  training loss:		2.705182E-02
  validation loss:		2.790815E-02
Epoch took 14.195s

Early stopping, val-loss increased over the last 5 epochs from 0.0269944159416 to 0.0281114147259
Training RMSE: 1.60275353267e-07
Validation RMSE: 1.61320976671e-07
