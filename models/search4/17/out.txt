Training network with 75120 training samples and 14085 validation samples
Epoch 1 of 100
  training loss:		7.375279E-01
  validation loss:		2.917712E-02
Epoch took 9.884s

Epoch 2 of 100
  training loss:		1.968945E-02
  validation loss:		1.298364E-02
Epoch took 9.481s

Epoch 3 of 100
  training loss:		9.883352E-03
  validation loss:		7.380449E-03
Epoch took 10.326s

Epoch 4 of 100
  training loss:		6.072640E-03
  validation loss:		4.878245E-03
Epoch took 10.495s

Epoch 5 of 100
  training loss:		4.094564E-03
  validation loss:		3.348097E-03
Epoch took 10.247s

Epoch 6 of 100
  training loss:		2.807673E-03
  validation loss:		2.361887E-03
Epoch took 9.609s

Epoch 7 of 100
  training loss:		2.007032E-03
  validation loss:		1.708561E-03
Epoch took 10.069s

Epoch 8 of 100
  training loss:		1.452631E-03
  validation loss:		1.260753E-03
Epoch took 9.358s

Epoch 9 of 100
  training loss:		1.092677E-03
  validation loss:		9.672831E-04
Epoch took 10.276s

Epoch 10 of 100
  training loss:		8.437151E-04
  validation loss:		7.515195E-04
Epoch took 10.177s

Epoch 11 of 100
  training loss:		6.562713E-04
  validation loss:		5.902390E-04
Epoch took 10.968s

Epoch 12 of 100
  training loss:		5.139941E-04
  validation loss:		4.609315E-04
Epoch took 9.739s

Epoch 13 of 100
  training loss:		4.001744E-04
  validation loss:		3.651767E-04
Epoch took 10.944s

Epoch 14 of 100
  training loss:		3.174171E-04
  validation loss:		2.906213E-04
Epoch took 10.643s

Epoch 15 of 100
  training loss:		2.518837E-04
  validation loss:		2.330294E-04
Epoch took 10.138s

Epoch 16 of 100
  training loss:		2.013782E-04
  validation loss:		1.886950E-04
Epoch took 10.324s

Epoch 17 of 100
  training loss:		1.621000E-04
  validation loss:		1.580351E-04
Epoch took 9.630s

Epoch 18 of 100
  training loss:		1.308423E-04
  validation loss:		1.244728E-04
Epoch took 9.904s

Epoch 19 of 100
  training loss:		1.044723E-04
  validation loss:		9.901725E-05
Epoch took 11.710s

Epoch 20 of 100
  training loss:		8.442967E-05
  validation loss:		8.369335E-05
Epoch took 10.167s

Epoch 21 of 100
  training loss:		6.892639E-05
  validation loss:		6.649763E-05
Epoch took 10.812s

Epoch 22 of 100
  training loss:		5.648228E-05
  validation loss:		5.452335E-05
Epoch took 9.952s

Epoch 23 of 100
  training loss:		4.546814E-05
  validation loss:		4.506499E-05
Epoch took 10.167s

Epoch 24 of 100
  training loss:		3.754777E-05
  validation loss:		3.654327E-05
Epoch took 10.104s

Epoch 25 of 100
  training loss:		3.049967E-05
  validation loss:		3.057629E-05
Epoch took 10.436s

Epoch 26 of 100
  training loss:		2.519095E-05
  validation loss:		2.524374E-05
Epoch took 10.759s

Epoch 27 of 100
  training loss:		2.075386E-05
  validation loss:		2.036632E-05
Epoch took 10.747s

Epoch 28 of 100
  training loss:		1.717876E-05
  validation loss:		1.936480E-05
Epoch took 10.317s

Epoch 29 of 100
  training loss:		1.358961E-05
  validation loss:		1.319976E-05
Epoch took 10.767s

Epoch 30 of 100
  training loss:		1.108118E-05
  validation loss:		1.053335E-05
Epoch took 10.044s

Epoch 31 of 100
  training loss:		8.497383E-06
  validation loss:		9.467014E-06
Epoch took 9.761s

Epoch 32 of 100
  training loss:		6.956597E-06
  validation loss:		8.049303E-06
Epoch took 11.073s

Epoch 33 of 100
  training loss:		5.814892E-06
  validation loss:		6.135119E-06
Epoch took 10.284s

Epoch 34 of 100
  training loss:		4.561438E-06
  validation loss:		6.063641E-06
Epoch took 9.995s

Epoch 35 of 100
  training loss:		3.972344E-06
  validation loss:		3.939457E-06
Epoch took 9.639s

Epoch 36 of 100
  training loss:		3.354420E-06
  validation loss:		3.422945E-06
Epoch took 11.495s

Epoch 37 of 100
  training loss:		2.657448E-06
  validation loss:		2.304722E-06
Epoch took 10.934s

Epoch 38 of 100
  training loss:		2.020115E-06
  validation loss:		1.641984E-06
Epoch took 10.110s

Epoch 39 of 100
  training loss:		1.830628E-06
  validation loss:		1.580945E-06
Epoch took 9.421s

Epoch 40 of 100
  training loss:		1.630390E-06
  validation loss:		2.154105E-06
Epoch took 9.140s

Epoch 41 of 100
  training loss:		1.950280E-06
  validation loss:		8.951078E-07
Epoch took 9.580s

Epoch 42 of 100
  training loss:		4.011748E-06
  validation loss:		9.195927E-07
Epoch took 10.261s

Epoch 43 of 100
  training loss:		7.495237E-07
  validation loss:		6.423530E-07
Epoch took 9.447s

Epoch 44 of 100
  training loss:		1.906221E-06
  validation loss:		4.952489E-07
Epoch took 11.037s

Epoch 45 of 100
  training loss:		4.616756E-06
  validation loss:		3.310223E-06
Epoch took 10.670s

Epoch 46 of 100
  training loss:		5.465051E-07
  validation loss:		4.122872E-07
Epoch took 9.963s

Epoch 47 of 100
  training loss:		1.249576E-06
  validation loss:		3.034132E-07
Epoch took 10.442s

Epoch 48 of 100
  training loss:		1.948823E-06
  validation loss:		6.061813E-07
Epoch took 10.863s

Epoch 49 of 100
  training loss:		1.328134E-06
  validation loss:		2.276737E-07
Epoch took 8.904s

Epoch 50 of 100
  training loss:		5.397039E-06
  validation loss:		1.471126E-05
Epoch took 9.210s

Early stopping, val-loss increased over the last 5 epochs from 1.2525050642e-06 to 3.25216393805e-06
Training RMSE: 0.00179384854881
Validation RMSE: 0.00181936745533
