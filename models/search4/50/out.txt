Training network with 75120 training samples and 14085 validation samples
Epoch 1 of 100
  training loss:		8.758734E-02
  validation loss:		6.345445E-02
Epoch took 17.103s

Epoch 2 of 100
  training loss:		5.631882E-02
  validation loss:		5.342493E-02
Epoch took 14.981s

Epoch 3 of 100
  training loss:		4.784807E-02
  validation loss:		4.591881E-02
Epoch took 16.626s

Epoch 4 of 100
  training loss:		4.303175E-02
  validation loss:		4.199722E-02
Epoch took 15.748s

Epoch 5 of 100
  training loss:		4.001845E-02
  validation loss:		4.229362E-02
Epoch took 16.946s

Epoch 6 of 100
  training loss:		3.779953E-02
  validation loss:		3.681864E-02
Epoch took 17.173s

Epoch 7 of 100
  training loss:		3.627886E-02
  validation loss:		3.584580E-02
Epoch took 16.194s

Epoch 8 of 100
  training loss:		3.492798E-02
  validation loss:		3.453079E-02
Epoch took 17.073s

Epoch 9 of 100
  training loss:		3.392766E-02
  validation loss:		3.350472E-02
Epoch took 15.534s

Epoch 10 of 100
  training loss:		3.296005E-02
  validation loss:		3.239020E-02
Epoch took 16.631s

Epoch 11 of 100
  training loss:		3.240684E-02
  validation loss:		3.302754E-02
Epoch took 15.806s

Epoch 12 of 100
  training loss:		3.173481E-02
  validation loss:		3.244200E-02
Epoch took 17.025s

Epoch 13 of 100
  training loss:		3.113109E-02
  validation loss:		3.109775E-02
Epoch took 15.735s

Epoch 14 of 100
  training loss:		3.070008E-02
  validation loss:		3.082269E-02
Epoch took 15.950s

Epoch 15 of 100
  training loss:		3.052172E-02
  validation loss:		3.081675E-02
Epoch took 16.039s

Epoch 16 of 100
  training loss:		2.993688E-02
  validation loss:		3.192802E-02
Epoch took 16.840s

Epoch 17 of 100
  training loss:		2.967455E-02
  validation loss:		3.015973E-02
Epoch took 16.368s

Epoch 18 of 100
  training loss:		2.927593E-02
  validation loss:		2.918537E-02
Epoch took 17.107s

Epoch 19 of 100
  training loss:		2.906363E-02
  validation loss:		2.887696E-02
Epoch took 17.270s

Epoch 20 of 100
  training loss:		2.885872E-02
  validation loss:		2.980143E-02
Epoch took 18.329s

Epoch 21 of 100
  training loss:		2.861086E-02
  validation loss:		2.917481E-02
Epoch took 16.837s

Epoch 22 of 100
  training loss:		2.842770E-02
  validation loss:		2.848393E-02
Epoch took 15.955s

Epoch 23 of 100
  training loss:		2.845623E-02
  validation loss:		2.841682E-02
Epoch took 16.665s

Epoch 24 of 100
  training loss:		2.820682E-02
  validation loss:		2.807159E-02
Epoch took 15.925s

Epoch 25 of 100
  training loss:		2.807497E-02
  validation loss:		2.804138E-02
Epoch took 16.851s

Epoch 26 of 100
  training loss:		2.785536E-02
  validation loss:		2.791021E-02
Epoch took 16.929s

Epoch 27 of 100
  training loss:		2.772414E-02
  validation loss:		2.797376E-02
Epoch took 16.458s

Epoch 28 of 100
  training loss:		2.772001E-02
  validation loss:		2.779520E-02
Epoch took 16.885s

Epoch 29 of 100
  training loss:		2.755789E-02
  validation loss:		2.927515E-02
Epoch took 16.748s

Epoch 30 of 100
  training loss:		2.746760E-02
  validation loss:		2.764526E-02
Epoch took 16.247s

Epoch 31 of 100
  training loss:		2.752157E-02
  validation loss:		2.734137E-02
Epoch took 14.796s

Epoch 32 of 100
  training loss:		2.729863E-02
  validation loss:		2.832009E-02
Epoch took 16.980s

Epoch 33 of 100
  training loss:		2.733454E-02
  validation loss:		2.703910E-02
Epoch took 16.088s

Epoch 34 of 100
  training loss:		2.712576E-02
  validation loss:		2.751168E-02
Epoch took 15.984s

Epoch 35 of 100
  training loss:		2.720089E-02
  validation loss:		2.743062E-02
Epoch took 16.107s

Epoch 36 of 100
  training loss:		2.702121E-02
  validation loss:		2.716819E-02
Epoch took 17.640s

Epoch 37 of 100
  training loss:		2.700916E-02
  validation loss:		2.743361E-02
Epoch took 16.587s

Epoch 38 of 100
  training loss:		2.711466E-02
  validation loss:		2.713233E-02
Epoch took 17.209s

Epoch 39 of 100
  training loss:		2.708805E-02
  validation loss:		2.702075E-02
Epoch took 16.234s

Epoch 40 of 100
  training loss:		2.691336E-02
  validation loss:		2.689855E-02
Epoch took 16.322s

Epoch 41 of 100
  training loss:		2.703233E-02
  validation loss:		2.753974E-02
Epoch took 14.921s

Epoch 42 of 100
  training loss:		2.693128E-02
  validation loss:		2.680337E-02
Epoch took 17.231s

Epoch 43 of 100
  training loss:		2.690231E-02
  validation loss:		2.731717E-02
Epoch took 16.414s

Epoch 44 of 100
  training loss:		2.678070E-02
  validation loss:		2.672697E-02
Epoch took 16.009s

Epoch 45 of 100
  training loss:		2.695460E-02
  validation loss:		2.679417E-02
Epoch took 15.885s

Epoch 46 of 100
  training loss:		2.685954E-02
  validation loss:		2.723738E-02
Epoch took 14.803s

Epoch 47 of 100
  training loss:		2.672883E-02
  validation loss:		2.714438E-02
Epoch took 17.132s

Epoch 48 of 100
  training loss:		2.686929E-02
  validation loss:		2.708771E-02
Epoch took 15.593s

Epoch 49 of 100
  training loss:		2.670557E-02
  validation loss:		2.766964E-02
Epoch took 16.713s

Epoch 50 of 100
  training loss:		2.686195E-02
  validation loss:		2.731643E-02
Epoch took 17.117s

Early stopping, val-loss increased over the last 5 epochs from 0.0270362822555 to 0.0272911094991
Training RMSE: 1.59493722435e-07
Validation RMSE: 1.60400725296e-07
