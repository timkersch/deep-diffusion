Training network with 75120 training samples and 14085 validation samples
Epoch 1 of 100
  training loss:		1.294074E-02
  validation loss:		4.759901E-04
Epoch took 8.660s

Epoch 2 of 100
  training loss:		2.218332E-04
  validation loss:		1.035501E-04
Epoch took 9.784s

Epoch 3 of 100
  training loss:		5.725396E-05
  validation loss:		3.194224E-05
Epoch took 9.682s

Epoch 4 of 100
  training loss:		2.008063E-05
  validation loss:		1.355524E-05
Epoch took 9.594s

Epoch 5 of 100
  training loss:		9.174602E-06
  validation loss:		6.825021E-06
Epoch took 8.257s

Epoch 6 of 100
  training loss:		4.532753E-06
  validation loss:		3.590129E-06
Epoch took 8.863s

Epoch 7 of 100
  training loss:		2.619374E-06
  validation loss:		2.683371E-06
Epoch took 10.313s

Epoch 8 of 100
  training loss:		1.535768E-06
  validation loss:		1.393267E-06
Epoch took 8.319s

Epoch 9 of 100
  training loss:		8.513437E-07
  validation loss:		6.591696E-07
Epoch took 9.808s

Epoch 10 of 100
  training loss:		5.934301E-07
  validation loss:		4.507826E-07
Epoch took 10.393s

Epoch 11 of 100
  training loss:		3.535477E-07
  validation loss:		2.693096E-07
Epoch took 9.744s

Epoch 12 of 100
  training loss:		8.029991E-07
  validation loss:		1.642162E-07
Epoch took 10.048s

Epoch 13 of 100
  training loss:		1.214751E-05
  validation loss:		2.145565E-04
Epoch took 9.813s

Epoch 14 of 100
  training loss:		1.020112E-04
  validation loss:		1.104585E-06
Epoch took 9.216s

Epoch 15 of 100
  training loss:		2.854047E-05
  validation loss:		4.934047E-05
Epoch took 9.358s

Early stopping, val-loss increased over the last 5 epochs from 1.75534391946e-06 to 5.30870244314e-05
Training RMSE: 0.000598521452584
Validation RMSE: 0.000670096359451
