Training network with 75120 training samples and 14085 validation samples
Epoch 1 of 100
  training loss:		1.217932E-02
  validation loss:		1.290784E-03
Epoch took 10.555s

Epoch 2 of 100
  training loss:		6.547941E-04
  validation loss:		3.184204E-04
Epoch took 11.747s

Epoch 3 of 100
  training loss:		1.828612E-04
  validation loss:		1.001417E-04
Epoch took 13.167s

Epoch 4 of 100
  training loss:		5.869898E-05
  validation loss:		3.673139E-05
Epoch took 11.316s

Epoch 5 of 100
  training loss:		2.296906E-05
  validation loss:		1.385929E-05
Epoch took 11.398s

Epoch 6 of 100
  training loss:		1.215103E-05
  validation loss:		6.478446E-06
Epoch took 12.084s

Epoch 7 of 100
  training loss:		7.148027E-06
  validation loss:		7.770330E-06
Epoch took 10.759s

Epoch 8 of 100
  training loss:		4.492321E-06
  validation loss:		2.949115E-06
Epoch took 11.862s

Epoch 9 of 100
  training loss:		3.957927E-06
  validation loss:		3.403585E-06
Epoch took 10.705s

Epoch 10 of 100
  training loss:		6.996171E-06
  validation loss:		2.743637E-06
Epoch took 11.292s

Epoch 11 of 100
  training loss:		4.355462E-06
  validation loss:		7.461546E-06
Epoch took 11.825s

Epoch 12 of 100
  training loss:		4.981888E-06
  validation loss:		8.269595E-07
Epoch took 11.878s

Epoch 13 of 100
  training loss:		1.758375E-05
  validation loss:		1.970822E-07
Epoch took 11.408s

Epoch 14 of 100
  training loss:		7.980010E-07
  validation loss:		1.737965E-05
Epoch took 11.668s

Epoch 15 of 100
  training loss:		1.156550E-05
  validation loss:		1.831729E-06
Epoch took 10.977s

Epoch 16 of 100
  training loss:		7.537647E-06
  validation loss:		5.017973E-06
Epoch took 11.940s

Epoch 17 of 100
  training loss:		5.687592E-06
  validation loss:		2.212756E-05
Epoch took 11.313s

Epoch 18 of 100
  training loss:		3.230066E-06
  validation loss:		1.146124E-06
Epoch took 10.847s

Epoch 19 of 100
  training loss:		1.165517E-05
  validation loss:		3.800207E-07
Epoch took 10.924s

Epoch 20 of 100
  training loss:		5.933296E-06
  validation loss:		8.747296E-07
Epoch took 12.498s

Epoch 21 of 100
  training loss:		1.153943E-05
  validation loss:		2.967083E-06
Epoch took 12.610s

Epoch 22 of 100
  training loss:		2.881990E-07
  validation loss:		5.428906E-07
Epoch took 12.097s

Epoch 23 of 100
  training loss:		1.199403E-05
  validation loss:		4.595210E-07
Epoch took 11.969s

Epoch 24 of 100
  training loss:		6.309567E-06
  validation loss:		9.543033E-06
Epoch took 12.296s

Epoch 25 of 100
  training loss:		1.498624E-06
  validation loss:		6.232526E-07
Epoch took 11.382s

Epoch 26 of 100
  training loss:		2.271266E-05
  validation loss:		1.437119E-07
Epoch took 11.632s

Epoch 27 of 100
  training loss:		1.344557E-07
  validation loss:		8.794522E-08
Epoch took 11.409s

Epoch 28 of 100
  training loss:		1.223574E-05
  validation loss:		1.273285E-07
Epoch took 11.255s

Epoch 29 of 100
  training loss:		4.106549E-07
  validation loss:		7.318831E-06
Epoch took 11.579s

Epoch 30 of 100
  training loss:		8.496625E-06
  validation loss:		2.899354E-06
Epoch took 12.486s

Epoch 31 of 100
  training loss:		1.012974E-05
  validation loss:		4.259233E-08
Epoch took 12.018s

Epoch 32 of 100
  training loss:		7.938597E-07
  validation loss:		2.277204E-05
Epoch took 11.444s

Epoch 33 of 100
  training loss:		1.215976E-05
  validation loss:		1.606674E-08
Epoch took 11.405s

Epoch 34 of 100
  training loss:		2.956347E-06
  validation loss:		5.702406E-07
Epoch took 10.932s

Epoch 35 of 100
  training loss:		1.667935E-05
  validation loss:		1.046517E-07
Epoch took 11.898s

Epoch 36 of 100
  training loss:		2.160337E-07
  validation loss:		1.778365E-07
Epoch took 11.516s

Epoch 37 of 100
  training loss:		1.234654E-05
  validation loss:		9.335302E-07
Epoch took 10.773s

Epoch 38 of 100
  training loss:		3.728879E-07
  validation loss:		2.287445E-05
Epoch took 11.100s

Epoch 39 of 100
  training loss:		7.644568E-06
  validation loss:		2.792869E-07
Epoch took 11.613s

Epoch 40 of 100
  training loss:		8.336126E-06
  validation loss:		4.664912E-08
Epoch took 10.750s

Early stopping, val-loss increased over the last 10 epochs from 2.47129505821e-06 to 4.78173494859e-06
Training RMSE: 0.00169216960142
Validation RMSE: 0.00170255047216
