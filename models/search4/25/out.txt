Training network with 75120 training samples and 14085 validation samples
Epoch 1 of 100
  training loss:		4.008644E-02
  validation loss:		6.286764E-03
Epoch took 12.506s

Epoch 2 of 100
  training loss:		3.862461E-03
  validation loss:		2.587275E-03
Epoch took 11.269s

Epoch 3 of 100
  training loss:		1.754149E-03
  validation loss:		1.282464E-03
Epoch took 11.912s

Epoch 4 of 100
  training loss:		9.241720E-04
  validation loss:		6.991139E-04
Epoch took 11.282s

Epoch 5 of 100
  training loss:		5.174612E-04
  validation loss:		3.896191E-04
Epoch took 11.415s

Epoch 6 of 100
  training loss:		2.933913E-04
  validation loss:		2.226765E-04
Epoch took 12.679s

Epoch 7 of 100
  training loss:		1.739301E-04
  validation loss:		1.347791E-04
Epoch took 11.702s

Epoch 8 of 100
  training loss:		1.078355E-04
  validation loss:		9.311078E-05
Epoch took 10.912s

Epoch 9 of 100
  training loss:		6.646140E-05
  validation loss:		5.449144E-05
Epoch took 12.351s

Epoch 10 of 100
  training loss:		4.637393E-05
  validation loss:		3.439023E-05
Epoch took 11.766s

Epoch 11 of 100
  training loss:		2.991833E-05
  validation loss:		2.221760E-05
Epoch took 11.066s

Epoch 12 of 100
  training loss:		1.974087E-05
  validation loss:		1.560436E-05
Epoch took 11.282s

Epoch 13 of 100
  training loss:		1.545194E-05
  validation loss:		1.237582E-05
Epoch took 11.696s

Epoch 14 of 100
  training loss:		1.141141E-05
  validation loss:		7.357277E-06
Epoch took 11.938s

Epoch 15 of 100
  training loss:		8.723018E-06
  validation loss:		8.430351E-06
Epoch took 12.348s

Epoch 16 of 100
  training loss:		5.619837E-06
  validation loss:		2.801335E-06
Epoch took 10.615s

Epoch 17 of 100
  training loss:		4.807109E-06
  validation loss:		4.310483E-06
Epoch took 12.323s

Epoch 18 of 100
  training loss:		4.997249E-06
  validation loss:		1.745818E-06
Epoch took 11.844s

Epoch 19 of 100
  training loss:		3.658383E-06
  validation loss:		3.435600E-06
Epoch took 11.937s

Epoch 20 of 100
  training loss:		5.305512E-06
  validation loss:		7.050519E-05
Epoch took 12.807s

Epoch 21 of 100
  training loss:		4.525831E-06
  validation loss:		5.952205E-06
Epoch took 10.958s

Epoch 22 of 100
  training loss:		1.776961E-06
  validation loss:		3.299101E-07
Epoch took 10.520s

Epoch 23 of 100
  training loss:		2.633626E-06
  validation loss:		3.551369E-07
Epoch took 10.901s

Epoch 24 of 100
  training loss:		4.080291E-06
  validation loss:		5.927085E-08
Epoch took 11.160s

Epoch 25 of 100
  training loss:		3.034651E-06
  validation loss:		1.656201E-06
Epoch took 11.580s

Epoch 26 of 100
  training loss:		4.392058E-06
  validation loss:		4.977658E-07
Epoch took 11.910s

Epoch 27 of 100
  training loss:		4.951741E-07
  validation loss:		4.167497E-07
Epoch took 12.105s

Epoch 28 of 100
  training loss:		6.647866E-06
  validation loss:		4.005966E-08
Epoch took 10.655s

Epoch 29 of 100
  training loss:		3.930948E-07
  validation loss:		5.568423E-07
Epoch took 10.904s

Epoch 30 of 100
  training loss:		4.395564E-06
  validation loss:		3.769617E-06
Epoch took 11.545s

Epoch 31 of 100
  training loss:		4.998808E-06
  validation loss:		3.868124E-08
Epoch took 11.503s

Epoch 32 of 100
  training loss:		2.591355E-06
  validation loss:		6.837534E-06
Epoch took 10.940s

Epoch 33 of 100
  training loss:		3.848522E-06
  validation loss:		4.822134E-07
Epoch took 12.299s

Epoch 34 of 100
  training loss:		7.396847E-07
  validation loss:		4.528097E-07
Epoch took 12.238s

Epoch 35 of 100
  training loss:		4.885421E-06
  validation loss:		4.019513E-08
Epoch took 12.200s

Epoch 36 of 100
  training loss:		4.678273E-06
  validation loss:		1.026130E-05
Epoch took 11.786s

Epoch 37 of 100
  training loss:		1.172204E-06
  validation loss:		5.023730E-08
Epoch took 12.993s

Epoch 38 of 100
  training loss:		7.302951E-06
  validation loss:		1.978542E-08
Epoch took 11.920s

Epoch 39 of 100
  training loss:		1.104280E-07
  validation loss:		9.224156E-07
Epoch took 12.433s

Epoch 40 of 100
  training loss:		4.451805E-06
  validation loss:		2.364195E-07
Epoch took 12.266s

Early stopping, val-loss increased over the last 10 epochs from 1.36337584154e-06 to 1.93415876537e-06
Training RMSE: 0.00195740639486
Validation RMSE: 0.00194202179016
