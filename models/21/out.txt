Epoch 1 of 200
  training loss:		9.095007E-02
  validation loss:		5.297077E-02

Epoch 2 of 200
  training loss:		3.257615E-02
  validation loss:		1.648699E-02

Epoch 3 of 200
  training loss:		9.714456E-03
  validation loss:		5.523661E-03

Epoch 4 of 200
  training loss:		4.593587E-03
  validation loss:		3.895384E-03

Epoch 5 of 200
  training loss:		3.863912E-03
  validation loss:		3.618875E-03

Epoch 6 of 200
  training loss:		3.622673E-03
  validation loss:		3.481050E-03

Epoch 7 of 200
  training loss:		3.374466E-03
  validation loss:		3.106982E-03

Epoch 8 of 200
  training loss:		3.027100E-03
  validation loss:		2.746798E-03

Epoch 9 of 200
  training loss:		2.651512E-03
  validation loss:		2.403258E-03

Epoch 10 of 200
  training loss:		2.329190E-03
  validation loss:		2.161813E-03

Epoch 11 of 200
  training loss:		2.109921E-03
  validation loss:		1.948998E-03

Epoch 12 of 200
  training loss:		1.921226E-03
  validation loss:		1.837501E-03

Epoch 13 of 200
  training loss:		1.754607E-03
  validation loss:		1.595190E-03

Epoch 14 of 200
  training loss:		1.493898E-03
  validation loss:		1.283120E-03

Epoch 15 of 200
  training loss:		1.149999E-03
  validation loss:		9.858468E-04

Epoch 16 of 200
  training loss:		9.030267E-04
  validation loss:		7.865249E-04

Epoch 17 of 200
  training loss:		7.597222E-04
  validation loss:		6.579044E-04

Epoch 18 of 200
  training loss:		6.319970E-04
  validation loss:		5.681637E-04

Epoch 19 of 200
  training loss:		5.438857E-04
  validation loss:		4.899369E-04

Epoch 20 of 200
  training loss:		4.660157E-04
  validation loss:		4.289182E-04

Epoch 21 of 200
  training loss:		4.135952E-04
  validation loss:		4.097036E-04

Epoch 22 of 200
  training loss:		3.733239E-04
  validation loss:		3.645661E-04

Epoch 23 of 200
  training loss:		3.277903E-04
  validation loss:		3.067683E-04

Epoch 24 of 200
  training loss:		3.060114E-04
  validation loss:		2.758431E-04

Epoch 25 of 200
  training loss:		2.770795E-04
  validation loss:		2.881030E-04

Epoch 26 of 200
  training loss:		2.554717E-04
  validation loss:		2.575626E-04

Epoch 27 of 200
  training loss:		2.419645E-04
  validation loss:		2.224452E-04

Epoch 28 of 200
  training loss:		2.278443E-04
  validation loss:		2.274490E-04

Epoch 29 of 200
  training loss:		2.164901E-04
  validation loss:		2.088487E-04

Epoch 30 of 200
  training loss:		1.986683E-04
  validation loss:		1.912415E-04

Epoch 31 of 200
  training loss:		1.958933E-04
  validation loss:		1.868988E-04

Epoch 32 of 200
  training loss:		1.940982E-04
  validation loss:		1.940258E-04

Epoch 33 of 200
  training loss:		1.830421E-04
  validation loss:		1.701699E-04

Epoch 34 of 200
  training loss:		1.840396E-04
  validation loss:		1.700853E-04

Epoch 35 of 200
  training loss:		1.720162E-04
  validation loss:		1.581646E-04

Epoch 36 of 200
  training loss:		1.650279E-04
  validation loss:		1.620508E-04

Epoch 37 of 200
  training loss:		1.561396E-04
  validation loss:		1.494751E-04

Epoch 38 of 200
  training loss:		1.555198E-04
  validation loss:		1.542473E-04

Epoch 39 of 200
  training loss:		1.498738E-04
  validation loss:		1.386318E-04

Epoch 40 of 200
  training loss:		1.501528E-04
  validation loss:		1.384368E-04

Epoch 41 of 200
  training loss:		1.484526E-04
  validation loss:		1.323875E-04

Epoch 42 of 200
  training loss:		1.379623E-04
  validation loss:		1.289372E-04

Epoch 43 of 200
  training loss:		1.393693E-04
  validation loss:		1.239285E-04

Epoch 44 of 200
  training loss:		1.263318E-04
  validation loss:		1.235553E-04

Epoch 45 of 200
  training loss:		1.222654E-04
  validation loss:		1.186786E-04

Epoch 46 of 200
  training loss:		1.224543E-04
  validation loss:		1.272631E-04

Epoch 47 of 200
  training loss:		1.167762E-04
  validation loss:		1.131677E-04

Epoch 48 of 200
  training loss:		1.165774E-04
  validation loss:		1.052041E-04

Epoch 49 of 200
  training loss:		1.141178E-04
  validation loss:		1.015151E-04

Epoch 50 of 200
  training loss:		1.049613E-04
  validation loss:		1.062664E-04

Epoch 51 of 200
  training loss:		1.064097E-04
  validation loss:		9.553038E-05

Epoch 52 of 200
  training loss:		9.936209E-05
  validation loss:		9.943405E-05

Epoch 53 of 200
  training loss:		9.719967E-05
  validation loss:		9.897833E-05

Epoch 54 of 200
  training loss:		9.435345E-05
  validation loss:		1.036290E-04

Epoch 55 of 200
  training loss:		9.389608E-05
  validation loss:		8.542564E-05

Epoch 56 of 200
  training loss:		9.009115E-05
  validation loss:		8.742618E-05

Epoch 57 of 200
  training loss:		8.873717E-05
  validation loss:		8.083095E-05

Epoch 58 of 200
  training loss:		8.146892E-05
  validation loss:		7.802865E-05

Epoch 59 of 200
  training loss:		7.984270E-05
  validation loss:		8.872499E-05

Epoch 60 of 200
  training loss:		7.992330E-05
  validation loss:		7.248532E-05

Epoch 61 of 200
  training loss:		8.235156E-05
  validation loss:		6.970219E-05

Epoch 62 of 200
  training loss:		7.128078E-05
  validation loss:		6.820854E-05

Epoch 63 of 200
  training loss:		7.416925E-05
  validation loss:		7.233807E-05

Epoch 64 of 200
  training loss:		7.401888E-05
  validation loss:		6.504284E-05

Epoch 65 of 200
  training loss:		6.931660E-05
  validation loss:		6.266103E-05

Epoch 66 of 200
  training loss:		6.752386E-05
  validation loss:		6.202061E-05

Epoch 67 of 200
  training loss:		6.852940E-05
  validation loss:		6.104622E-05

Epoch 68 of 200
  training loss:		6.414588E-05
  validation loss:		6.966881E-05

Epoch 69 of 200
  training loss:		6.760631E-05
  validation loss:		6.391633E-05

Epoch 70 of 200
  training loss:		6.395687E-05
  validation loss:		5.595523E-05

Epoch 71 of 200
  training loss:		5.831626E-05
  validation loss:		5.525641E-05

Epoch 72 of 200
  training loss:		5.955967E-05
  validation loss:		5.554652E-05

Epoch 73 of 200
  training loss:		5.801013E-05
  validation loss:		5.158731E-05

Epoch 74 of 200
  training loss:		5.360934E-05
  validation loss:		5.224454E-05

Epoch 75 of 200
  training loss:		5.337780E-05
  validation loss:		5.274578E-05

Epoch 76 of 200
  training loss:		5.154052E-05
  validation loss:		4.542767E-05

Epoch 77 of 200
  training loss:		5.170758E-05
  validation loss:		4.964803E-05

Epoch 78 of 200
  training loss:		4.732853E-05
  validation loss:		4.301910E-05

Epoch 79 of 200
  training loss:		4.788270E-05
  validation loss:		4.923825E-05

Epoch 80 of 200
  training loss:		4.765012E-05
  validation loss:		5.085985E-05

Epoch 81 of 200
  training loss:		4.625792E-05
  validation loss:		4.067658E-05

Epoch 82 of 200
  training loss:		4.432147E-05
  validation loss:		6.063764E-05

Epoch 83 of 200
  training loss:		4.500517E-05
  validation loss:		5.002711E-05

Epoch 84 of 200
  training loss:		3.977065E-05
  validation loss:		4.331612E-05

Epoch 85 of 200
  training loss:		4.042980E-05
  validation loss:		4.071889E-05

Epoch 86 of 200
  training loss:		4.005679E-05
  validation loss:		4.112668E-05

Epoch 87 of 200
  training loss:		4.087364E-05
  validation loss:		3.327136E-05

Epoch 88 of 200
  training loss:		3.880523E-05
  validation loss:		3.215719E-05

Epoch 89 of 200
  training loss:		3.619903E-05
  validation loss:		3.480322E-05

Epoch 90 of 200
  training loss:		3.376520E-05
  validation loss:		3.286952E-05

Epoch 91 of 200
  training loss:		3.399702E-05
  validation loss:		4.173626E-05

Epoch 92 of 200
  training loss:		3.471205E-05
  validation loss:		3.914956E-05

Epoch 93 of 200
  training loss:		3.340678E-05
  validation loss:		2.797401E-05

Epoch 94 of 200
  training loss:		3.573080E-05
  validation loss:		3.489858E-05

Epoch 95 of 200
  training loss:		3.060241E-05
  validation loss:		2.667250E-05

Epoch 96 of 200
  training loss:		3.459881E-05
  validation loss:		2.597068E-05

Epoch 97 of 200
  training loss:		3.320098E-05
  validation loss:		2.992639E-05

Epoch 98 of 200
  training loss:		2.922966E-05
  validation loss:		2.483108E-05

Epoch 99 of 200
  training loss:		2.886336E-05
  validation loss:		2.399904E-05

Epoch 100 of 200
  training loss:		2.871848E-05
  validation loss:		3.141238E-05

Epoch 101 of 200
  training loss:		2.714254E-05
  validation loss:		2.903183E-05

Epoch 102 of 200
  training loss:		2.562317E-05
  validation loss:		3.677228E-05

Epoch 103 of 200
  training loss:		2.656631E-05
  validation loss:		2.301900E-05

Epoch 104 of 200
  training loss:		2.381845E-05
  validation loss:		2.183752E-05

Epoch 105 of 200
  training loss:		2.366074E-05
  validation loss:		2.192620E-05

Epoch 106 of 200
  training loss:		2.493385E-05
  validation loss:		1.977442E-05

Epoch 107 of 200
  training loss:		2.402180E-05
  validation loss:		4.014455E-05

Epoch 108 of 200
  training loss:		2.269629E-05
  validation loss:		2.345279E-05

Epoch 109 of 200
  training loss:		2.462025E-05
  validation loss:		1.852184E-05

Epoch 110 of 200
  training loss:		2.044679E-05
  validation loss:		2.146791E-05

Epoch 111 of 200
  training loss:		2.234029E-05
  validation loss:		2.071338E-05

Epoch 112 of 200
  training loss:		2.024754E-05
  validation loss:		2.795414E-05

Epoch 113 of 200
  training loss:		1.928482E-05
  validation loss:		1.636086E-05

Epoch 114 of 200
  training loss:		2.112393E-05
  validation loss:		1.847767E-05

Epoch 115 of 200
  training loss:		1.899940E-05
  validation loss:		3.934770E-05

Epoch 116 of 200
  training loss:		1.931983E-05
  validation loss:		1.520142E-05

Epoch 117 of 200
  training loss:		1.858877E-05
  validation loss:		3.798827E-05

Epoch 118 of 200
  training loss:		1.823216E-05
  validation loss:		1.561684E-05

Epoch 119 of 200
  training loss:		1.667656E-05
  validation loss:		1.428340E-05

Epoch 120 of 200
  training loss:		1.651040E-05
  validation loss:		2.385432E-05

Epoch 121 of 200
  training loss:		1.760902E-05
  validation loss:		1.921090E-05

Epoch 122 of 200
  training loss:		1.605065E-05
  validation loss:		1.881421E-05

Epoch 123 of 200
  training loss:		1.633810E-05
  validation loss:		2.013894E-05

Epoch 124 of 200
  training loss:		1.652740E-05
  validation loss:		1.943225E-05

Epoch 125 of 200
  training loss:		1.648125E-05
  validation loss:		1.526663E-05

Epoch 126 of 200
  training loss:		1.672782E-05
  validation loss:		1.890789E-05

Epoch 127 of 200
  training loss:		1.533167E-05
  validation loss:		1.483448E-05

Epoch 128 of 200
  training loss:		1.567315E-05
  validation loss:		1.123010E-05

Epoch 129 of 200
  training loss:		1.254023E-05
  validation loss:		1.105943E-05

Epoch 130 of 200
  training loss:		1.396466E-05
  validation loss:		1.056643E-05

Epoch 131 of 200
  training loss:		1.377526E-05
  validation loss:		3.316425E-05

Epoch 132 of 200
  training loss:		1.338172E-05
  validation loss:		1.481614E-05

Epoch 133 of 200
  training loss:		1.203181E-05
  validation loss:		1.033537E-05

Epoch 134 of 200
  training loss:		1.552023E-05
  validation loss:		9.766934E-06

Epoch 135 of 200
  training loss:		1.193035E-05
  validation loss:		1.246280E-05

Epoch 136 of 200
  training loss:		1.394481E-05
  validation loss:		2.184492E-05

Epoch 137 of 200
  training loss:		1.340735E-05
  validation loss:		9.481335E-06

Epoch 138 of 200
  training loss:		1.167813E-05
  validation loss:		8.845614E-06

Epoch 139 of 200
  training loss:		1.076429E-05
  validation loss:		1.011443E-05

Epoch 140 of 200
  training loss:		1.165271E-05
  validation loss:		9.649442E-06

Epoch 141 of 200
  training loss:		1.201756E-05
  validation loss:		1.076790E-05

Epoch 142 of 200
  training loss:		1.222997E-05
  validation loss:		1.104501E-05

Epoch 143 of 200
  training loss:		1.031792E-05
  validation loss:		7.934033E-06

Epoch 144 of 200
  training loss:		1.116815E-05
  validation loss:		1.574348E-05

Epoch 145 of 200
  training loss:		1.001707E-05
  validation loss:		8.762717E-06

Epoch 146 of 200
  training loss:		1.115946E-05
  validation loss:		2.601013E-05

Epoch 147 of 200
  training loss:		1.035315E-05
  validation loss:		1.180882E-05

Epoch 148 of 200
  training loss:		1.106184E-05
  validation loss:		7.616972E-06

Epoch 149 of 200
  training loss:		1.118360E-05
  validation loss:		8.026638E-06

Epoch 150 of 200
  training loss:		8.550852E-06
  validation loss:		3.372605E-05

Epoch 151 of 200
  training loss:		1.137851E-05
  validation loss:		8.258341E-06

Epoch 152 of 200
  training loss:		1.017623E-05
  validation loss:		6.566801E-06

Epoch 153 of 200
  training loss:		8.836849E-06
  validation loss:		6.751631E-06

Epoch 154 of 200
  training loss:		9.193765E-06
  validation loss:		1.080923E-05

Epoch 155 of 200
  training loss:		9.921104E-06
  validation loss:		8.523183E-06

Epoch 156 of 200
  training loss:		9.063674E-06
  validation loss:		9.694589E-06

Epoch 157 of 200
  training loss:		1.010041E-05
  validation loss:		1.413126E-05

Epoch 158 of 200
  training loss:		8.788810E-06
  validation loss:		5.835063E-06

Epoch 159 of 200
  training loss:		8.161534E-06
  validation loss:		2.051725E-05

Epoch 160 of 200
  training loss:		7.912901E-06
  validation loss:		6.781783E-06

Epoch 161 of 200
  training loss:		7.668979E-06
  validation loss:		6.204726E-06

Epoch 162 of 200
  training loss:		9.438166E-06
  validation loss:		1.196977E-05

Epoch 163 of 200
  training loss:		9.148848E-06
  validation loss:		5.737097E-06

Epoch 164 of 200
  training loss:		7.514745E-06
  validation loss:		1.270931E-05

Epoch 165 of 200
  training loss:		8.529773E-06
  validation loss:		7.552800E-06

Epoch 166 of 200
  training loss:		7.391714E-06
  validation loss:		5.548880E-06

Epoch 167 of 200
  training loss:		7.695558E-06
  validation loss:		6.167726E-06

Epoch 168 of 200
  training loss:		7.683378E-06
  validation loss:		5.529829E-06

Epoch 169 of 200
  training loss:		6.815949E-06
  validation loss:		1.093285E-05

Epoch 170 of 200
  training loss:		7.954672E-06
  validation loss:		6.054613E-06

Epoch 171 of 200
  training loss:		7.235727E-06
  validation loss:		6.750290E-06

Epoch 172 of 200
  training loss:		6.761037E-06
  validation loss:		4.571087E-06

Epoch 173 of 200
  training loss:		7.055187E-06
  validation loss:		4.636826E-06

Epoch 174 of 200
  training loss:		6.651676E-06
  validation loss:		6.509444E-06

Epoch 175 of 200
  training loss:		7.290531E-06
  validation loss:		7.635987E-06

Epoch 176 of 200
  training loss:		7.529301E-06
  validation loss:		4.504391E-06

Epoch 177 of 200
  training loss:		6.801721E-06
  validation loss:		5.250920E-06

Epoch 178 of 200
  training loss:		6.100982E-06
  validation loss:		3.964865E-06

Epoch 179 of 200
  training loss:		6.058941E-06
  validation loss:		6.551933E-06

Epoch 180 of 200
  training loss:		7.447919E-06
  validation loss:		8.447625E-06

Epoch 181 of 200
  training loss:		5.615528E-06
  validation loss:		4.486683E-06

Epoch 182 of 200
  training loss:		6.920973E-06
  validation loss:		7.111318E-06

Epoch 183 of 200
  training loss:		6.392575E-06
  validation loss:		1.098125E-05

Epoch 184 of 200
  training loss:		6.181320E-06
  validation loss:		4.172139E-06

Epoch 185 of 200
  training loss:		5.738515E-06
  validation loss:		4.350168E-06

Epoch 186 of 200
  training loss:		6.463911E-06
  validation loss:		4.284023E-06

Epoch 187 of 200
  training loss:		6.923989E-06
  validation loss:		7.151707E-06

Epoch 188 of 200
  training loss:		5.056059E-06
  validation loss:		3.773085E-06

Epoch 189 of 200
  training loss:		6.246957E-06
  validation loss:		5.180508E-06

Epoch 190 of 200
  training loss:		5.098656E-06
  validation loss:		3.438840E-06

Epoch 191 of 200
  training loss:		5.823299E-06
  validation loss:		3.216493E-06

Epoch 192 of 200
  training loss:		7.975004E-06
  validation loss:		3.839590E-06

Epoch 193 of 200
  training loss:		4.897290E-06
  validation loss:		3.075178E-06

Epoch 194 of 200
  training loss:		5.146305E-06
  validation loss:		3.352256E-06

Epoch 195 of 200
  training loss:		4.407873E-06
  validation loss:		3.006150E-06

Epoch 196 of 200
  training loss:		6.381649E-06
  validation loss:		3.385581E-06

Epoch 197 of 200
  training loss:		5.414329E-06
  validation loss:		3.162837E-06

Epoch 198 of 200
  training loss:		5.246444E-06
  validation loss:		2.908754E-06

Epoch 199 of 200
  training loss:		5.242309E-06
  validation loss:		4.773787E-06

Epoch 200 of 200
  training loss:		5.469253E-06
  validation loss:		2.168194E-05

Training-set, Scaled RMSE: 0.00462993449858
Training-set, Original RMSE: 4.30821621299e-09
Validation-set, Scaled RMSE: 0.00465634966199
Validation-set, Original RMSE: 4.33279577418e-09
Test-set, Scaled RMSE: 0.00461985388751
Test-set, Original RMSE: 4.29883607954e-09
