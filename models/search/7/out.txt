Epoch 1 of 500
  training loss:		7.664263E-02
  validation loss:		5.307827E-02

Epoch 2 of 500
  training loss:		2.890688E-02
  validation loss:		1.276136E-02

Epoch 3 of 500
  training loss:		9.490132E-03
  validation loss:		8.081248E-03

Epoch 4 of 500
  training loss:		7.487835E-03
  validation loss:		7.132017E-03

Epoch 5 of 500
  training loss:		6.454421E-03
  validation loss:		5.919841E-03

Epoch 6 of 500
  training loss:		5.472577E-03
  validation loss:		5.286696E-03

Epoch 7 of 500
  training loss:		4.517146E-03
  validation loss:		4.173288E-03

Epoch 8 of 500
  training loss:		3.733688E-03
  validation loss:		3.301601E-03

Epoch 9 of 500
  training loss:		3.075851E-03
  validation loss:		2.802069E-03

Epoch 10 of 500
  training loss:		2.599747E-03
  validation loss:		2.593157E-03

Epoch 11 of 500
  training loss:		2.229675E-03
  validation loss:		2.084581E-03

Epoch 12 of 500
  training loss:		1.901074E-03
  validation loss:		1.651376E-03

Epoch 13 of 500
  training loss:		1.459727E-03
  validation loss:		1.378855E-03

Epoch 14 of 500
  training loss:		1.036144E-03
  validation loss:		9.124958E-04

Epoch 15 of 500
  training loss:		7.744000E-04
  validation loss:		6.560683E-04

Epoch 16 of 500
  training loss:		6.010008E-04
  validation loss:		5.186339E-04

Epoch 17 of 500
  training loss:		4.834046E-04
  validation loss:		4.169554E-04

Epoch 18 of 500
  training loss:		3.924845E-04
  validation loss:		3.500696E-04

Epoch 19 of 500
  training loss:		3.267808E-04
  validation loss:		2.950618E-04

Epoch 20 of 500
  training loss:		2.788945E-04
  validation loss:		2.392401E-04

Epoch 21 of 500
  training loss:		2.343224E-04
  validation loss:		2.079245E-04

Epoch 22 of 500
  training loss:		2.079806E-04
  validation loss:		2.066643E-04

Epoch 23 of 500
  training loss:		1.774313E-04
  validation loss:		1.658142E-04

Epoch 24 of 500
  training loss:		1.614478E-04
  validation loss:		1.394850E-04

Epoch 25 of 500
  training loss:		1.419921E-04
  validation loss:		1.320532E-04

Epoch 26 of 500
  training loss:		1.282439E-04
  validation loss:		1.110440E-04

Epoch 27 of 500
  training loss:		1.161188E-04
  validation loss:		1.001221E-04

Epoch 28 of 500
  training loss:		1.053631E-04
  validation loss:		1.554782E-04

Epoch 29 of 500
  training loss:		9.925331E-05
  validation loss:		8.914036E-05

Epoch 30 of 500
  training loss:		8.938878E-05
  validation loss:		8.882807E-05

Epoch 31 of 500
  training loss:		8.265098E-05
  validation loss:		7.198662E-05

Epoch 32 of 500
  training loss:		7.624065E-05
  validation loss:		7.630475E-05

Epoch 33 of 500
  training loss:		7.211127E-05
  validation loss:		9.771704E-05

Epoch 34 of 500
  training loss:		6.555584E-05
  validation loss:		6.252041E-05

Epoch 35 of 500
  training loss:		6.159188E-05
  validation loss:		5.713438E-05

Epoch 36 of 500
  training loss:		5.810789E-05
  validation loss:		5.059226E-05

Epoch 37 of 500
  training loss:		5.618176E-05
  validation loss:		5.296133E-05

Epoch 38 of 500
  training loss:		5.161515E-05
  validation loss:		4.972662E-05

Epoch 39 of 500
  training loss:		4.949699E-05
  validation loss:		4.456800E-05

Epoch 40 of 500
  training loss:		4.996687E-05
  validation loss:		4.546625E-05

Epoch 41 of 500
  training loss:		4.593758E-05
  validation loss:		4.134003E-05

Epoch 42 of 500
  training loss:		4.361350E-05
  validation loss:		3.774044E-05

Epoch 43 of 500
  training loss:		4.190434E-05
  validation loss:		3.640847E-05

Epoch 44 of 500
  training loss:		4.159904E-05
  validation loss:		3.773845E-05

Epoch 45 of 500
  training loss:		3.932457E-05
  validation loss:		3.503935E-05

Epoch 46 of 500
  training loss:		3.698057E-05
  validation loss:		3.314337E-05

Epoch 47 of 500
  training loss:		3.444832E-05
  validation loss:		3.265305E-05

Epoch 48 of 500
  training loss:		3.438570E-05
  validation loss:		3.078577E-05

Epoch 49 of 500
  training loss:		3.145898E-05
  validation loss:		2.817957E-05

Epoch 50 of 500
  training loss:		3.181861E-05
  validation loss:		3.332895E-05

Epoch 51 of 500
  training loss:		2.959518E-05
  validation loss:		2.936881E-05

Epoch 52 of 500
  training loss:		2.932777E-05
  validation loss:		2.665982E-05

Epoch 53 of 500
  training loss:		2.807255E-05
  validation loss:		2.396939E-05

Epoch 54 of 500
  training loss:		2.736909E-05
  validation loss:		2.220074E-05

Epoch 55 of 500
  training loss:		2.539003E-05
  validation loss:		2.207059E-05

Epoch 56 of 500
  training loss:		2.670971E-05
  validation loss:		2.066668E-05

Epoch 57 of 500
  training loss:		2.442639E-05
  validation loss:		2.271549E-05

Epoch 58 of 500
  training loss:		2.330884E-05
  validation loss:		2.145705E-05

Epoch 59 of 500
  training loss:		2.317301E-05
  validation loss:		1.923166E-05

Epoch 60 of 500
  training loss:		2.159914E-05
  validation loss:		1.863642E-05

Epoch 61 of 500
  training loss:		2.097923E-05
  validation loss:		2.072297E-05

Epoch 62 of 500
  training loss:		2.112924E-05
  validation loss:		1.674017E-05

Epoch 63 of 500
  training loss:		2.020111E-05
  validation loss:		1.820864E-05

Epoch 64 of 500
  training loss:		1.922511E-05
  validation loss:		1.777040E-05

Epoch 65 of 500
  training loss:		1.940264E-05
  validation loss:		1.534510E-05

Epoch 66 of 500
  training loss:		1.914555E-05
  validation loss:		1.656958E-05

Epoch 67 of 500
  training loss:		1.866291E-05
  validation loss:		1.746262E-05

Epoch 68 of 500
  training loss:		1.792546E-05
  validation loss:		1.498668E-05

Epoch 69 of 500
  training loss:		1.661039E-05
  validation loss:		1.368860E-05

Epoch 70 of 500
  training loss:		1.740033E-05
  validation loss:		1.511624E-05

Epoch 71 of 500
  training loss:		1.595247E-05
  validation loss:		1.287511E-05

Epoch 72 of 500
  training loss:		1.648491E-05
  validation loss:		1.240688E-05

Epoch 73 of 500
  training loss:		1.509733E-05
  validation loss:		1.204458E-05

Epoch 74 of 500
  training loss:		1.540922E-05
  validation loss:		2.042036E-05

Epoch 75 of 500
  training loss:		1.471410E-05
  validation loss:		1.179444E-05

Epoch 76 of 500
  training loss:		1.525339E-05
  validation loss:		1.654904E-05

Epoch 77 of 500
  training loss:		1.478380E-05
  validation loss:		1.101972E-05

Epoch 78 of 500
  training loss:		1.410040E-05
  validation loss:		1.058509E-05

Epoch 79 of 500
  training loss:		1.432102E-05
  validation loss:		1.058469E-05

Epoch 80 of 500
  training loss:		1.344951E-05
  validation loss:		1.265925E-05

Epoch 81 of 500
  training loss:		1.345730E-05
  validation loss:		1.032805E-05

Epoch 82 of 500
  training loss:		1.300551E-05
  validation loss:		1.025996E-05

Epoch 83 of 500
  training loss:		1.309716E-05
  validation loss:		1.727688E-05

Epoch 84 of 500
  training loss:		1.169668E-05
  validation loss:		9.532358E-06

Epoch 85 of 500
  training loss:		1.170740E-05
  validation loss:		1.458466E-05

Early stopping, val-loss increased over the last 5 epochs from 0.00216120222492 to 0.00218176336182
Training-set, RMSE: 3.10545954601e-09
Validation-set, RMSE: 3.03026756517e-09
