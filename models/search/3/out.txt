Training network with 45280 training samples and 8490 validation samples
Epoch 1 of 500
  training loss:		5.419687E-02
  validation loss:		1.429115E-02

Epoch 2 of 500
  training loss:		9.225559E-03
  validation loss:		7.453274E-03

Epoch 3 of 500
  training loss:		6.665784E-03
  validation loss:		5.629762E-03

Epoch 4 of 500
  training loss:		4.872298E-03
  validation loss:		4.044526E-03

Epoch 5 of 500
  training loss:		3.445874E-03
  validation loss:		2.818697E-03

Epoch 6 of 500
  training loss:		2.522894E-03
  validation loss:		2.096497E-03

Epoch 7 of 500
  training loss:		1.710832E-03
  validation loss:		1.201030E-03

Epoch 8 of 500
  training loss:		9.762232E-04
  validation loss:		7.074139E-04

Epoch 9 of 500
  training loss:		6.153544E-04
  validation loss:		5.680474E-04

Epoch 10 of 500
  training loss:		4.194595E-04
  validation loss:		3.242723E-04

Epoch 11 of 500
  training loss:		3.035831E-04
  validation loss:		2.570681E-04

Epoch 12 of 500
  training loss:		2.284283E-04
  validation loss:		1.976811E-04

Epoch 13 of 500
  training loss:		1.764910E-04
  validation loss:		1.548224E-04

Epoch 14 of 500
  training loss:		1.442302E-04
  validation loss:		1.195376E-04

Epoch 15 of 500
  training loss:		1.220097E-04
  validation loss:		9.938738E-05

Epoch 16 of 500
  training loss:		1.034672E-04
  validation loss:		8.452662E-05

Epoch 17 of 500
  training loss:		8.855121E-05
  validation loss:		1.046205E-04

Epoch 18 of 500
  training loss:		7.580356E-05
  validation loss:		6.432021E-05

Epoch 19 of 500
  training loss:		6.734512E-05
  validation loss:		5.748613E-05

Epoch 20 of 500
  training loss:		6.099195E-05
  validation loss:		6.001500E-05

Epoch 21 of 500
  training loss:		5.460153E-05
  validation loss:		4.420709E-05

Epoch 22 of 500
  training loss:		4.968962E-05
  validation loss:		4.271304E-05

Epoch 23 of 500
  training loss:		4.535712E-05
  validation loss:		3.680505E-05

Epoch 24 of 500
  training loss:		4.303965E-05
  validation loss:		5.429047E-05

Epoch 25 of 500
  training loss:		3.890026E-05
  validation loss:		5.201775E-05

Epoch 26 of 500
  training loss:		3.739116E-05
  validation loss:		3.516891E-05

Epoch 27 of 500
  training loss:		3.380029E-05
  validation loss:		2.669137E-05

Epoch 28 of 500
  training loss:		3.276485E-05
  validation loss:		3.087766E-05

Epoch 29 of 500
  training loss:		3.070127E-05
  validation loss:		6.314276E-05

Epoch 30 of 500
  training loss:		2.824611E-05
  validation loss:		2.266978E-05

Epoch 31 of 500
  training loss:		2.637412E-05
  validation loss:		2.113377E-05

Epoch 32 of 500
  training loss:		2.513161E-05
  validation loss:		1.940067E-05

Epoch 33 of 500
  training loss:		2.409250E-05
  validation loss:		1.956673E-05

Epoch 34 of 500
  training loss:		2.227602E-05
  validation loss:		1.836747E-05

Epoch 35 of 500
  training loss:		2.098132E-05
  validation loss:		1.940162E-05

Epoch 36 of 500
  training loss:		2.002355E-05
  validation loss:		1.663699E-05

Epoch 37 of 500
  training loss:		2.002962E-05
  validation loss:		1.734749E-05

Epoch 38 of 500
  training loss:		1.843753E-05
  validation loss:		1.600735E-05

Epoch 39 of 500
  training loss:		1.758136E-05
  validation loss:		1.520535E-05

Epoch 40 of 500
  training loss:		1.813845E-05
  validation loss:		1.578486E-05

Epoch 41 of 500
  training loss:		1.603483E-05
  validation loss:		1.273410E-05

Epoch 42 of 500
  training loss:		1.581914E-05
  validation loss:		1.179635E-05

Epoch 43 of 500
  training loss:		1.457562E-05
  validation loss:		1.210555E-05

Epoch 44 of 500
  training loss:		1.494532E-05
  validation loss:		1.085708E-05

Epoch 45 of 500
  training loss:		1.390211E-05
  validation loss:		1.039720E-05

Epoch 46 of 500
  training loss:		1.316765E-05
  validation loss:		1.215763E-05

Epoch 47 of 500
  training loss:		1.270043E-05
  validation loss:		1.149896E-05

Epoch 48 of 500
  training loss:		1.253221E-05
  validation loss:		1.029985E-05

Epoch 49 of 500
  training loss:		1.132948E-05
  validation loss:		8.439982E-06

Epoch 50 of 500
  training loss:		1.174395E-05
  validation loss:		9.682706E-06

Epoch 51 of 500
  training loss:		1.180699E-05
  validation loss:		8.515884E-06

Epoch 52 of 500
  training loss:		1.013316E-05
  validation loss:		1.569201E-05

Epoch 53 of 500
  training loss:		1.029268E-05
  validation loss:		1.864852E-05

Epoch 54 of 500
  training loss:		9.785807E-06
  validation loss:		7.969468E-06

Epoch 55 of 500
  training loss:		9.475330E-06
  validation loss:		8.229522E-06

Epoch 56 of 500
  training loss:		9.199065E-06
  validation loss:		8.742474E-06

Epoch 57 of 500
  training loss:		8.638423E-06
  validation loss:		8.807135E-06

Epoch 58 of 500
  training loss:		8.289020E-06
  validation loss:		6.401575E-06

Epoch 59 of 500
  training loss:		8.129816E-06
  validation loss:		8.250435E-06

Epoch 60 of 500
  training loss:		8.268556E-06
  validation loss:		6.161548E-06

Epoch 61 of 500
  training loss:		8.083195E-06
  validation loss:		5.129365E-06

Epoch 62 of 500
  training loss:		7.599018E-06
  validation loss:		7.213509E-06

Epoch 63 of 500
  training loss:		6.916110E-06
  validation loss:		7.055178E-06

Epoch 64 of 500
  training loss:		7.321953E-06
  validation loss:		1.015542E-05

Epoch 65 of 500
  training loss:		6.406164E-06
  validation loss:		7.232018E-06

Epoch 66 of 500
  training loss:		6.412089E-06
  validation loss:		4.916543E-06

Epoch 67 of 500
  training loss:		6.464826E-06
  validation loss:		5.896504E-06

Epoch 68 of 500
  training loss:		6.166121E-06
  validation loss:		7.847120E-06

Epoch 69 of 500
  training loss:		6.066331E-06
  validation loss:		9.456981E-06

Epoch 70 of 500
  training loss:		5.965324E-06
  validation loss:		1.072431E-05

Epoch 71 of 500
  training loss:		5.438829E-06
  validation loss:		3.757055E-06

Epoch 72 of 500
  training loss:		5.814948E-06
  validation loss:		7.281705E-06

Epoch 73 of 500
  training loss:		5.239888E-06
  validation loss:		7.682025E-06

Epoch 74 of 500
  training loss:		5.327020E-06
  validation loss:		4.415001E-06

Epoch 75 of 500
  training loss:		4.731069E-06
  validation loss:		3.741553E-06

Epoch 76 of 500
  training loss:		4.914643E-06
  validation loss:		3.154064E-06

Epoch 77 of 500
  training loss:		4.434232E-06
  validation loss:		3.617999E-06

Epoch 78 of 500
  training loss:		5.058292E-06
  validation loss:		3.430259E-06

Epoch 79 of 500
  training loss:		4.596573E-06
  validation loss:		2.703296E-06

Epoch 80 of 500
  training loss:		4.472157E-06
  validation loss:		2.546357E-06

Epoch 81 of 500
  training loss:		4.205794E-06
  validation loss:		3.286477E-06

Epoch 82 of 500
  training loss:		4.136872E-06
  validation loss:		5.616627E-06

Epoch 83 of 500
  training loss:		4.297174E-06
  validation loss:		2.313207E-06

Epoch 84 of 500
  training loss:		4.104701E-06
  validation loss:		2.366312E-06

Epoch 85 of 500
  training loss:		3.600996E-06
  validation loss:		7.139981E-06

Epoch 86 of 500
  training loss:		3.822290E-06
  validation loss:		2.314017E-06

Epoch 87 of 500
  training loss:		3.617555E-06
  validation loss:		2.865747E-06

Epoch 88 of 500
  training loss:		3.822654E-06
  validation loss:		2.482584E-06

Epoch 89 of 500
  training loss:		3.382720E-06
  validation loss:		3.597454E-06

Epoch 90 of 500
  training loss:		3.294032E-06
  validation loss:		1.835929E-06

Epoch 91 of 500
  training loss:		3.388053E-06
  validation loss:		4.780147E-06

Epoch 92 of 500
  training loss:		3.451970E-06
  validation loss:		4.570794E-06

Epoch 93 of 500
  training loss:		3.193109E-06
  validation loss:		6.955513E-06

Epoch 94 of 500
  training loss:		2.943799E-06
  validation loss:		2.910280E-06

Epoch 95 of 500
  training loss:		2.986512E-06
  validation loss:		3.261075E-06

Epoch 96 of 500
  training loss:		3.198077E-06
  validation loss:		1.526170E-06

Epoch 97 of 500
  training loss:		2.985459E-06
  validation loss:		1.817293E-06

Epoch 98 of 500
  training loss:		2.915709E-06
  validation loss:		3.137081E-06

Epoch 99 of 500
  training loss:		2.893142E-06
  validation loss:		2.628832E-06

Epoch 100 of 500
  training loss:		2.717589E-06
  validation loss:		1.595206E-06

Epoch 101 of 500
  training loss:		2.793065E-06
  validation loss:		1.324982E-06

Epoch 102 of 500
  training loss:		2.495361E-06
  validation loss:		1.679641E-06

Epoch 103 of 500
  training loss:		2.451184E-06
  validation loss:		1.323142E-06

Epoch 104 of 500
  training loss:		2.539213E-06
  validation loss:		2.006170E-06

Epoch 105 of 500
  training loss:		2.578455E-06
  validation loss:		1.266900E-06

Epoch 106 of 500
  training loss:		2.277225E-06
  validation loss:		1.399026E-06

Epoch 107 of 500
  training loss:		2.377245E-06
  validation loss:		1.723281E-06

Epoch 108 of 500
  training loss:		2.270031E-06
  validation loss:		1.965846E-06

Epoch 109 of 500
  training loss:		2.228405E-06
  validation loss:		2.674606E-06

Epoch 110 of 500
  training loss:		2.170806E-06
  validation loss:		5.018770E-06

Epoch 111 of 500
  training loss:		2.378928E-06
  validation loss:		9.642593E-07

Epoch 112 of 500
  training loss:		2.147667E-06
  validation loss:		1.008326E-06

Epoch 113 of 500
  training loss:		2.110110E-06
  validation loss:		9.546454E-07

Epoch 114 of 500
  training loss:		2.131352E-06
  validation loss:		6.841043E-06

Epoch 115 of 500
  training loss:		2.053939E-06
  validation loss:		1.955045E-06

Epoch 116 of 500
  training loss:		2.011664E-06
  validation loss:		1.145657E-06

Epoch 117 of 500
  training loss:		2.146829E-06
  validation loss:		9.315673E-07

Epoch 118 of 500
  training loss:		2.029698E-06
  validation loss:		8.163774E-07

Epoch 119 of 500
  training loss:		1.773257E-06
  validation loss:		9.347232E-07

Epoch 120 of 500
  training loss:		1.915964E-06
  validation loss:		8.688825E-07

Epoch 121 of 500
  training loss:		1.994561E-06
  validation loss:		8.378177E-07

Epoch 122 of 500
  training loss:		1.913382E-06
  validation loss:		3.659944E-06

Epoch 123 of 500
  training loss:		1.771847E-06
  validation loss:		5.471084E-06

Epoch 124 of 500
  training loss:		1.852714E-06
  validation loss:		1.166973E-06

Epoch 125 of 500
  training loss:		1.786143E-06
  validation loss:		1.091406E-06

Epoch 126 of 500
  training loss:		1.886956E-06
  validation loss:		7.498851E-07

Epoch 127 of 500
  training loss:		1.687137E-06
  validation loss:		3.802775E-06

Epoch 128 of 500
  training loss:		1.663334E-06
  validation loss:		9.292465E-07

Epoch 129 of 500
  training loss:		1.772810E-06
  validation loss:		3.490395E-06

Epoch 130 of 500
  training loss:		1.566745E-06
  validation loss:		1.215715E-06

Early stopping, val-loss increased over the last 10 epochs from 0.000435143935143 to 0.000594003894365
Training RMSE: 1.83272454731e-09
Validation RMSE: 1.83188445027e-09
