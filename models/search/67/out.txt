Epoch 1 of 500
  training loss:		1.578524E-03
  validation loss:		4.230103E-10

Epoch 2 of 500
  training loss:		2.677815E-10
  validation loss:		1.397615E-10

Epoch 3 of 500
  training loss:		7.168342E-11
  validation loss:		2.724815E-11

Epoch 4 of 500
  training loss:		1.198734E-11
  validation loss:		3.849706E-12

Epoch 5 of 500
  training loss:		2.087866E-12
  validation loss:		1.371674E-12

Epoch 6 of 500
  training loss:		1.194127E-12
  validation loss:		1.146256E-12

Epoch 7 of 500
  training loss:		1.119736E-12
  validation loss:		1.071167E-12

Epoch 8 of 500
  training loss:		1.065251E-12
  validation loss:		1.006206E-12

Epoch 9 of 500
  training loss:		9.853450E-13
  validation loss:		8.886998E-13

Epoch 10 of 500
  training loss:		8.927733E-13
  validation loss:		8.003592E-13

Epoch 11 of 500
  training loss:		8.958306E-13
  validation loss:		9.621627E-13

Epoch 12 of 500
  training loss:		9.121641E-13
  validation loss:		6.159753E-13

Epoch 13 of 500
  training loss:		1.436644E-12
  validation loss:		9.475324E-12

Epoch 14 of 500
  training loss:		2.602675E-07
  validation loss:		1.661819E-09

Epoch 15 of 500
  training loss:		3.315399E-07
  validation loss:		2.013419E-10

Epoch 16 of 500
  training loss:		2.789948E-07
  validation loss:		4.717359E-07

Epoch 17 of 500
  training loss:		2.770826E-07
  validation loss:		2.227851E-09

Epoch 18 of 500
  training loss:		2.550561E-07
  validation loss:		3.043580E-09

Epoch 19 of 500
  training loss:		2.805003E-07
  validation loss:		1.614932E-11

Epoch 20 of 500
  training loss:		2.614033E-07
  validation loss:		4.096444E-10

Early stopping, val-loss increased over the last 10 epochs from 1.05627101065e-08 to 8.43580991263e-06
Training-set, RMSE: 4.0011816629e-06
Validation-set, RMSE: 4.0192716159e-06
