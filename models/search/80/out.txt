Epoch 1 of 500
  training loss:		5.216876E-03
  validation loss:		8.253829E-08

Epoch 2 of 500
  training loss:		4.992109E-08
  validation loss:		2.979589E-08

Epoch 3 of 500
  training loss:		1.702072E-08
  validation loss:		8.783879E-09

Epoch 4 of 500
  training loss:		4.794488E-09
  validation loss:		2.376586E-09

Epoch 5 of 500
  training loss:		1.415046E-09
  validation loss:		8.629737E-10

Epoch 6 of 500
  training loss:		6.189485E-10
  validation loss:		4.614202E-10

Epoch 7 of 500
  training loss:		3.753459E-10
  validation loss:		3.182980E-10

Epoch 8 of 500
  training loss:		2.572132E-10
  validation loss:		2.062018E-10

Epoch 9 of 500
  training loss:		1.768141E-10
  validation loss:		1.461101E-10

Epoch 10 of 500
  training loss:		1.229388E-10
  validation loss:		1.101050E-10

Epoch 11 of 500
  training loss:		9.287917E-11
  validation loss:		7.530625E-11

Epoch 12 of 500
  training loss:		7.128627E-11
  validation loss:		6.292785E-11

Epoch 13 of 500
  training loss:		5.487693E-11
  validation loss:		4.607298E-11

Epoch 14 of 500
  training loss:		4.435278E-11
  validation loss:		4.257264E-11

Epoch 15 of 500
  training loss:		3.895238E-11
  validation loss:		3.152854E-11

Epoch 16 of 500
  training loss:		3.388741E-11
  validation loss:		2.885667E-11

Epoch 17 of 500
  training loss:		3.442327E-11
  validation loss:		2.228449E-11

Epoch 18 of 500
  training loss:		3.983067E-11
  validation loss:		3.518710E-11

Epoch 19 of 500
  training loss:		6.005158E-05
  validation loss:		1.032980E-04

Epoch 20 of 500
  training loss:		3.484339E-06
  validation loss:		4.316946E-09

Early stopping, val-loss increased over the last 10 epochs from 5.52638896646e-07 to 0.000454531582917
Training-set, RMSE: 0.0101591405298
Validation-set, RMSE: 0.0101636408132
