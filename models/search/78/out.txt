Epoch 1 of 500
  training loss:		1.678029E-03
  validation loss:		2.397505E-08

Epoch 2 of 500
  training loss:		1.877269E-08
  validation loss:		1.749600E-08

Epoch 3 of 500
  training loss:		1.615756E-08
  validation loss:		1.520377E-08

Epoch 4 of 500
  training loss:		1.359267E-08
  validation loss:		1.234226E-08

Epoch 5 of 500
  training loss:		1.113914E-08
  validation loss:		1.006792E-08

Epoch 6 of 500
  training loss:		8.932838E-09
  validation loss:		7.943986E-09

Epoch 7 of 500
  training loss:		7.038314E-09
  validation loss:		6.231763E-09

Epoch 8 of 500
  training loss:		5.450179E-09
  validation loss:		4.832169E-09

Epoch 9 of 500
  training loss:		4.161238E-09
  validation loss:		3.703185E-09

Epoch 10 of 500
  training loss:		3.116206E-09
  validation loss:		2.668209E-09

Epoch 11 of 500
  training loss:		2.297602E-09
  validation loss:		1.950400E-09

Epoch 12 of 500
  training loss:		1.673420E-09
  validation loss:		1.437119E-09

Epoch 13 of 500
  training loss:		1.195385E-09
  validation loss:		1.006514E-09

Epoch 14 of 500
  training loss:		8.374882E-10
  validation loss:		8.035827E-10

Epoch 15 of 500
  training loss:		5.800155E-10
  validation loss:		4.786552E-10

Epoch 16 of 500
  training loss:		3.974366E-10
  validation loss:		3.266458E-10

Epoch 17 of 500
  training loss:		2.721028E-10
  validation loss:		2.233737E-10

Epoch 18 of 500
  training loss:		1.839311E-10
  validation loss:		1.503654E-10

Epoch 19 of 500
  training loss:		1.257596E-10
  validation loss:		1.046007E-10

Epoch 20 of 500
  training loss:		8.825460E-11
  validation loss:		7.471697E-11

Epoch 21 of 500
  training loss:		6.378636E-11
  validation loss:		5.610402E-11

Epoch 22 of 500
  training loss:		5.003172E-11
  validation loss:		4.455546E-11

Epoch 23 of 500
  training loss:		4.123633E-11
  validation loss:		3.789783E-11

Epoch 24 of 500
  training loss:		3.527840E-11
  validation loss:		3.893005E-11

Epoch 25 of 500
  training loss:		3.265303E-11
  validation loss:		3.124525E-11

Epoch 26 of 500
  training loss:		3.037889E-11
  validation loss:		2.966732E-11

Epoch 27 of 500
  training loss:		2.949813E-11
  validation loss:		2.991522E-11

Epoch 28 of 500
  training loss:		2.827917E-11
  validation loss:		2.822289E-11

Epoch 29 of 500
  training loss:		2.921790E-11
  validation loss:		2.883879E-11

Epoch 30 of 500
  training loss:		2.933239E-11
  validation loss:		3.036081E-11

Epoch 31 of 500
  training loss:		2.903602E-11
  validation loss:		2.675649E-11

Epoch 32 of 500
  training loss:		2.871933E-11
  validation loss:		2.783654E-11

Epoch 33 of 500
  training loss:		2.870777E-11
  validation loss:		4.071279E-11

Epoch 34 of 500
  training loss:		3.035289E-11
  validation loss:		5.186341E-11

Epoch 35 of 500
  training loss:		3.105055E-11
  validation loss:		3.925639E-11

Epoch 36 of 500
  training loss:		4.247851E-11
  validation loss:		2.461552E-11

Epoch 37 of 500
  training loss:		4.589406E-11
  validation loss:		7.870369E-11

Epoch 38 of 500
  training loss:		1.071793E-06
  validation loss:		3.209407E-08

Epoch 39 of 500
  training loss:		6.604164E-07
  validation loss:		5.147894E-07

Epoch 40 of 500
  training loss:		8.682654E-07
  validation loss:		4.967433E-08

Early stopping, val-loss increased over the last 10 epochs from 1.56524564719e-09 to 2.62612898947e-06
Training-set, RMSE: 0.000717495466418
Validation-set, RMSE: 0.000717487398417
