Training network with 45280 training samples and 8490 validation samples
Epoch 1 of 500
  training loss:		5.285294E-02
  validation loss:		1.396298E-02

Epoch 2 of 500
  training loss:		8.974096E-03
  validation loss:		7.212815E-03

Epoch 3 of 500
  training loss:		6.397902E-03
  validation loss:		5.324420E-03

Epoch 4 of 500
  training loss:		4.576588E-03
  validation loss:		3.731684E-03

Epoch 5 of 500
  training loss:		3.217477E-03
  validation loss:		2.636268E-03

Epoch 6 of 500
  training loss:		2.378803E-03
  validation loss:		2.024874E-03

Epoch 7 of 500
  training loss:		1.799886E-03
  validation loss:		1.370814E-03

Epoch 8 of 500
  training loss:		1.058396E-03
  validation loss:		8.619007E-04

Epoch 9 of 500
  training loss:		6.385811E-04
  validation loss:		4.773374E-04

Epoch 10 of 500
  training loss:		4.318351E-04
  validation loss:		3.519131E-04

Epoch 11 of 500
  training loss:		3.113726E-04
  validation loss:		2.529439E-04

Epoch 12 of 500
  training loss:		2.331816E-04
  validation loss:		2.169647E-04

Epoch 13 of 500
  training loss:		1.841920E-04
  validation loss:		1.480102E-04

Epoch 14 of 500
  training loss:		1.472952E-04
  validation loss:		1.618087E-04

Epoch 15 of 500
  training loss:		1.232560E-04
  validation loss:		9.949344E-05

Epoch 16 of 500
  training loss:		1.030254E-04
  validation loss:		8.440166E-05

Epoch 17 of 500
  training loss:		8.944646E-05
  validation loss:		7.244241E-05

Epoch 18 of 500
  training loss:		7.744295E-05
  validation loss:		8.650613E-05

Epoch 19 of 500
  training loss:		6.799774E-05
  validation loss:		5.593248E-05

Epoch 20 of 500
  training loss:		6.125574E-05
  validation loss:		7.598817E-05

Epoch 21 of 500
  training loss:		5.502564E-05
  validation loss:		4.370748E-05

Epoch 22 of 500
  training loss:		4.929926E-05
  validation loss:		5.875006E-05

Epoch 23 of 500
  training loss:		4.609810E-05
  validation loss:		3.737981E-05

Epoch 24 of 500
  training loss:		4.098972E-05
  validation loss:		4.286150E-05

Epoch 25 of 500
  training loss:		3.935379E-05
  validation loss:		3.113908E-05

Epoch 26 of 500
  training loss:		3.435904E-05
  validation loss:		3.031436E-05

Epoch 27 of 500
  training loss:		3.358181E-05
  validation loss:		2.577970E-05

Epoch 28 of 500
  training loss:		2.982271E-05
  validation loss:		2.459344E-05

Epoch 29 of 500
  training loss:		2.904389E-05
  validation loss:		2.955092E-05

Epoch 30 of 500
  training loss:		2.770546E-05
  validation loss:		2.348649E-05

Epoch 31 of 500
  training loss:		2.519867E-05
  validation loss:		2.111575E-05

Epoch 32 of 500
  training loss:		2.398536E-05
  validation loss:		2.136341E-05

Epoch 33 of 500
  training loss:		2.271270E-05
  validation loss:		1.824256E-05

Epoch 34 of 500
  training loss:		2.193746E-05
  validation loss:		1.749073E-05

Epoch 35 of 500
  training loss:		2.030753E-05
  validation loss:		1.514842E-05

Epoch 36 of 500
  training loss:		1.949045E-05
  validation loss:		1.525885E-05

Epoch 37 of 500
  training loss:		1.859210E-05
  validation loss:		2.077979E-05

Epoch 38 of 500
  training loss:		1.787105E-05
  validation loss:		4.433539E-05

Epoch 39 of 500
  training loss:		1.675891E-05
  validation loss:		1.403844E-05

Epoch 40 of 500
  training loss:		1.624565E-05
  validation loss:		2.504182E-05

Epoch 41 of 500
  training loss:		1.639228E-05
  validation loss:		1.119157E-05

Epoch 42 of 500
  training loss:		1.488538E-05
  validation loss:		1.081222E-05

Epoch 43 of 500
  training loss:		1.427041E-05
  validation loss:		1.228896E-05

Epoch 44 of 500
  training loss:		1.389471E-05
  validation loss:		1.247666E-05

Epoch 45 of 500
  training loss:		1.363203E-05
  validation loss:		1.081020E-05

Epoch 46 of 500
  training loss:		1.250605E-05
  validation loss:		1.221015E-05

Epoch 47 of 500
  training loss:		1.284399E-05
  validation loss:		1.994742E-05

Epoch 48 of 500
  training loss:		1.251915E-05
  validation loss:		1.371962E-05

Epoch 49 of 500
  training loss:		1.138231E-05
  validation loss:		1.370520E-05

Epoch 50 of 500
  training loss:		1.129994E-05
  validation loss:		9.824654E-06

Epoch 51 of 500
  training loss:		1.072860E-05
  validation loss:		7.590836E-06

Epoch 52 of 500
  training loss:		1.042233E-05
  validation loss:		7.482385E-06

Epoch 53 of 500
  training loss:		9.467214E-06
  validation loss:		6.856674E-06

Epoch 54 of 500
  training loss:		9.835270E-06
  validation loss:		9.834259E-06

Epoch 55 of 500
  training loss:		9.924667E-06
  validation loss:		6.650030E-06

Epoch 56 of 500
  training loss:		8.904825E-06
  validation loss:		6.666439E-06

Epoch 57 of 500
  training loss:		8.508747E-06
  validation loss:		9.870340E-06

Epoch 58 of 500
  training loss:		8.449810E-06
  validation loss:		6.217829E-06

Epoch 59 of 500
  training loss:		8.521174E-06
  validation loss:		6.830427E-06

Epoch 60 of 500
  training loss:		7.462918E-06
  validation loss:		1.351737E-05

Epoch 61 of 500
  training loss:		7.998261E-06
  validation loss:		6.324516E-06

Epoch 62 of 500
  training loss:		7.445132E-06
  validation loss:		6.095632E-06

Epoch 63 of 500
  training loss:		7.213334E-06
  validation loss:		5.437737E-06

Epoch 64 of 500
  training loss:		7.361215E-06
  validation loss:		6.798994E-06

Epoch 65 of 500
  training loss:		6.809936E-06
  validation loss:		5.402584E-06

Epoch 66 of 500
  training loss:		6.431015E-06
  validation loss:		4.211770E-06

Epoch 67 of 500
  training loss:		6.410802E-06
  validation loss:		3.943452E-06

Epoch 68 of 500
  training loss:		6.259943E-06
  validation loss:		4.150477E-06

Epoch 69 of 500
  training loss:		5.765127E-06
  validation loss:		1.741462E-05

Epoch 70 of 500
  training loss:		5.919384E-06
  validation loss:		5.153393E-06

Epoch 71 of 500
  training loss:		5.811988E-06
  validation loss:		5.276021E-06

Epoch 72 of 500
  training loss:		5.773187E-06
  validation loss:		1.285045E-05

Epoch 73 of 500
  training loss:		5.618540E-06
  validation loss:		3.242856E-06

Epoch 74 of 500
  training loss:		5.257284E-06
  validation loss:		3.208113E-06

Epoch 75 of 500
  training loss:		4.996214E-06
  validation loss:		3.495835E-06

Epoch 76 of 500
  training loss:		4.872113E-06
  validation loss:		2.879786E-06

Epoch 77 of 500
  training loss:		5.080545E-06
  validation loss:		5.169577E-06

Epoch 78 of 500
  training loss:		4.468506E-06
  validation loss:		3.228688E-06

Epoch 79 of 500
  training loss:		4.512116E-06
  validation loss:		2.894824E-06

Epoch 80 of 500
  training loss:		4.580862E-06
  validation loss:		2.542763E-06

Epoch 81 of 500
  training loss:		4.515283E-06
  validation loss:		2.425286E-06

Epoch 82 of 500
  training loss:		4.290301E-06
  validation loss:		2.334068E-06

Epoch 83 of 500
  training loss:		3.840493E-06
  validation loss:		2.685098E-06

Epoch 84 of 500
  training loss:		3.970662E-06
  validation loss:		2.278199E-06

Epoch 85 of 500
  training loss:		3.954433E-06
  validation loss:		3.535919E-06

Epoch 86 of 500
  training loss:		3.849743E-06
  validation loss:		2.090901E-06

Epoch 87 of 500
  training loss:		3.967424E-06
  validation loss:		2.934901E-06

Epoch 88 of 500
  training loss:		3.621344E-06
  validation loss:		2.430270E-06

Epoch 89 of 500
  training loss:		3.790130E-06
  validation loss:		1.974944E-06

Epoch 90 of 500
  training loss:		3.607317E-06
  validation loss:		2.449074E-06

Epoch 91 of 500
  training loss:		3.475644E-06
  validation loss:		4.947286E-06

Epoch 92 of 500
  training loss:		3.291394E-06
  validation loss:		2.587084E-06

Epoch 93 of 500
  training loss:		3.305344E-06
  validation loss:		2.386908E-06

Epoch 94 of 500
  training loss:		3.121293E-06
  validation loss:		1.656760E-06

Epoch 95 of 500
  training loss:		3.355631E-06
  validation loss:		2.514360E-06

Epoch 96 of 500
  training loss:		2.958345E-06
  validation loss:		1.869147E-06

Epoch 97 of 500
  training loss:		3.195147E-06
  validation loss:		5.345397E-06

Epoch 98 of 500
  training loss:		2.742648E-06
  validation loss:		1.523926E-06

Epoch 99 of 500
  training loss:		2.805884E-06
  validation loss:		5.484939E-06

Epoch 100 of 500
  training loss:		2.760907E-06
  validation loss:		1.653311E-06

Epoch 101 of 500
  training loss:		2.752755E-06
  validation loss:		2.551401E-06

Epoch 102 of 500
  training loss:		2.696499E-06
  validation loss:		2.651182E-06

Epoch 103 of 500
  training loss:		2.637074E-06
  validation loss:		1.361344E-06

Epoch 104 of 500
  training loss:		2.583642E-06
  validation loss:		1.301075E-06

Epoch 105 of 500
  training loss:		2.671394E-06
  validation loss:		1.217144E-06

Epoch 106 of 500
  training loss:		2.494147E-06
  validation loss:		4.713255E-06

Epoch 107 of 500
  training loss:		2.557873E-06
  validation loss:		1.201741E-06

Epoch 108 of 500
  training loss:		2.353442E-06
  validation loss:		1.184087E-06

Epoch 109 of 500
  training loss:		2.496666E-06
  validation loss:		1.089200E-06

Epoch 110 of 500
  training loss:		2.321578E-06
  validation loss:		1.143730E-06

Epoch 111 of 500
  training loss:		2.280362E-06
  validation loss:		1.341324E-06

Epoch 112 of 500
  training loss:		2.570593E-06
  validation loss:		3.918092E-06

Epoch 113 of 500
  training loss:		2.196959E-06
  validation loss:		9.791295E-07

Epoch 114 of 500
  training loss:		2.233136E-06
  validation loss:		2.801921E-06

Epoch 115 of 500
  training loss:		2.379584E-06
  validation loss:		6.782543E-06

Epoch 116 of 500
  training loss:		2.080716E-06
  validation loss:		2.869141E-06

Epoch 117 of 500
  training loss:		2.250827E-06
  validation loss:		9.431609E-07

Epoch 118 of 500
  training loss:		2.007778E-06
  validation loss:		9.488393E-07

Epoch 119 of 500
  training loss:		2.220535E-06
  validation loss:		1.117087E-06

Epoch 120 of 500
  training loss:		1.967955E-06
  validation loss:		3.249068E-06

Epoch 121 of 500
  training loss:		2.034591E-06
  validation loss:		2.409539E-06

Epoch 122 of 500
  training loss:		2.109397E-06
  validation loss:		9.295827E-07

Epoch 123 of 500
  training loss:		2.007693E-06
  validation loss:		2.549866E-06

Epoch 124 of 500
  training loss:		1.836949E-06
  validation loss:		1.461648E-06

Epoch 125 of 500
  training loss:		2.007943E-06
  validation loss:		2.021410E-06

Epoch 126 of 500
  training loss:		1.815958E-06
  validation loss:		2.830605E-06

Epoch 127 of 500
  training loss:		1.905276E-06
  validation loss:		9.954691E-07

Epoch 128 of 500
  training loss:		1.953001E-06
  validation loss:		6.914242E-07

Epoch 129 of 500
  training loss:		1.767010E-06
  validation loss:		1.280751E-06

Epoch 130 of 500
  training loss:		1.750130E-06
  validation loss:		2.296009E-06

Epoch 131 of 500
  training loss:		1.891246E-06
  validation loss:		2.306380E-06

Epoch 132 of 500
  training loss:		1.762870E-06
  validation loss:		9.087331E-07

Epoch 133 of 500
  training loss:		1.814804E-06
  validation loss:		6.467133E-07

Epoch 134 of 500
  training loss:		1.629454E-06
  validation loss:		1.269037E-06

Epoch 135 of 500
  training loss:		1.762964E-06
  validation loss:		6.234074E-07

Epoch 136 of 500
  training loss:		1.604891E-06
  validation loss:		7.404435E-07

Epoch 137 of 500
  training loss:		1.761563E-06
  validation loss:		6.159803E-07

Epoch 138 of 500
  training loss:		1.629991E-06
  validation loss:		1.841308E-06

Epoch 139 of 500
  training loss:		1.635863E-06
  validation loss:		3.464464E-06

Epoch 140 of 500
  training loss:		1.587519E-06
  validation loss:		6.139490E-07

Epoch 141 of 500
  training loss:		1.604299E-06
  validation loss:		5.522364E-07

Epoch 142 of 500
  training loss:		1.727927E-06
  validation loss:		1.930121E-06

Epoch 143 of 500
  training loss:		1.495973E-06
  validation loss:		2.460363E-06

Epoch 144 of 500
  training loss:		1.649756E-06
  validation loss:		5.015636E-07

Epoch 145 of 500
  training loss:		1.518411E-06
  validation loss:		9.173158E-07

Epoch 146 of 500
  training loss:		1.544749E-06
  validation loss:		5.179658E-07

Epoch 147 of 500
  training loss:		1.529647E-06
  validation loss:		4.417622E-06

Epoch 148 of 500
  training loss:		1.398113E-06
  validation loss:		5.040209E-06

Epoch 149 of 500
  training loss:		1.530444E-06
  validation loss:		1.469144E-06

Epoch 150 of 500
  training loss:		1.448823E-06
  validation loss:		4.730153E-07

Early stopping, val-loss increased over the last 15 epochs from 0.000410230155791 to 0.000451484028296
Training RMSE: 1.18782650921e-09
Validation RMSE: 1.18752316208e-09
