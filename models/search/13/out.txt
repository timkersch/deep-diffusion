Training network with 45280 training samples and 8490 validation samples
Epoch 1 of 500
  training loss:		8.467940E-02
  validation loss:		7.211371E-02

Epoch 2 of 500
  training loss:		5.649172E-02
  validation loss:		3.969797E-02

Epoch 3 of 500
  training loss:		2.565104E-02
  validation loss:		1.525620E-02

Epoch 4 of 500
  training loss:		1.103618E-02
  validation loss:		8.838283E-03

Epoch 5 of 500
  training loss:		8.046259E-03
  validation loss:		7.546674E-03

Epoch 6 of 500
  training loss:		7.180265E-03
  validation loss:		6.911986E-03

Epoch 7 of 500
  training loss:		6.523077E-03
  validation loss:		6.181326E-03

Epoch 8 of 500
  training loss:		5.887268E-03
  validation loss:		5.507628E-03

Epoch 9 of 500
  training loss:		5.244927E-03
  validation loss:		4.976919E-03

Epoch 10 of 500
  training loss:		4.621720E-03
  validation loss:		4.294036E-03

Epoch 11 of 500
  training loss:		4.054567E-03
  validation loss:		3.742763E-03

Epoch 12 of 500
  training loss:		3.552060E-03
  validation loss:		3.325555E-03

Epoch 13 of 500
  training loss:		3.124601E-03
  validation loss:		2.894793E-03

Epoch 14 of 500
  training loss:		2.765708E-03
  validation loss:		2.622680E-03

Epoch 15 of 500
  training loss:		2.453913E-03
  validation loss:		2.309283E-03

Epoch 16 of 500
  training loss:		2.218279E-03
  validation loss:		2.113136E-03

Epoch 17 of 500
  training loss:		2.026347E-03
  validation loss:		1.978675E-03

Epoch 18 of 500
  training loss:		1.877991E-03
  validation loss:		1.765213E-03

Epoch 19 of 500
  training loss:		1.715238E-03
  validation loss:		1.667943E-03

Epoch 20 of 500
  training loss:		1.512465E-03
  validation loss:		1.329055E-03

Epoch 21 of 500
  training loss:		1.215223E-03
  validation loss:		1.104739E-03

Epoch 22 of 500
  training loss:		9.542797E-04
  validation loss:		8.349311E-04

Epoch 23 of 500
  training loss:		7.716198E-04
  validation loss:		6.791624E-04

Epoch 24 of 500
  training loss:		6.451836E-04
  validation loss:		6.291724E-04

Epoch 25 of 500
  training loss:		5.512145E-04
  validation loss:		5.012212E-04

Epoch 26 of 500
  training loss:		4.719698E-04
  validation loss:		4.243254E-04

Epoch 27 of 500
  training loss:		4.107935E-04
  validation loss:		3.907821E-04

Epoch 28 of 500
  training loss:		3.628470E-04
  validation loss:		3.315359E-04

Epoch 29 of 500
  training loss:		3.165448E-04
  validation loss:		2.908950E-04

Epoch 30 of 500
  training loss:		2.838302E-04
  validation loss:		2.571770E-04

Epoch 31 of 500
  training loss:		2.557582E-04
  validation loss:		3.394758E-04

Epoch 32 of 500
  training loss:		2.286130E-04
  validation loss:		2.184493E-04

Epoch 33 of 500
  training loss:		2.066869E-04
  validation loss:		1.960612E-04

Epoch 34 of 500
  training loss:		1.909068E-04
  validation loss:		1.844054E-04

Epoch 35 of 500
  training loss:		1.714201E-04
  validation loss:		1.650086E-04

Epoch 36 of 500
  training loss:		1.575106E-04
  validation loss:		1.604375E-04

Epoch 37 of 500
  training loss:		1.434999E-04
  validation loss:		1.389430E-04

Epoch 38 of 500
  training loss:		1.344394E-04
  validation loss:		1.308731E-04

Epoch 39 of 500
  training loss:		1.239780E-04
  validation loss:		1.146606E-04

Epoch 40 of 500
  training loss:		1.186720E-04
  validation loss:		1.174043E-04

Epoch 41 of 500
  training loss:		1.082995E-04
  validation loss:		1.052542E-04

Epoch 42 of 500
  training loss:		1.036468E-04
  validation loss:		1.013305E-04

Epoch 43 of 500
  training loss:		9.578275E-05
  validation loss:		9.514825E-05

Epoch 44 of 500
  training loss:		8.985236E-05
  validation loss:		8.593236E-05

Epoch 45 of 500
  training loss:		8.433376E-05
  validation loss:		8.201855E-05

Epoch 46 of 500
  training loss:		7.929918E-05
  validation loss:		7.400305E-05

Epoch 47 of 500
  training loss:		7.613667E-05
  validation loss:		6.940398E-05

Epoch 48 of 500
  training loss:		7.444441E-05
  validation loss:		6.644948E-05

Epoch 49 of 500
  training loss:		6.810734E-05
  validation loss:		6.277811E-05

Epoch 50 of 500
  training loss:		6.617233E-05
  validation loss:		6.891672E-05

Epoch 51 of 500
  training loss:		6.225222E-05
  validation loss:		6.357588E-05

Epoch 52 of 500
  training loss:		6.075452E-05
  validation loss:		5.481096E-05

Epoch 53 of 500
  training loss:		5.603793E-05
  validation loss:		5.237500E-05

Epoch 54 of 500
  training loss:		5.760082E-05
  validation loss:		5.842266E-05

Epoch 55 of 500
  training loss:		5.278797E-05
  validation loss:		4.970045E-05

Epoch 56 of 500
  training loss:		5.150753E-05
  validation loss:		4.923733E-05

Epoch 57 of 500
  training loss:		4.809256E-05
  validation loss:		5.062136E-05

Epoch 58 of 500
  training loss:		4.818217E-05
  validation loss:		4.464045E-05

Epoch 59 of 500
  training loss:		4.640721E-05
  validation loss:		4.759645E-05

Epoch 60 of 500
  training loss:		4.510805E-05
  validation loss:		4.789085E-05

Epoch 61 of 500
  training loss:		4.411010E-05
  validation loss:		4.310563E-05

Epoch 62 of 500
  training loss:		4.166884E-05
  validation loss:		3.782481E-05

Epoch 63 of 500
  training loss:		4.131296E-05
  validation loss:		3.859949E-05

Epoch 64 of 500
  training loss:		3.878373E-05
  validation loss:		4.399111E-05

Epoch 65 of 500
  training loss:		3.860801E-05
  validation loss:		3.482246E-05

Epoch 66 of 500
  training loss:		3.696151E-05
  validation loss:		3.358466E-05

Epoch 67 of 500
  training loss:		3.684982E-05
  validation loss:		3.539769E-05

Epoch 68 of 500
  training loss:		3.492499E-05
  validation loss:		3.307956E-05

Epoch 69 of 500
  training loss:		3.345954E-05
  validation loss:		3.108376E-05

Epoch 70 of 500
  training loss:		3.290460E-05
  validation loss:		3.389233E-05

Epoch 71 of 500
  training loss:		3.230828E-05
  validation loss:		2.990885E-05

Epoch 72 of 500
  training loss:		3.213279E-05
  validation loss:		2.810595E-05

Epoch 73 of 500
  training loss:		3.094062E-05
  validation loss:		2.743866E-05

Epoch 74 of 500
  training loss:		2.881433E-05
  validation loss:		3.475271E-05

Epoch 75 of 500
  training loss:		2.883388E-05
  validation loss:		2.676587E-05

Epoch 76 of 500
  training loss:		2.810230E-05
  validation loss:		2.616788E-05

Epoch 77 of 500
  training loss:		2.771619E-05
  validation loss:		2.621707E-05

Epoch 78 of 500
  training loss:		2.742839E-05
  validation loss:		2.409842E-05

Epoch 79 of 500
  training loss:		2.533494E-05
  validation loss:		2.326590E-05

Epoch 80 of 500
  training loss:		2.513733E-05
  validation loss:		2.301311E-05

Epoch 81 of 500
  training loss:		2.516711E-05
  validation loss:		3.057778E-05

Epoch 82 of 500
  training loss:		2.334009E-05
  validation loss:		2.155590E-05

Epoch 83 of 500
  training loss:		2.443179E-05
  validation loss:		2.268550E-05

Epoch 84 of 500
  training loss:		2.248697E-05
  validation loss:		2.035442E-05

Epoch 85 of 500
  training loss:		2.337009E-05
  validation loss:		2.007912E-05

Epoch 86 of 500
  training loss:		2.160975E-05
  validation loss:		1.920177E-05

Epoch 87 of 500
  training loss:		2.200866E-05
  validation loss:		1.937512E-05

Epoch 88 of 500
  training loss:		2.128478E-05
  validation loss:		2.391404E-05

Epoch 89 of 500
  training loss:		2.182714E-05
  validation loss:		1.840392E-05

Epoch 90 of 500
  training loss:		1.980323E-05
  validation loss:		1.836600E-05

Epoch 91 of 500
  training loss:		1.976729E-05
  validation loss:		2.077604E-05

Epoch 92 of 500
  training loss:		1.954710E-05
  validation loss:		1.749024E-05

Epoch 93 of 500
  training loss:		1.939843E-05
  validation loss:		1.652382E-05

Epoch 94 of 500
  training loss:		1.867321E-05
  validation loss:		1.698328E-05

Epoch 95 of 500
  training loss:		1.788230E-05
  validation loss:		2.812804E-05

Epoch 96 of 500
  training loss:		1.783306E-05
  validation loss:		2.763027E-05

Epoch 97 of 500
  training loss:		1.790515E-05
  validation loss:		2.144095E-05

Epoch 98 of 500
  training loss:		1.647201E-05
  validation loss:		1.617633E-05

Epoch 99 of 500
  training loss:		1.676009E-05
  validation loss:		1.757584E-05

Epoch 100 of 500
  training loss:		1.654331E-05
  validation loss:		1.425510E-05

Epoch 101 of 500
  training loss:		1.610112E-05
  validation loss:		1.564415E-05

Epoch 102 of 500
  training loss:		1.645912E-05
  validation loss:		1.366480E-05

Epoch 103 of 500
  training loss:		1.535605E-05
  validation loss:		1.355372E-05

Epoch 104 of 500
  training loss:		1.512855E-05
  validation loss:		1.514529E-05

Epoch 105 of 500
  training loss:		1.513594E-05
  validation loss:		1.335921E-05

Epoch 106 of 500
  training loss:		1.517523E-05
  validation loss:		1.364902E-05

Epoch 107 of 500
  training loss:		1.425814E-05
  validation loss:		1.792795E-05

Epoch 108 of 500
  training loss:		1.390535E-05
  validation loss:		1.241695E-05

Epoch 109 of 500
  training loss:		1.385362E-05
  validation loss:		1.202746E-05

Epoch 110 of 500
  training loss:		1.338688E-05
  validation loss:		1.453103E-05

Epoch 111 of 500
  training loss:		1.345545E-05
  validation loss:		1.267016E-05

Epoch 112 of 500
  training loss:		1.330419E-05
  validation loss:		1.409954E-05

Epoch 113 of 500
  training loss:		1.272377E-05
  validation loss:		1.443307E-05

Epoch 114 of 500
  training loss:		1.240380E-05
  validation loss:		1.424071E-05

Epoch 115 of 500
  training loss:		1.367537E-05
  validation loss:		1.129734E-05

Epoch 116 of 500
  training loss:		1.191635E-05
  validation loss:		1.181337E-05

Epoch 117 of 500
  training loss:		1.191935E-05
  validation loss:		1.886246E-05

Epoch 118 of 500
  training loss:		1.189703E-05
  validation loss:		1.050914E-05

Epoch 119 of 500
  training loss:		1.135949E-05
  validation loss:		1.544608E-05

Epoch 120 of 500
  training loss:		1.194375E-05
  validation loss:		1.022443E-05

Epoch 121 of 500
  training loss:		1.183502E-05
  validation loss:		9.588025E-06

Epoch 122 of 500
  training loss:		1.137892E-05
  validation loss:		9.600841E-06

Epoch 123 of 500
  training loss:		1.102469E-05
  validation loss:		1.024269E-05

Epoch 124 of 500
  training loss:		1.054594E-05
  validation loss:		1.205485E-05

Epoch 125 of 500
  training loss:		1.055647E-05
  validation loss:		9.080991E-06

Epoch 126 of 500
  training loss:		1.111018E-05
  validation loss:		8.806693E-06

Epoch 127 of 500
  training loss:		1.038175E-05
  validation loss:		9.340858E-06

Epoch 128 of 500
  training loss:		1.005042E-05
  validation loss:		1.006887E-05

Epoch 129 of 500
  training loss:		9.669597E-06
  validation loss:		8.492752E-06

Epoch 130 of 500
  training loss:		9.253274E-06
  validation loss:		8.189137E-06

Epoch 131 of 500
  training loss:		9.938295E-06
  validation loss:		8.094498E-06

Epoch 132 of 500
  training loss:		9.807546E-06
  validation loss:		9.802131E-06

Epoch 133 of 500
  training loss:		9.744591E-06
  validation loss:		8.312684E-06

Epoch 134 of 500
  training loss:		9.146488E-06
  validation loss:		1.113743E-05

Epoch 135 of 500
  training loss:		9.282113E-06
  validation loss:		7.909290E-06

Epoch 136 of 500
  training loss:		9.453544E-06
  validation loss:		7.702228E-06

Epoch 137 of 500
  training loss:		8.465457E-06
  validation loss:		7.551300E-06

Epoch 138 of 500
  training loss:		8.889139E-06
  validation loss:		7.927745E-06

Epoch 139 of 500
  training loss:		8.620143E-06
  validation loss:		1.291734E-05

Epoch 140 of 500
  training loss:		8.639924E-06
  validation loss:		1.003113E-05

Epoch 141 of 500
  training loss:		8.082086E-06
  validation loss:		8.390596E-06

Epoch 142 of 500
  training loss:		8.378315E-06
  validation loss:		9.908519E-06

Epoch 143 of 500
  training loss:		8.255628E-06
  validation loss:		8.721784E-06

Epoch 144 of 500
  training loss:		7.529587E-06
  validation loss:		6.729918E-06

Epoch 145 of 500
  training loss:		8.263691E-06
  validation loss:		7.238881E-06

Epoch 146 of 500
  training loss:		8.158856E-06
  validation loss:		8.006599E-06

Epoch 147 of 500
  training loss:		7.847777E-06
  validation loss:		1.109895E-05

Epoch 148 of 500
  training loss:		7.191391E-06
  validation loss:		6.658125E-06

Epoch 149 of 500
  training loss:		7.249370E-06
  validation loss:		6.506452E-06

Epoch 150 of 500
  training loss:		7.182905E-06
  validation loss:		1.596928E-05

Epoch 151 of 500
  training loss:		7.395190E-06
  validation loss:		7.047373E-06

Epoch 152 of 500
  training loss:		7.067656E-06
  validation loss:		7.431449E-06

Epoch 153 of 500
  training loss:		7.189305E-06
  validation loss:		5.644391E-06

Epoch 154 of 500
  training loss:		6.641171E-06
  validation loss:		8.420009E-06

Epoch 155 of 500
  training loss:		7.099648E-06
  validation loss:		5.464798E-06

Epoch 156 of 500
  training loss:		7.189932E-06
  validation loss:		6.452950E-06

Epoch 157 of 500
  training loss:		7.431606E-06
  validation loss:		8.863618E-06

Epoch 158 of 500
  training loss:		6.670087E-06
  validation loss:		9.538647E-06

Epoch 159 of 500
  training loss:		6.086429E-06
  validation loss:		6.676051E-06

Epoch 160 of 500
  training loss:		6.314502E-06
  validation loss:		5.463697E-06

Epoch 161 of 500
  training loss:		6.168605E-06
  validation loss:		5.021261E-06

Epoch 162 of 500
  training loss:		6.552475E-06
  validation loss:		5.789076E-06

Epoch 163 of 500
  training loss:		6.242164E-06
  validation loss:		4.827789E-06

Epoch 164 of 500
  training loss:		5.835085E-06
  validation loss:		5.435892E-06

Epoch 165 of 500
  training loss:		6.089185E-06
  validation loss:		5.340605E-06

Epoch 166 of 500
  training loss:		5.876318E-06
  validation loss:		4.663320E-06

Epoch 167 of 500
  training loss:		5.540445E-06
  validation loss:		4.870282E-06

Epoch 168 of 500
  training loss:		5.795751E-06
  validation loss:		5.428930E-06

Epoch 169 of 500
  training loss:		5.385966E-06
  validation loss:		7.729502E-06

Epoch 170 of 500
  training loss:		5.890438E-06
  validation loss:		6.304700E-06

Epoch 171 of 500
  training loss:		5.378220E-06
  validation loss:		4.302531E-06

Epoch 172 of 500
  training loss:		5.663499E-06
  validation loss:		8.522038E-06

Epoch 173 of 500
  training loss:		5.265216E-06
  validation loss:		4.344929E-06

Epoch 174 of 500
  training loss:		5.369335E-06
  validation loss:		4.844033E-06

Epoch 175 of 500
  training loss:		5.172778E-06
  validation loss:		5.677667E-06

Epoch 176 of 500
  training loss:		5.399120E-06
  validation loss:		4.039268E-06

Epoch 177 of 500
  training loss:		5.002545E-06
  validation loss:		5.750078E-06

Epoch 178 of 500
  training loss:		4.882096E-06
  validation loss:		7.336747E-06

Epoch 179 of 500
  training loss:		4.837972E-06
  validation loss:		4.423063E-06

Epoch 180 of 500
  training loss:		4.967650E-06
  validation loss:		4.472735E-06

Epoch 181 of 500
  training loss:		4.827742E-06
  validation loss:		5.296805E-06

Epoch 182 of 500
  training loss:		4.714348E-06
  validation loss:		3.632583E-06

Epoch 183 of 500
  training loss:		4.985800E-06
  validation loss:		3.974010E-06

Epoch 184 of 500
  training loss:		4.597740E-06
  validation loss:		6.437537E-06

Epoch 185 of 500
  training loss:		4.721287E-06
  validation loss:		4.336319E-06

Epoch 186 of 500
  training loss:		4.498793E-06
  validation loss:		3.455554E-06

Epoch 187 of 500
  training loss:		4.470445E-06
  validation loss:		3.405846E-06

Epoch 188 of 500
  training loss:		4.493551E-06
  validation loss:		9.141390E-06

Epoch 189 of 500
  training loss:		4.546671E-06
  validation loss:		3.380214E-06

Epoch 190 of 500
  training loss:		4.008682E-06
  validation loss:		3.280114E-06

Epoch 191 of 500
  training loss:		4.422921E-06
  validation loss:		6.614283E-06

Epoch 192 of 500
  training loss:		4.069350E-06
  validation loss:		4.226054E-06

Epoch 193 of 500
  training loss:		4.401275E-06
  validation loss:		3.313841E-06

Epoch 194 of 500
  training loss:		3.846258E-06
  validation loss:		3.106013E-06

Epoch 195 of 500
  training loss:		4.083189E-06
  validation loss:		5.695166E-06

Epoch 196 of 500
  training loss:		3.947855E-06
  validation loss:		2.963531E-06

Epoch 197 of 500
  training loss:		4.118801E-06
  validation loss:		3.676540E-06

Epoch 198 of 500
  training loss:		3.917954E-06
  validation loss:		2.869376E-06

Epoch 199 of 500
  training loss:		3.953222E-06
  validation loss:		3.966167E-06

Epoch 200 of 500
  training loss:		3.853602E-06
  validation loss:		2.918077E-06

Epoch 201 of 500
  training loss:		3.849800E-06
  validation loss:		2.807106E-06

Epoch 202 of 500
  training loss:		3.621934E-06
  validation loss:		2.785942E-06

Epoch 203 of 500
  training loss:		3.534100E-06
  validation loss:		2.822152E-06

Epoch 204 of 500
  training loss:		3.992754E-06
  validation loss:		2.695416E-06

Epoch 205 of 500
  training loss:		3.481693E-06
  validation loss:		2.627738E-06

Epoch 206 of 500
  training loss:		3.457075E-06
  validation loss:		3.533096E-06

Epoch 207 of 500
  training loss:		3.236350E-06
  validation loss:		2.790483E-06

Epoch 208 of 500
  training loss:		3.558226E-06
  validation loss:		2.530956E-06

Epoch 209 of 500
  training loss:		3.689808E-06
  validation loss:		3.248339E-06

Epoch 210 of 500
  training loss:		3.629007E-06
  validation loss:		2.541460E-06

Epoch 211 of 500
  training loss:		3.564298E-06
  validation loss:		5.423148E-06

Epoch 212 of 500
  training loss:		3.275241E-06
  validation loss:		5.221194E-06

Epoch 213 of 500
  training loss:		3.430582E-06
  validation loss:		6.321548E-06

Epoch 214 of 500
  training loss:		3.381438E-06
  validation loss:		2.359521E-06

Epoch 215 of 500
  training loss:		2.905482E-06
  validation loss:		4.247920E-06

Epoch 216 of 500
  training loss:		3.387613E-06
  validation loss:		2.863556E-06

Epoch 217 of 500
  training loss:		3.215660E-06
  validation loss:		4.128735E-06

Epoch 218 of 500
  training loss:		2.966132E-06
  validation loss:		2.789520E-06

Epoch 219 of 500
  training loss:		3.264011E-06
  validation loss:		2.396828E-06

Epoch 220 of 500
  training loss:		2.973885E-06
  validation loss:		3.196747E-06

Early stopping, val-loss increased over the last 10 epochs from 0.000187325747622 to 0.000257061534198
Training RMSE: 1.50330374473e-09
Validation RMSE: 1.51529853851e-09
