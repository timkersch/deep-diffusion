Epoch 1 of 500
  training loss:		1.430824E-01
  validation loss:		8.099625E-02

Epoch 2 of 500
  training loss:		7.040514E-02
  validation loss:		5.983462E-02

Epoch 3 of 500
  training loss:		4.935242E-02
  validation loss:		3.984133E-02

Epoch 4 of 500
  training loss:		3.170099E-02
  validation loss:		2.455967E-02

Epoch 5 of 500
  training loss:		1.903922E-02
  validation loss:		1.440555E-02

Epoch 6 of 500
  training loss:		1.127393E-02
  validation loss:		8.745928E-03

Epoch 7 of 500
  training loss:		7.243451E-03
  validation loss:		6.050485E-03

Epoch 8 of 500
  training loss:		5.424927E-03
  validation loss:		4.900547E-03

Epoch 9 of 500
  training loss:		4.688110E-03
  validation loss:		4.457636E-03

Epoch 10 of 500
  training loss:		4.359943E-03
  validation loss:		4.220197E-03

Epoch 11 of 500
  training loss:		4.176262E-03
  validation loss:		4.055799E-03

Epoch 12 of 500
  training loss:		4.012984E-03
  validation loss:		3.891384E-03

Epoch 13 of 500
  training loss:		3.852965E-03
  validation loss:		3.740348E-03

Epoch 14 of 500
  training loss:		3.689262E-03
  validation loss:		3.572023E-03

Epoch 15 of 500
  training loss:		3.523407E-03
  validation loss:		3.393417E-03

Epoch 16 of 500
  training loss:		3.355500E-03
  validation loss:		3.244676E-03

Epoch 17 of 500
  training loss:		3.198000E-03
  validation loss:		3.078130E-03

Epoch 18 of 500
  training loss:		3.046291E-03
  validation loss:		2.934374E-03

Epoch 19 of 500
  training loss:		2.916590E-03
  validation loss:		2.803709E-03

Epoch 20 of 500
  training loss:		2.788234E-03
  validation loss:		2.687930E-03

Epoch 21 of 500
  training loss:		2.676640E-03
  validation loss:		2.577776E-03

Epoch 22 of 500
  training loss:		2.574849E-03
  validation loss:		2.484940E-03

Epoch 23 of 500
  training loss:		2.487039E-03
  validation loss:		2.426610E-03

Epoch 24 of 500
  training loss:		2.404466E-03
  validation loss:		2.325693E-03

Epoch 25 of 500
  training loss:		2.328882E-03
  validation loss:		2.253147E-03

Epoch 26 of 500
  training loss:		2.267192E-03
  validation loss:		2.187633E-03

Epoch 27 of 500
  training loss:		2.197201E-03
  validation loss:		2.126064E-03

Epoch 28 of 500
  training loss:		2.134927E-03
  validation loss:		2.087259E-03

Epoch 29 of 500
  training loss:		2.060241E-03
  validation loss:		1.969651E-03

Epoch 30 of 500
  training loss:		1.937475E-03
  validation loss:		1.848287E-03

Epoch 31 of 500
  training loss:		1.707570E-03
  validation loss:		1.543058E-03

Epoch 32 of 500
  training loss:		1.450977E-03
  validation loss:		1.263922E-03

Epoch 33 of 500
  training loss:		1.157434E-03
  validation loss:		1.031047E-03

Epoch 34 of 500
  training loss:		9.345161E-04
  validation loss:		8.209517E-04

Epoch 35 of 500
  training loss:		7.635751E-04
  validation loss:		6.865929E-04

Epoch 36 of 500
  training loss:		6.388721E-04
  validation loss:		5.822509E-04

Epoch 37 of 500
  training loss:		5.540515E-04
  validation loss:		5.078468E-04

Epoch 38 of 500
  training loss:		4.891276E-04
  validation loss:		4.560961E-04

Epoch 39 of 500
  training loss:		4.394029E-04
  validation loss:		4.111292E-04

Epoch 40 of 500
  training loss:		4.087188E-04
  validation loss:		3.851046E-04

Epoch 41 of 500
  training loss:		3.675015E-04
  validation loss:		3.480919E-04

Epoch 42 of 500
  training loss:		3.455181E-04
  validation loss:		3.215413E-04

Epoch 43 of 500
  training loss:		3.190852E-04
  validation loss:		3.031928E-04

Epoch 44 of 500
  training loss:		2.969151E-04
  validation loss:		2.809836E-04

Epoch 45 of 500
  training loss:		2.770985E-04
  validation loss:		2.627153E-04

Epoch 46 of 500
  training loss:		2.621227E-04
  validation loss:		2.467417E-04

Epoch 47 of 500
  training loss:		2.459264E-04
  validation loss:		2.320508E-04

Epoch 48 of 500
  training loss:		2.302739E-04
  validation loss:		2.170848E-04

Epoch 49 of 500
  training loss:		2.189596E-04
  validation loss:		2.060297E-04

Epoch 50 of 500
  training loss:		2.064432E-04
  validation loss:		1.954916E-04

Epoch 51 of 500
  training loss:		1.954849E-04
  validation loss:		1.839927E-04

Epoch 52 of 500
  training loss:		1.887644E-04
  validation loss:		1.747339E-04

Epoch 53 of 500
  training loss:		1.764216E-04
  validation loss:		1.660544E-04

Epoch 54 of 500
  training loss:		1.698142E-04
  validation loss:		1.584248E-04

Epoch 55 of 500
  training loss:		1.603924E-04
  validation loss:		1.549762E-04

Epoch 56 of 500
  training loss:		1.564791E-04
  validation loss:		1.462357E-04

Epoch 57 of 500
  training loss:		1.451302E-04
  validation loss:		1.383490E-04

Epoch 58 of 500
  training loss:		1.404391E-04
  validation loss:		1.364925E-04

Epoch 59 of 500
  training loss:		1.343298E-04
  validation loss:		1.265022E-04

Epoch 60 of 500
  training loss:		1.319564E-04
  validation loss:		1.243391E-04

Epoch 61 of 500
  training loss:		1.235260E-04
  validation loss:		1.165784E-04

Epoch 62 of 500
  training loss:		1.193687E-04
  validation loss:		1.388607E-04

Epoch 63 of 500
  training loss:		1.154693E-04
  validation loss:		1.076059E-04

Epoch 64 of 500
  training loss:		1.130457E-04
  validation loss:		1.055755E-04

Epoch 65 of 500
  training loss:		1.077875E-04
  validation loss:		1.000258E-04

Epoch 66 of 500
  training loss:		1.035288E-04
  validation loss:		1.024412E-04

Epoch 67 of 500
  training loss:		1.006088E-04
  validation loss:		1.000432E-04

Epoch 68 of 500
  training loss:		9.672271E-05
  validation loss:		9.090170E-05

Epoch 69 of 500
  training loss:		9.436107E-05
  validation loss:		8.841302E-05

Epoch 70 of 500
  training loss:		9.141659E-05
  validation loss:		8.477491E-05

Epoch 71 of 500
  training loss:		8.836398E-05
  validation loss:		8.352991E-05

Epoch 72 of 500
  training loss:		8.742827E-05
  validation loss:		7.973168E-05

Epoch 73 of 500
  training loss:		8.428404E-05
  validation loss:		8.238404E-05

Epoch 74 of 500
  training loss:		8.109028E-05
  validation loss:		7.512460E-05

Epoch 75 of 500
  training loss:		8.112674E-05
  validation loss:		8.307740E-05

Epoch 76 of 500
  training loss:		7.667545E-05
  validation loss:		7.553584E-05

Epoch 77 of 500
  training loss:		7.407513E-05
  validation loss:		8.164668E-05

Epoch 78 of 500
  training loss:		7.179963E-05
  validation loss:		6.670892E-05

Epoch 79 of 500
  training loss:		7.136813E-05
  validation loss:		6.483936E-05

Epoch 80 of 500
  training loss:		6.762994E-05
  validation loss:		6.604982E-05

Epoch 81 of 500
  training loss:		6.530790E-05
  validation loss:		6.274665E-05

Epoch 82 of 500
  training loss:		6.635993E-05
  validation loss:		6.178820E-05

Epoch 83 of 500
  training loss:		6.348227E-05
  validation loss:		5.978379E-05

Epoch 84 of 500
  training loss:		6.070189E-05
  validation loss:		6.383309E-05

Epoch 85 of 500
  training loss:		6.115993E-05
  validation loss:		5.558350E-05

Epoch 86 of 500
  training loss:		5.625938E-05
  validation loss:		5.320865E-05

Epoch 87 of 500
  training loss:		5.553382E-05
  validation loss:		5.454438E-05

Epoch 88 of 500
  training loss:		5.417243E-05
  validation loss:		5.041167E-05

Epoch 89 of 500
  training loss:		5.301963E-05
  validation loss:		4.997923E-05

Epoch 90 of 500
  training loss:		5.095762E-05
  validation loss:		4.764028E-05

Epoch 91 of 500
  training loss:		4.921076E-05
  validation loss:		4.610731E-05

Epoch 92 of 500
  training loss:		5.021316E-05
  validation loss:		4.731701E-05

Epoch 93 of 500
  training loss:		4.668817E-05
  validation loss:		4.890546E-05

Epoch 94 of 500
  training loss:		4.581348E-05
  validation loss:		4.287618E-05

Epoch 95 of 500
  training loss:		4.450319E-05
  validation loss:		4.858480E-05

Epoch 96 of 500
  training loss:		4.356048E-05
  validation loss:		4.059284E-05

Epoch 97 of 500
  training loss:		4.395179E-05
  validation loss:		4.606588E-05

Epoch 98 of 500
  training loss:		4.241862E-05
  validation loss:		3.982226E-05

Epoch 99 of 500
  training loss:		3.986459E-05
  validation loss:		3.743298E-05

Epoch 100 of 500
  training loss:		3.901790E-05
  validation loss:		3.739998E-05

Epoch 101 of 500
  training loss:		3.857811E-05
  validation loss:		3.494210E-05

Epoch 102 of 500
  training loss:		3.611744E-05
  validation loss:		3.673293E-05

Epoch 103 of 500
  training loss:		3.596842E-05
  validation loss:		3.344565E-05

Epoch 104 of 500
  training loss:		3.609684E-05
  validation loss:		4.068185E-05

Epoch 105 of 500
  training loss:		3.453563E-05
  validation loss:		3.611469E-05

Epoch 106 of 500
  training loss:		3.375274E-05
  validation loss:		3.065563E-05

Epoch 107 of 500
  training loss:		3.223170E-05
  validation loss:		2.994287E-05

Epoch 108 of 500
  training loss:		3.233261E-05
  validation loss:		3.659977E-05

Epoch 109 of 500
  training loss:		3.241531E-05
  validation loss:		2.853360E-05

Epoch 110 of 500
  training loss:		3.061960E-05
  validation loss:		2.868849E-05

Epoch 111 of 500
  training loss:		2.949324E-05
  validation loss:		2.730773E-05

Epoch 112 of 500
  training loss:		2.906696E-05
  validation loss:		2.669587E-05

Epoch 113 of 500
  training loss:		2.790135E-05
  validation loss:		4.108753E-05

Epoch 114 of 500
  training loss:		2.862213E-05
  validation loss:		2.534835E-05

Epoch 115 of 500
  training loss:		2.765506E-05
  validation loss:		2.758792E-05

Epoch 116 of 500
  training loss:		2.703277E-05
  validation loss:		2.665612E-05

Epoch 117 of 500
  training loss:		2.758572E-05
  validation loss:		2.384812E-05

Epoch 118 of 500
  training loss:		2.582036E-05
  validation loss:		2.358065E-05

Epoch 119 of 500
  training loss:		2.690020E-05
  validation loss:		2.504396E-05

Epoch 120 of 500
  training loss:		2.464859E-05
  validation loss:		3.304375E-05

Epoch 121 of 500
  training loss:		2.528338E-05
  validation loss:		2.168511E-05

Epoch 122 of 500
  training loss:		2.318096E-05
  validation loss:		2.183127E-05

Epoch 123 of 500
  training loss:		2.302079E-05
  validation loss:		2.219647E-05

Epoch 124 of 500
  training loss:		2.190298E-05
  validation loss:		2.393544E-05

Epoch 125 of 500
  training loss:		2.289687E-05
  validation loss:		2.766455E-05

Epoch 126 of 500
  training loss:		2.177225E-05
  validation loss:		1.985706E-05

Epoch 127 of 500
  training loss:		2.138224E-05
  validation loss:		1.939456E-05

Epoch 128 of 500
  training loss:		2.184789E-05
  validation loss:		2.389954E-05

Epoch 129 of 500
  training loss:		2.134696E-05
  validation loss:		1.913886E-05

Epoch 130 of 500
  training loss:		2.006383E-05
  validation loss:		1.825405E-05

Epoch 131 of 500
  training loss:		2.046364E-05
  validation loss:		2.757496E-05

Epoch 132 of 500
  training loss:		1.960966E-05
  validation loss:		1.770800E-05

Epoch 133 of 500
  training loss:		1.976846E-05
  validation loss:		2.178146E-05

Epoch 134 of 500
  training loss:		1.937418E-05
  validation loss:		3.145974E-05

Epoch 135 of 500
  training loss:		1.990605E-05
  validation loss:		2.803611E-05

Early stopping, val-loss increased over the last 5 epochs from 0.000884787708851 to 0.00111373034614
Training-set, RMSE: 5.55495493638e-09
Validation-set, RMSE: 5.49986167135e-09
