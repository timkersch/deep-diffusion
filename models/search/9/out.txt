Training network with 45280 training samples and 8490 validation samples
Epoch 1 of 500
  training loss:		6.263382E-02
  validation loss:		2.291761E-02

Epoch 2 of 500
  training loss:		1.221745E-02
  validation loss:		8.709087E-03

Epoch 3 of 500
  training loss:		7.824118E-03
  validation loss:		6.870855E-03

Epoch 4 of 500
  training loss:		6.132941E-03
  validation loss:		5.195288E-03

Epoch 5 of 500
  training loss:		4.495496E-03
  validation loss:		3.727878E-03

Epoch 6 of 500
  training loss:		3.233303E-03
  validation loss:		2.635452E-03

Epoch 7 of 500
  training loss:		2.129719E-03
  validation loss:		1.505104E-03

Epoch 8 of 500
  training loss:		1.220247E-03
  validation loss:		9.097958E-04

Epoch 9 of 500
  training loss:		7.926119E-04
  validation loss:		6.132071E-04

Epoch 10 of 500
  training loss:		5.538747E-04
  validation loss:		4.447261E-04

Epoch 11 of 500
  training loss:		3.999453E-04
  validation loss:		3.378875E-04

Epoch 12 of 500
  training loss:		2.960609E-04
  validation loss:		2.436687E-04

Epoch 13 of 500
  training loss:		2.377626E-04
  validation loss:		2.153469E-04

Epoch 14 of 500
  training loss:		1.873409E-04
  validation loss:		1.706534E-04

Epoch 15 of 500
  training loss:		1.555475E-04
  validation loss:		1.298682E-04

Epoch 16 of 500
  training loss:		1.317882E-04
  validation loss:		1.100012E-04

Epoch 17 of 500
  training loss:		1.117071E-04
  validation loss:		1.373603E-04

Epoch 18 of 500
  training loss:		9.863222E-05
  validation loss:		7.967791E-05

Epoch 19 of 500
  training loss:		8.366891E-05
  validation loss:		7.196195E-05

Epoch 20 of 500
  training loss:		7.244346E-05
  validation loss:		6.517199E-05

Epoch 21 of 500
  training loss:		6.806705E-05
  validation loss:		6.340546E-05

Epoch 22 of 500
  training loss:		6.352593E-05
  validation loss:		6.901295E-05

Epoch 23 of 500
  training loss:		5.703914E-05
  validation loss:		4.800510E-05

Epoch 24 of 500
  training loss:		5.376597E-05
  validation loss:		4.798064E-05

Epoch 25 of 500
  training loss:		4.901907E-05
  validation loss:		4.197556E-05

Epoch 26 of 500
  training loss:		4.540841E-05
  validation loss:		3.783020E-05

Epoch 27 of 500
  training loss:		4.170057E-05
  validation loss:		4.734611E-05

Epoch 28 of 500
  training loss:		3.929217E-05
  validation loss:		3.876147E-05

Epoch 29 of 500
  training loss:		3.836943E-05
  validation loss:		2.804136E-05

Epoch 30 of 500
  training loss:		3.499843E-05
  validation loss:		2.576341E-05

Epoch 31 of 500
  training loss:		3.256358E-05
  validation loss:		3.003004E-05

Epoch 32 of 500
  training loss:		3.079410E-05
  validation loss:		2.387241E-05

Epoch 33 of 500
  training loss:		2.824592E-05
  validation loss:		2.344215E-05

Epoch 34 of 500
  training loss:		2.786999E-05
  validation loss:		2.168246E-05

Epoch 35 of 500
  training loss:		2.568103E-05
  validation loss:		1.905548E-05

Epoch 36 of 500
  training loss:		2.575125E-05
  validation loss:		1.800678E-05

Epoch 37 of 500
  training loss:		2.509298E-05
  validation loss:		1.963584E-05

Epoch 38 of 500
  training loss:		2.324494E-05
  validation loss:		2.976430E-05

Epoch 39 of 500
  training loss:		2.269427E-05
  validation loss:		2.129308E-05

Epoch 40 of 500
  training loss:		2.024223E-05
  validation loss:		1.548025E-05

Epoch 41 of 500
  training loss:		1.949675E-05
  validation loss:		2.776068E-05

Epoch 42 of 500
  training loss:		1.880258E-05
  validation loss:		3.285555E-05

Epoch 43 of 500
  training loss:		2.015852E-05
  validation loss:		2.281868E-05

Epoch 44 of 500
  training loss:		1.824294E-05
  validation loss:		1.678390E-05

Epoch 45 of 500
  training loss:		1.808512E-05
  validation loss:		1.542102E-05

Epoch 46 of 500
  training loss:		1.598604E-05
  validation loss:		1.591967E-05

Epoch 47 of 500
  training loss:		1.528618E-05
  validation loss:		1.927775E-05

Epoch 48 of 500
  training loss:		1.577903E-05
  validation loss:		1.275929E-05

Epoch 49 of 500
  training loss:		1.503649E-05
  validation loss:		2.664682E-05

Epoch 50 of 500
  training loss:		1.519756E-05
  validation loss:		1.099422E-05

Epoch 51 of 500
  training loss:		1.260046E-05
  validation loss:		9.234760E-06

Epoch 52 of 500
  training loss:		1.385025E-05
  validation loss:		9.055753E-06

Epoch 53 of 500
  training loss:		1.270514E-05
  validation loss:		8.694494E-06

Epoch 54 of 500
  training loss:		1.344023E-05
  validation loss:		9.245390E-06

Epoch 55 of 500
  training loss:		1.234005E-05
  validation loss:		2.247307E-05

Epoch 56 of 500
  training loss:		1.202674E-05
  validation loss:		1.751743E-05

Epoch 57 of 500
  training loss:		1.014083E-05
  validation loss:		7.179546E-06

Epoch 58 of 500
  training loss:		1.171048E-05
  validation loss:		7.651670E-06

Epoch 59 of 500
  training loss:		1.053622E-05
  validation loss:		2.190634E-05

Epoch 60 of 500
  training loss:		1.106029E-05
  validation loss:		2.728336E-05

Epoch 61 of 500
  training loss:		9.505137E-06
  validation loss:		1.069832E-05

Epoch 62 of 500
  training loss:		9.952973E-06
  validation loss:		5.949683E-06

Epoch 63 of 500
  training loss:		9.376148E-06
  validation loss:		6.849690E-06

Epoch 64 of 500
  training loss:		9.228708E-06
  validation loss:		5.332607E-06

Epoch 65 of 500
  training loss:		8.760527E-06
  validation loss:		5.525907E-06

Epoch 66 of 500
  training loss:		9.262780E-06
  validation loss:		9.289884E-06

Epoch 67 of 500
  training loss:		8.110655E-06
  validation loss:		1.347423E-05

Epoch 68 of 500
  training loss:		8.474761E-06
  validation loss:		1.232384E-05

Epoch 69 of 500
  training loss:		7.724131E-06
  validation loss:		5.068816E-06

Epoch 70 of 500
  training loss:		8.299036E-06
  validation loss:		8.061871E-06

Epoch 71 of 500
  training loss:		8.270677E-06
  validation loss:		9.916967E-06

Epoch 72 of 500
  training loss:		6.814728E-06
  validation loss:		4.846545E-06

Epoch 73 of 500
  training loss:		7.633498E-06
  validation loss:		1.178547E-05

Epoch 74 of 500
  training loss:		7.895528E-06
  validation loss:		3.686046E-06

Epoch 75 of 500
  training loss:		7.341052E-06
  validation loss:		1.275893E-05

Epoch 76 of 500
  training loss:		6.015038E-06
  validation loss:		3.696986E-06

Epoch 77 of 500
  training loss:		6.207080E-06
  validation loss:		6.441198E-06

Epoch 78 of 500
  training loss:		6.735950E-06
  validation loss:		3.626683E-06

Epoch 79 of 500
  training loss:		6.156559E-06
  validation loss:		3.155253E-06

Epoch 80 of 500
  training loss:		5.965053E-06
  validation loss:		1.701708E-05

Epoch 81 of 500
  training loss:		5.962802E-06
  validation loss:		3.130228E-06

Epoch 82 of 500
  training loss:		6.490940E-06
  validation loss:		6.711743E-06

Epoch 83 of 500
  training loss:		5.950168E-06
  validation loss:		2.778039E-06

Epoch 84 of 500
  training loss:		5.859206E-06
  validation loss:		2.608721E-06

Epoch 85 of 500
  training loss:		4.995875E-06
  validation loss:		6.940672E-06

Epoch 86 of 500
  training loss:		5.981771E-06
  validation loss:		2.699424E-06

Epoch 87 of 500
  training loss:		5.814964E-06
  validation loss:		2.736001E-06

Epoch 88 of 500
  training loss:		4.624752E-06
  validation loss:		3.743949E-06

Epoch 89 of 500
  training loss:		5.475645E-06
  validation loss:		2.238949E-06

Epoch 90 of 500
  training loss:		5.215410E-06
  validation loss:		4.913173E-06

Epoch 91 of 500
  training loss:		5.172528E-06
  validation loss:		2.081007E-06

Epoch 92 of 500
  training loss:		4.787306E-06
  validation loss:		4.625013E-06

Epoch 93 of 500
  training loss:		5.332428E-06
  validation loss:		3.657004E-06

Epoch 94 of 500
  training loss:		4.499943E-06
  validation loss:		4.995449E-06

Epoch 95 of 500
  training loss:		4.976146E-06
  validation loss:		1.363459E-05

Epoch 96 of 500
  training loss:		4.801653E-06
  validation loss:		7.618498E-06

Epoch 97 of 500
  training loss:		4.973918E-06
  validation loss:		3.629638E-06

Epoch 98 of 500
  training loss:		4.202078E-06
  validation loss:		4.827906E-06

Epoch 99 of 500
  training loss:		4.490820E-06
  validation loss:		1.700572E-06

Epoch 100 of 500
  training loss:		4.356367E-06
  validation loss:		1.251015E-05

Early stopping, val-loss increased over the last 10 epochs from 0.00050821185593 to 0.00078249363821
Training RMSE: 1.26948707382e-09
Validation RMSE: 1.27939903185e-09
