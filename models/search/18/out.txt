Epoch 1 of 500
  training loss:		8.973559E-02
  validation loss:		7.323417E-02

Epoch 2 of 500
  training loss:		4.435702E-02
  validation loss:		1.799380E-02

Epoch 3 of 500
  training loss:		9.105189E-03
  validation loss:		5.191517E-03

Epoch 4 of 500
  training loss:		4.706673E-03
  validation loss:		4.378327E-03

Epoch 5 of 500
  training loss:		4.195634E-03
  validation loss:		3.939584E-03

Epoch 6 of 500
  training loss:		3.764409E-03
  validation loss:		3.496097E-03

Epoch 7 of 500
  training loss:		3.359033E-03
  validation loss:		3.109057E-03

Epoch 8 of 500
  training loss:		3.004494E-03
  validation loss:		2.882784E-03

Epoch 9 of 500
  training loss:		2.725818E-03
  validation loss:		2.560105E-03

Epoch 10 of 500
  training loss:		2.530103E-03
  validation loss:		2.414622E-03

Epoch 11 of 500
  training loss:		2.358341E-03
  validation loss:		2.297790E-03

Epoch 12 of 500
  training loss:		2.221974E-03
  validation loss:		2.121870E-03

Epoch 13 of 500
  training loss:		1.993731E-03
  validation loss:		1.746203E-03

Epoch 14 of 500
  training loss:		1.533428E-03
  validation loss:		1.245424E-03

Epoch 15 of 500
  training loss:		1.031182E-03
  validation loss:		8.269692E-04

Epoch 16 of 500
  training loss:		7.287992E-04
  validation loss:		6.210455E-04

Epoch 17 of 500
  training loss:		5.615711E-04
  validation loss:		4.909614E-04

Epoch 18 of 500
  training loss:		4.621050E-04
  validation loss:		4.179479E-04

Epoch 19 of 500
  training loss:		3.973255E-04
  validation loss:		3.997133E-04

Epoch 20 of 500
  training loss:		3.520696E-04
  validation loss:		3.414888E-04

Epoch 21 of 500
  training loss:		3.209478E-04
  validation loss:		2.878285E-04

Epoch 22 of 500
  training loss:		2.835740E-04
  validation loss:		2.632706E-04

Epoch 23 of 500
  training loss:		2.561441E-04
  validation loss:		2.505612E-04

Epoch 24 of 500
  training loss:		2.333102E-04
  validation loss:		2.265856E-04

Epoch 25 of 500
  training loss:		2.164800E-04
  validation loss:		1.994291E-04

Epoch 26 of 500
  training loss:		1.999744E-04
  validation loss:		1.847320E-04

Epoch 27 of 500
  training loss:		1.861747E-04
  validation loss:		1.745555E-04

Epoch 28 of 500
  training loss:		1.729108E-04
  validation loss:		1.590432E-04

Epoch 29 of 500
  training loss:		1.620560E-04
  validation loss:		1.482374E-04

Epoch 30 of 500
  training loss:		1.518198E-04
  validation loss:		1.474311E-04

Epoch 31 of 500
  training loss:		1.429087E-04
  validation loss:		1.404705E-04

Epoch 32 of 500
  training loss:		1.331574E-04
  validation loss:		1.403386E-04

Epoch 33 of 500
  training loss:		1.281416E-04
  validation loss:		1.324936E-04

Epoch 34 of 500
  training loss:		1.211093E-04
  validation loss:		1.143090E-04

Epoch 35 of 500
  training loss:		1.182919E-04
  validation loss:		1.060997E-04

Epoch 36 of 500
  training loss:		1.108632E-04
  validation loss:		1.014963E-04

Epoch 37 of 500
  training loss:		1.043599E-04
  validation loss:		1.001131E-04

Epoch 38 of 500
  training loss:		1.004182E-04
  validation loss:		1.004217E-04

Epoch 39 of 500
  training loss:		9.856007E-05
  validation loss:		9.027477E-05

Epoch 40 of 500
  training loss:		9.400109E-05
  validation loss:		8.461695E-05

Epoch 41 of 500
  training loss:		8.800348E-05
  validation loss:		8.706240E-05

Epoch 42 of 500
  training loss:		8.579291E-05
  validation loss:		8.776035E-05

Epoch 43 of 500
  training loss:		8.379951E-05
  validation loss:		7.569714E-05

Epoch 44 of 500
  training loss:		8.074426E-05
  validation loss:		7.285620E-05

Epoch 45 of 500
  training loss:		8.192853E-05
  validation loss:		7.296262E-05

Epoch 46 of 500
  training loss:		7.300774E-05
  validation loss:		6.689622E-05

Epoch 47 of 500
  training loss:		7.305635E-05
  validation loss:		6.676036E-05

Epoch 48 of 500
  training loss:		6.742139E-05
  validation loss:		6.221876E-05

Epoch 49 of 500
  training loss:		6.708032E-05
  validation loss:		6.245532E-05

Epoch 50 of 500
  training loss:		7.008396E-05
  validation loss:		9.814192E-05

Epoch 51 of 500
  training loss:		6.124983E-05
  validation loss:		5.663499E-05

Epoch 52 of 500
  training loss:		6.401806E-05
  validation loss:		6.592826E-05

Epoch 53 of 500
  training loss:		5.825834E-05
  validation loss:		5.172175E-05

Epoch 54 of 500
  training loss:		5.502018E-05
  validation loss:		4.989645E-05

Epoch 55 of 500
  training loss:		5.598231E-05
  validation loss:		5.320797E-05

Epoch 56 of 500
  training loss:		5.202765E-05
  validation loss:		5.175789E-05

Epoch 57 of 500
  training loss:		4.984065E-05
  validation loss:		4.692501E-05

Epoch 58 of 500
  training loss:		4.878939E-05
  validation loss:		4.705997E-05

Epoch 59 of 500
  training loss:		4.633205E-05
  validation loss:		4.298282E-05

Epoch 60 of 500
  training loss:		4.570484E-05
  validation loss:		4.051209E-05

Epoch 61 of 500
  training loss:		4.490682E-05
  validation loss:		4.160032E-05

Epoch 62 of 500
  training loss:		4.139199E-05
  validation loss:		4.851901E-05

Epoch 63 of 500
  training loss:		4.139924E-05
  validation loss:		3.670492E-05

Epoch 64 of 500
  training loss:		4.070819E-05
  validation loss:		4.399090E-05

Epoch 65 of 500
  training loss:		3.868246E-05
  validation loss:		3.467769E-05

Epoch 66 of 500
  training loss:		3.650652E-05
  validation loss:		3.258198E-05

Epoch 67 of 500
  training loss:		3.767572E-05
  validation loss:		3.150646E-05

Epoch 68 of 500
  training loss:		3.498193E-05
  validation loss:		3.215690E-05

Epoch 69 of 500
  training loss:		3.329119E-05
  validation loss:		4.860930E-05

Epoch 70 of 500
  training loss:		3.311129E-05
  validation loss:		3.118055E-05

Epoch 71 of 500
  training loss:		3.026987E-05
  validation loss:		2.840515E-05

Epoch 72 of 500
  training loss:		2.959903E-05
  validation loss:		2.796305E-05

Epoch 73 of 500
  training loss:		2.839527E-05
  validation loss:		4.148120E-05

Epoch 74 of 500
  training loss:		2.851816E-05
  validation loss:		2.413190E-05

Epoch 75 of 500
  training loss:		3.043934E-05
  validation loss:		2.345701E-05

Epoch 76 of 500
  training loss:		2.612427E-05
  validation loss:		2.544452E-05

Epoch 77 of 500
  training loss:		2.672793E-05
  validation loss:		2.203341E-05

Epoch 78 of 500
  training loss:		2.523153E-05
  validation loss:		2.640441E-05

Epoch 79 of 500
  training loss:		2.584335E-05
  validation loss:		2.700911E-05

Epoch 80 of 500
  training loss:		2.291132E-05
  validation loss:		1.985074E-05

Epoch 81 of 500
  training loss:		2.300354E-05
  validation loss:		2.531683E-05

Epoch 82 of 500
  training loss:		2.220567E-05
  validation loss:		2.032862E-05

Epoch 83 of 500
  training loss:		2.050838E-05
  validation loss:		1.901131E-05

Epoch 84 of 500
  training loss:		2.247718E-05
  validation loss:		1.897364E-05

Epoch 85 of 500
  training loss:		2.046786E-05
  validation loss:		1.706532E-05

Epoch 86 of 500
  training loss:		2.019957E-05
  validation loss:		1.760399E-05

Epoch 87 of 500
  training loss:		2.192820E-05
  validation loss:		1.615761E-05

Epoch 88 of 500
  training loss:		1.961394E-05
  validation loss:		2.241104E-05

Epoch 89 of 500
  training loss:		1.891334E-05
  validation loss:		1.642405E-05

Epoch 90 of 500
  training loss:		1.898824E-05
  validation loss:		1.793920E-05

Epoch 91 of 500
  training loss:		1.726091E-05
  validation loss:		1.486389E-05

Epoch 92 of 500
  training loss:		1.741800E-05
  validation loss:		1.548187E-05

Epoch 93 of 500
  training loss:		1.885937E-05
  validation loss:		1.516766E-05

Epoch 94 of 500
  training loss:		1.628295E-05
  validation loss:		2.736206E-05

Epoch 95 of 500
  training loss:		1.837592E-05
  validation loss:		1.469107E-05

Epoch 96 of 500
  training loss:		1.609437E-05
  validation loss:		1.337035E-05

Epoch 97 of 500
  training loss:		1.693962E-05
  validation loss:		1.263873E-05

Epoch 98 of 500
  training loss:		1.544018E-05
  validation loss:		1.236134E-05

Epoch 99 of 500
  training loss:		1.453733E-05
  validation loss:		2.843391E-05

Epoch 100 of 500
  training loss:		1.661169E-05
  validation loss:		1.498704E-05

Epoch 101 of 500
  training loss:		1.480569E-05
  validation loss:		1.528957E-05

Epoch 102 of 500
  training loss:		1.566547E-05
  validation loss:		1.620145E-05

Epoch 103 of 500
  training loss:		1.444511E-05
  validation loss:		1.119147E-05

Epoch 104 of 500
  training loss:		1.465616E-05
  validation loss:		1.102705E-05

Epoch 105 of 500
  training loss:		1.289134E-05
  validation loss:		2.289420E-05

Epoch 106 of 500
  training loss:		1.508434E-05
  validation loss:		1.046176E-05

Epoch 107 of 500
  training loss:		1.347044E-05
  validation loss:		1.372924E-05

Epoch 108 of 500
  training loss:		1.356573E-05
  validation loss:		1.530227E-05

Epoch 109 of 500
  training loss:		1.265905E-05
  validation loss:		1.048246E-05

Epoch 110 of 500
  training loss:		1.237715E-05
  validation loss:		1.105286E-05

Epoch 111 of 500
  training loss:		1.302184E-05
  validation loss:		9.939171E-06

Epoch 112 of 500
  training loss:		1.339653E-05
  validation loss:		1.263006E-05

Epoch 113 of 500
  training loss:		1.197135E-05
  validation loss:		1.058194E-05

Epoch 114 of 500
  training loss:		1.224891E-05
  validation loss:		1.487462E-05

Epoch 115 of 500
  training loss:		1.159223E-05
  validation loss:		1.079991E-05

Epoch 116 of 500
  training loss:		1.248856E-05
  validation loss:		1.251757E-05

Epoch 117 of 500
  training loss:		1.104532E-05
  validation loss:		1.119953E-05

Epoch 118 of 500
  training loss:		1.138279E-05
  validation loss:		8.568166E-06

Epoch 119 of 500
  training loss:		1.204592E-05
  validation loss:		8.308999E-06

Epoch 120 of 500
  training loss:		1.104630E-05
  validation loss:		1.178581E-05

Epoch 121 of 500
  training loss:		1.013550E-05
  validation loss:		9.463053E-06

Epoch 122 of 500
  training loss:		1.148374E-05
  validation loss:		1.600999E-05

Epoch 123 of 500
  training loss:		1.229418E-05
  validation loss:		7.871835E-06

Epoch 124 of 500
  training loss:		9.896108E-06
  validation loss:		7.767977E-06

Epoch 125 of 500
  training loss:		9.730256E-06
  validation loss:		1.076443E-05

Epoch 126 of 500
  training loss:		1.046636E-05
  validation loss:		7.481413E-06

Epoch 127 of 500
  training loss:		1.066952E-05
  validation loss:		8.718179E-06

Epoch 128 of 500
  training loss:		1.157726E-05
  validation loss:		7.623653E-06

Epoch 129 of 500
  training loss:		9.868128E-06
  validation loss:		1.048079E-05

Epoch 130 of 500
  training loss:		9.876327E-06
  validation loss:		1.096726E-05

Epoch 131 of 500
  training loss:		9.891328E-06
  validation loss:		7.368693E-06

Epoch 132 of 500
  training loss:		9.037289E-06
  validation loss:		7.122932E-06

Epoch 133 of 500
  training loss:		1.129521E-05
  validation loss:		7.533063E-06

Epoch 134 of 500
  training loss:		1.009957E-05
  validation loss:		6.705164E-06

Epoch 135 of 500
  training loss:		9.432663E-06
  validation loss:		7.475684E-06

Epoch 136 of 500
  training loss:		8.624494E-06
  validation loss:		6.614041E-06

Epoch 137 of 500
  training loss:		9.091594E-06
  validation loss:		6.412419E-06

Epoch 138 of 500
  training loss:		8.384805E-06
  validation loss:		7.204510E-06

Epoch 139 of 500
  training loss:		8.985277E-06
  validation loss:		6.694685E-06

Epoch 140 of 500
  training loss:		9.775860E-06
  validation loss:		8.236531E-06

Epoch 141 of 500
  training loss:		1.083798E-05
  validation loss:		6.881322E-06

Epoch 142 of 500
  training loss:		8.248091E-06
  validation loss:		6.807319E-06

Epoch 143 of 500
  training loss:		8.814382E-06
  validation loss:		1.116807E-05

Epoch 144 of 500
  training loss:		1.060009E-05
  validation loss:		5.900128E-06

Epoch 145 of 500
  training loss:		7.524432E-06
  validation loss:		7.404192E-06

Early stopping, val-loss increased over the last 5 epochs from 0.00030942724277 to 0.000335817053048
Training-set, RMSE: 2.4413608575e-09
Validation-set, RMSE: 2.3778804942e-09
