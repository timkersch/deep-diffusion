Epoch 1 of 500
  training loss:		4.737319E-04
  validation loss:		1.449946E-09

Epoch 2 of 500
  training loss:		1.988220E-10
  validation loss:		1.551269E-10

Epoch 3 of 500
  training loss:		1.530356E-10
  validation loss:		1.520887E-10

Epoch 4 of 500
  training loss:		1.494753E-10
  validation loss:		1.503484E-10

Epoch 5 of 500
  training loss:		1.452959E-10
  validation loss:		1.438297E-10

Epoch 6 of 500
  training loss:		1.405820E-10
  validation loss:		1.391328E-10

Epoch 7 of 500
  training loss:		1.363358E-10
  validation loss:		1.338954E-10

Epoch 8 of 500
  training loss:		1.313854E-10
  validation loss:		1.288810E-10

Epoch 9 of 500
  training loss:		1.260360E-10
  validation loss:		1.233653E-10

Epoch 10 of 500
  training loss:		1.207656E-10
  validation loss:		1.184090E-10

Epoch 11 of 500
  training loss:		1.157264E-10
  validation loss:		1.137966E-10

Epoch 12 of 500
  training loss:		1.104335E-10
  validation loss:		1.079031E-10

Epoch 13 of 500
  training loss:		1.056155E-10
  validation loss:		1.029760E-10

Epoch 14 of 500
  training loss:		1.009692E-10
  validation loss:		9.748166E-11

Epoch 15 of 500
  training loss:		9.571176E-11
  validation loss:		9.579852E-11

Epoch 16 of 500
  training loss:		9.086085E-11
  validation loss:		8.985864E-11

Epoch 17 of 500
  training loss:		8.667407E-11
  validation loss:		8.374020E-11

Epoch 18 of 500
  training loss:		8.194187E-11
  validation loss:		7.925077E-11

Epoch 19 of 500
  training loss:		7.821325E-11
  validation loss:		7.498216E-11

Epoch 20 of 500
  training loss:		7.368656E-11
  validation loss:		7.089082E-11

Epoch 21 of 500
  training loss:		6.960054E-11
  validation loss:		6.745897E-11

Epoch 22 of 500
  training loss:		6.678810E-11
  validation loss:		6.476836E-11

Epoch 23 of 500
  training loss:		6.241083E-11
  validation loss:		6.151376E-11

Epoch 24 of 500
  training loss:		6.040538E-11
  validation loss:		5.727386E-11

Epoch 25 of 500
  training loss:		5.607152E-11
  validation loss:		5.308752E-11

Epoch 26 of 500
  training loss:		5.308761E-11
  validation loss:		5.152007E-11

Epoch 27 of 500
  training loss:		5.116056E-11
  validation loss:		4.747544E-11

Epoch 28 of 500
  training loss:		4.903823E-11
  validation loss:		4.705376E-11

Epoch 29 of 500
  training loss:		4.526111E-11
  validation loss:		4.185257E-11

Epoch 30 of 500
  training loss:		4.323903E-11
  validation loss:		4.137396E-11

Epoch 31 of 500
  training loss:		4.157213E-11
  validation loss:		3.937678E-11

Epoch 32 of 500
  training loss:		3.950656E-11
  validation loss:		3.516071E-11

Epoch 33 of 500
  training loss:		3.936628E-11
  validation loss:		3.334679E-11

Epoch 34 of 500
  training loss:		3.923681E-11
  validation loss:		8.012089E-11

Epoch 35 of 500
  training loss:		3.732856E-11
  validation loss:		6.771571E-11

Epoch 36 of 500
  training loss:		4.319388E-11
  validation loss:		3.396338E-11

Epoch 37 of 500
  training loss:		5.041336E-11
  validation loss:		2.772485E-11

Epoch 38 of 500
  training loss:		2.313774E-07
  validation loss:		4.673248E-09

Epoch 39 of 500
  training loss:		2.537332E-07
  validation loss:		9.579601E-10

Epoch 40 of 500
  training loss:		1.955058E-07
  validation loss:		4.379976E-10

Early stopping, val-loss increased over the last 10 epochs from 2.34686432336e-09 to 2.81011053882e-08
Training-set, RMSE: 3.09712863696e-05
Validation-set, RMSE: 3.09538000199e-05
