Epoch 1 of 500
  training loss:		3.649466E-04
  validation loss:		2.742958E-09

Epoch 2 of 500
  training loss:		8.100631E-10
  validation loss:		1.197364E-10

Epoch 3 of 500
  training loss:		3.445641E-06
  validation loss:		5.185691E-04

Epoch 4 of 500
  training loss:		9.093693E-06
  validation loss:		1.680766E-07

Epoch 5 of 500
  training loss:		7.833820E-06
  validation loss:		7.710981E-08

Epoch 6 of 500
  training loss:		5.927873E-06
  validation loss:		1.759149E-08

Epoch 7 of 500
  training loss:		4.786524E-06
  validation loss:		5.965896E-07

Epoch 8 of 500
  training loss:		2.751785E-06
  validation loss:		8.742052E-07

Epoch 9 of 500
  training loss:		2.388571E-06
  validation loss:		7.032822E-08

Epoch 10 of 500
  training loss:		1.842366E-06
  validation loss:		7.374125E-09

Epoch 11 of 500
  training loss:		1.354090E-06
  validation loss:		1.286710E-13

Epoch 12 of 500
  training loss:		1.320932E-06
  validation loss:		1.629044E-09

Epoch 13 of 500
  training loss:		1.488708E-06
  validation loss:		6.271068E-07

Epoch 14 of 500
  training loss:		1.060002E-06
  validation loss:		2.741200E-13

Epoch 15 of 500
  training loss:		1.148710E-06
  validation loss:		3.207757E-10

Epoch 16 of 500
  training loss:		1.242656E-06
  validation loss:		1.610739E-10

Epoch 17 of 500
  training loss:		1.060507E-06
  validation loss:		1.291402E-09

Epoch 18 of 500
  training loss:		1.157909E-06
  validation loss:		1.288035E-06

Epoch 19 of 500
  training loss:		1.254203E-06
  validation loss:		9.342626E-12

Epoch 20 of 500
  training loss:		1.252703E-06
  validation loss:		4.077933E-10

Epoch 21 of 500
  training loss:		1.129249E-06
  validation loss:		2.826648E-10

Epoch 22 of 500
  training loss:		1.085781E-06
  validation loss:		2.357137E-12

Epoch 23 of 500
  training loss:		1.137719E-06
  validation loss:		2.883849E-12

Epoch 24 of 500
  training loss:		1.208070E-06
  validation loss:		2.895991E-12

Epoch 25 of 500
  training loss:		1.189554E-06
  validation loss:		5.259651E-09

Epoch 26 of 500
  training loss:		1.104237E-06
  validation loss:		7.657657E-08

Epoch 27 of 500
  training loss:		1.073450E-06
  validation loss:		6.261224E-10

Epoch 28 of 500
  training loss:		1.212071E-06
  validation loss:		1.224696E-08

Epoch 29 of 500
  training loss:		1.042958E-06
  validation loss:		2.445984E-09

Epoch 30 of 500
  training loss:		1.107690E-06
  validation loss:		1.604373E-08

Epoch 31 of 500
  training loss:		1.153227E-06
  validation loss:		1.105034E-09

Epoch 32 of 500
  training loss:		1.122670E-06
  validation loss:		2.388028E-09

Epoch 33 of 500
  training loss:		1.161515E-06
  validation loss:		6.000913E-08

Epoch 34 of 500
  training loss:		1.079888E-06
  validation loss:		1.192771E-09

Epoch 35 of 500
  training loss:		1.043058E-06
  validation loss:		1.741956E-07

Epoch 36 of 500
  training loss:		1.117387E-06
  validation loss:		3.563487E-07

Epoch 37 of 500
  training loss:		1.175485E-06
  validation loss:		8.160959E-07

Epoch 38 of 500
  training loss:		1.175386E-06
  validation loss:		1.513472E-09

Epoch 39 of 500
  training loss:		1.052870E-06
  validation loss:		3.463949E-10

Epoch 40 of 500
  training loss:		1.073250E-06
  validation loss:		2.677576E-10

Early stopping, val-loss increased over the last 10 epochs from 4.00619052922e-06 to 4.98952347145e-05
Training-set, RMSE: 1.86238808271e-05
Validation-set, RMSE: 1.86124389516e-05
