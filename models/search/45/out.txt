Training network with 45280 training samples and 8490 validation samples
Epoch 1 of 500
  training loss:		5.339103E-03
  validation loss:		9.375426E-04

Epoch 2 of 500
  training loss:		7.467184E-04
  validation loss:		1.478620E-04

Epoch 3 of 500
  training loss:		9.883609E-05
  validation loss:		3.868830E-05

Epoch 4 of 500
  training loss:		5.486073E-05
  validation loss:		3.963717E-05

Epoch 5 of 500
  training loss:		4.436722E-05
  validation loss:		2.033834E-05

Epoch 6 of 500
  training loss:		3.958162E-05
  validation loss:		5.103858E-05

Epoch 7 of 500
  training loss:		3.783139E-05
  validation loss:		2.765579E-05

Epoch 8 of 500
  training loss:		3.412466E-05
  validation loss:		9.312037E-06

Epoch 9 of 500
  training loss:		2.667363E-05
  validation loss:		7.313348E-06

Epoch 10 of 500
  training loss:		2.836485E-05
  validation loss:		1.382097E-05

Epoch 11 of 500
  training loss:		2.700473E-05
  validation loss:		1.119567E-05

Epoch 12 of 500
  training loss:		2.725198E-05
  validation loss:		1.709847E-05

Epoch 13 of 500
  training loss:		2.376577E-05
  validation loss:		5.884269E-06

Epoch 14 of 500
  training loss:		2.449807E-05
  validation loss:		3.826873E-05

Epoch 15 of 500
  training loss:		2.280018E-05
  validation loss:		1.723949E-05

Epoch 16 of 500
  training loss:		2.513305E-05
  validation loss:		1.940039E-05

Epoch 17 of 500
  training loss:		1.992849E-05
  validation loss:		7.712476E-06

Epoch 18 of 500
  training loss:		2.165957E-05
  validation loss:		3.155293E-05

Epoch 19 of 500
  training loss:		2.080063E-05
  validation loss:		8.987376E-06

Epoch 20 of 500
  training loss:		1.945114E-05
  validation loss:		3.915671E-06

Epoch 21 of 500
  training loss:		2.416420E-05
  validation loss:		7.712691E-05

Epoch 22 of 500
  training loss:		2.207997E-05
  validation loss:		8.824186E-06

Epoch 23 of 500
  training loss:		2.046867E-05
  validation loss:		3.126266E-06

Epoch 24 of 500
  training loss:		1.946051E-05
  validation loss:		4.057311E-06

Epoch 25 of 500
  training loss:		2.254350E-05
  validation loss:		2.848630E-06

Epoch 26 of 500
  training loss:		1.578705E-05
  validation loss:		3.383517E-05

Epoch 27 of 500
  training loss:		2.190506E-05
  validation loss:		1.232082E-05

Epoch 28 of 500
  training loss:		1.880974E-05
  validation loss:		1.282626E-04

Epoch 29 of 500
  training loss:		1.660910E-05
  validation loss:		9.864480E-06

Epoch 30 of 500
  training loss:		1.916540E-05
  validation loss:		4.622370E-06

Epoch 31 of 500
  training loss:		2.136057E-05
  validation loss:		1.262939E-05

Epoch 32 of 500
  training loss:		1.766983E-05
  validation loss:		3.473889E-06

Epoch 33 of 500
  training loss:		1.833200E-05
  validation loss:		6.870035E-06

Epoch 34 of 500
  training loss:		1.837834E-05
  validation loss:		2.266025E-06

Epoch 35 of 500
  training loss:		1.648392E-05
  validation loss:		3.734381E-06

Epoch 36 of 500
  training loss:		1.786902E-05
  validation loss:		7.165413E-06

Epoch 37 of 500
  training loss:		1.739953E-05
  validation loss:		3.227347E-05

Epoch 38 of 500
  training loss:		1.756775E-05
  validation loss:		7.181868E-05

Epoch 39 of 500
  training loss:		1.738026E-05
  validation loss:		8.401553E-05

Epoch 40 of 500
  training loss:		1.674014E-05
  validation loss:		4.074563E-06

Epoch 41 of 500
  training loss:		1.736175E-05
  validation loss:		2.556019E-05

Epoch 42 of 500
  training loss:		1.650968E-05
  validation loss:		2.648703E-05

Epoch 43 of 500
  training loss:		1.587246E-05
  validation loss:		2.391907E-05

Epoch 44 of 500
  training loss:		1.829499E-05
  validation loss:		1.947901E-04

Epoch 45 of 500
  training loss:		1.609168E-05
  validation loss:		4.147641E-06

Epoch 46 of 500
  training loss:		1.777299E-05
  validation loss:		1.603184E-05

Epoch 47 of 500
  training loss:		1.566342E-05
  validation loss:		3.743846E-06

Epoch 48 of 500
  training loss:		1.498247E-05
  validation loss:		3.756802E-05

Epoch 49 of 500
  training loss:		1.751186E-05
  validation loss:		4.742431E-06

Epoch 50 of 500
  training loss:		1.712843E-05
  validation loss:		2.006191E-05

Epoch 51 of 500
  training loss:		1.575158E-05
  validation loss:		1.001350E-05

Epoch 52 of 500
  training loss:		1.724463E-05
  validation loss:		8.740720E-06

Epoch 53 of 500
  training loss:		1.489956E-05
  validation loss:		1.182603E-06

Epoch 54 of 500
  training loss:		1.601504E-05
  validation loss:		1.081934E-06

Epoch 55 of 500
  training loss:		1.692203E-05
  validation loss:		3.487589E-06

Epoch 56 of 500
  training loss:		1.486286E-05
  validation loss:		1.170047E-06

Epoch 57 of 500
  training loss:		1.650234E-05
  validation loss:		1.203661E-05

Epoch 58 of 500
  training loss:		1.460326E-05
  validation loss:		1.011616E-05

Epoch 59 of 500
  training loss:		1.640235E-05
  validation loss:		1.503789E-05

Epoch 60 of 500
  training loss:		1.551711E-05
  validation loss:		2.163119E-06

Epoch 61 of 500
  training loss:		1.329615E-05
  validation loss:		2.543530E-05

Epoch 62 of 500
  training loss:		1.488323E-05
  validation loss:		2.324035E-06

Epoch 63 of 500
  training loss:		1.299323E-05
  validation loss:		2.370568E-06

Epoch 64 of 500
  training loss:		1.406468E-05
  validation loss:		8.158526E-06

Epoch 65 of 500
  training loss:		1.803044E-05
  validation loss:		5.948707E-05

Epoch 66 of 500
  training loss:		1.256998E-05
  validation loss:		9.647043E-07

Epoch 67 of 500
  training loss:		1.390042E-05
  validation loss:		3.183243E-05

Epoch 68 of 500
  training loss:		1.460225E-05
  validation loss:		5.303708E-05

Epoch 69 of 500
  training loss:		1.236027E-05
  validation loss:		3.067394E-06

Epoch 70 of 500
  training loss:		1.335051E-05
  validation loss:		3.056329E-06

Epoch 71 of 500
  training loss:		1.137137E-05
  validation loss:		1.009027E-05

Epoch 72 of 500
  training loss:		1.500281E-05
  validation loss:		1.795140E-06

Epoch 73 of 500
  training loss:		1.292503E-05
  validation loss:		4.414070E-05

Epoch 74 of 500
  training loss:		1.269323E-05
  validation loss:		1.620164E-06

Epoch 75 of 500
  training loss:		1.438645E-05
  validation loss:		3.703939E-06

Epoch 76 of 500
  training loss:		1.132243E-05
  validation loss:		2.502398E-06

Epoch 77 of 500
  training loss:		1.281962E-05
  validation loss:		1.925232E-06

Epoch 78 of 500
  training loss:		1.140443E-05
  validation loss:		3.898906E-05

Epoch 79 of 500
  training loss:		1.318340E-05
  validation loss:		1.526398E-05

Epoch 80 of 500
  training loss:		1.232352E-05
  validation loss:		9.142963E-06

Epoch 81 of 500
  training loss:		1.366789E-05
  validation loss:		9.871275E-05

Epoch 82 of 500
  training loss:		9.939549E-06
  validation loss:		2.787749E-05

Epoch 83 of 500
  training loss:		1.267725E-05
  validation loss:		2.087162E-05

Epoch 84 of 500
  training loss:		1.106504E-05
  validation loss:		4.537774E-06

Epoch 85 of 500
  training loss:		1.109145E-05
  validation loss:		2.140243E-05

Epoch 86 of 500
  training loss:		1.256465E-05
  validation loss:		1.200550E-06

Epoch 87 of 500
  training loss:		1.057144E-05
  validation loss:		1.236087E-05

Epoch 88 of 500
  training loss:		1.216022E-05
  validation loss:		5.748505E-06

Epoch 89 of 500
  training loss:		1.141436E-05
  validation loss:		1.591241E-05

Epoch 90 of 500
  training loss:		1.180323E-05
  validation loss:		1.040069E-06

Epoch 91 of 500
  training loss:		9.782679E-06
  validation loss:		3.160069E-06

Epoch 92 of 500
  training loss:		1.241560E-05
  validation loss:		8.400070E-07

Epoch 93 of 500
  training loss:		9.976755E-06
  validation loss:		1.463280E-06

Epoch 94 of 500
  training loss:		1.146167E-05
  validation loss:		1.334119E-05

Epoch 95 of 500
  training loss:		1.008228E-05
  validation loss:		1.437520E-05

Epoch 96 of 500
  training loss:		1.164169E-05
  validation loss:		1.490997E-06

Epoch 97 of 500
  training loss:		9.737007E-06
  validation loss:		3.228158E-06

Epoch 98 of 500
  training loss:		1.181487E-05
  validation loss:		1.123599E-06

Epoch 99 of 500
  training loss:		9.647064E-06
  validation loss:		8.468901E-06

Epoch 100 of 500
  training loss:		1.026097E-05
  validation loss:		2.554328E-05

Epoch 101 of 500
  training loss:		9.775447E-06
  validation loss:		6.417519E-06

Epoch 102 of 500
  training loss:		1.053986E-05
  validation loss:		1.322935E-05

Epoch 103 of 500
  training loss:		9.738076E-06
  validation loss:		5.060535E-06

Epoch 104 of 500
  training loss:		1.054819E-05
  validation loss:		4.510432E-06

Epoch 105 of 500
  training loss:		1.045981E-05
  validation loss:		7.210947E-06

Epoch 106 of 500
  training loss:		8.515846E-06
  validation loss:		7.000452E-06

Epoch 107 of 500
  training loss:		1.072751E-05
  validation loss:		4.890306E-06

Epoch 108 of 500
  training loss:		1.138123E-05
  validation loss:		7.440742E-07

Epoch 109 of 500
  training loss:		8.389274E-06
  validation loss:		3.206976E-05

Epoch 110 of 500
  training loss:		9.783159E-06
  validation loss:		7.410819E-07

Epoch 111 of 500
  training loss:		1.078674E-05
  validation loss:		2.500045E-06

Epoch 112 of 500
  training loss:		9.708037E-06
  validation loss:		1.154412E-05

Epoch 113 of 500
  training loss:		9.783174E-06
  validation loss:		1.615752E-05

Epoch 114 of 500
  training loss:		9.324393E-06
  validation loss:		2.417917E-06

Epoch 115 of 500
  training loss:		9.366985E-06
  validation loss:		8.973131E-07

Epoch 116 of 500
  training loss:		1.010921E-05
  validation loss:		2.483094E-05

Epoch 117 of 500
  training loss:		9.994297E-06
  validation loss:		5.422252E-06

Epoch 118 of 500
  training loss:		9.776926E-06
  validation loss:		6.864419E-06

Epoch 119 of 500
  training loss:		9.765760E-06
  validation loss:		5.547298E-06

Epoch 120 of 500
  training loss:		8.395596E-06
  validation loss:		5.327792E-06

Epoch 121 of 500
  training loss:		9.591802E-06
  validation loss:		5.258693E-06

Epoch 122 of 500
  training loss:		9.451188E-06
  validation loss:		1.224055E-06

Epoch 123 of 500
  training loss:		8.152530E-06
  validation loss:		1.158955E-06

Epoch 124 of 500
  training loss:		1.080017E-05
  validation loss:		1.751008E-05

Epoch 125 of 500
  training loss:		7.952903E-06
  validation loss:		3.042350E-06

Epoch 126 of 500
  training loss:		1.343411E-05
  validation loss:		1.055112E-06

Epoch 127 of 500
  training loss:		6.140085E-06
  validation loss:		3.792928E-06

Epoch 128 of 500
  training loss:		1.122501E-05
  validation loss:		1.744219E-05

Epoch 129 of 500
  training loss:		9.109255E-06
  validation loss:		1.774027E-05

Epoch 130 of 500
  training loss:		9.061650E-06
  validation loss:		3.160069E-06

Epoch 131 of 500
  training loss:		9.134554E-06
  validation loss:		1.963812E-05

Epoch 132 of 500
  training loss:		8.698699E-06
  validation loss:		2.993043E-06

Epoch 133 of 500
  training loss:		8.870649E-06
  validation loss:		5.367080E-06

Epoch 134 of 500
  training loss:		9.559286E-06
  validation loss:		1.050317E-06

Epoch 135 of 500
  training loss:		7.706124E-06
  validation loss:		1.508575E-05

Epoch 136 of 500
  training loss:		9.575469E-06
  validation loss:		2.084630E-06

Epoch 137 of 500
  training loss:		8.981585E-06
  validation loss:		6.530554E-07

Epoch 138 of 500
  training loss:		8.080156E-06
  validation loss:		2.486361E-05

Epoch 139 of 500
  training loss:		8.277033E-06
  validation loss:		3.571073E-05

Epoch 140 of 500
  training loss:		9.392768E-06
  validation loss:		5.747432E-06

Early stopping, val-loss increased over the last 20 epochs from 0.00216483888729 to 0.00244566464839
Training RMSE: 5.86488986517e-09
Validation RMSE: 5.85577825457e-09
