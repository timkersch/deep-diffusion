Epoch 1 of 500
  training loss:		2.656999E-02
  validation loss:		9.217223E-05

Epoch 2 of 500
  training loss:		7.495182E-06
  validation loss:		5.133510E-07

Epoch 3 of 500
  training loss:		4.894602E-07
  validation loss:		4.698327E-07

Epoch 4 of 500
  training loss:		4.435670E-07
  validation loss:		4.218400E-07

Epoch 5 of 500
  training loss:		3.940807E-07
  validation loss:		3.713104E-07

Epoch 6 of 500
  training loss:		3.432650E-07
  validation loss:		3.193971E-07

Epoch 7 of 500
  training loss:		2.933764E-07
  validation loss:		2.707470E-07

Epoch 8 of 500
  training loss:		2.457629E-07
  validation loss:		2.247627E-07

Epoch 9 of 500
  training loss:		2.016289E-07
  validation loss:		1.818795E-07

Epoch 10 of 500
  training loss:		1.622719E-07
  validation loss:		1.448414E-07

Epoch 11 of 500
  training loss:		1.276384E-07
  validation loss:		1.126687E-07

Epoch 12 of 500
  training loss:		9.803124E-08
  validation loss:		8.566540E-08

Epoch 13 of 500
  training loss:		7.373901E-08
  validation loss:		6.361938E-08

Epoch 14 of 500
  training loss:		5.411885E-08
  validation loss:		4.622254E-08

Epoch 15 of 500
  training loss:		3.876917E-08
  validation loss:		3.262901E-08

Epoch 16 of 500
  training loss:		2.711067E-08
  validation loss:		2.259366E-08

Epoch 17 of 500
  training loss:		1.856035E-08
  validation loss:		1.532771E-08

Epoch 18 of 500
  training loss:		1.246046E-08
  validation loss:		1.023627E-08

Epoch 19 of 500
  training loss:		8.260976E-09
  validation loss:		6.774122E-09

Epoch 20 of 500
  training loss:		5.462173E-09
  validation loss:		4.508900E-09

Epoch 21 of 500
  training loss:		3.665793E-09
  validation loss:		3.093028E-09

Epoch 22 of 500
  training loss:		2.552851E-09
  validation loss:		2.225033E-09

Epoch 23 of 500
  training loss:		1.889339E-09
  validation loss:		1.720630E-09

Epoch 24 of 500
  training loss:		1.506365E-09
  validation loss:		1.433308E-09

Epoch 25 of 500
  training loss:		1.294618E-09
  validation loss:		1.272983E-09

Epoch 26 of 500
  training loss:		1.178266E-09
  validation loss:		1.182267E-09

Epoch 27 of 500
  training loss:		1.115473E-09
  validation loss:		1.134570E-09

Epoch 28 of 500
  training loss:		1.079770E-09
  validation loss:		1.105172E-09

Epoch 29 of 500
  training loss:		1.055068E-09
  validation loss:		1.084226E-09

Epoch 30 of 500
  training loss:		1.037736E-09
  validation loss:		1.066218E-09

Epoch 31 of 500
  training loss:		1.022773E-09
  validation loss:		1.048359E-09

Epoch 32 of 500
  training loss:		1.007292E-09
  validation loss:		1.037478E-09

Epoch 33 of 500
  training loss:		9.911196E-10
  validation loss:		1.018844E-09

Epoch 34 of 500
  training loss:		9.745545E-10
  validation loss:		1.002250E-09

Epoch 35 of 500
  training loss:		9.582784E-10
  validation loss:		9.860373E-10

Epoch 36 of 500
  training loss:		9.397506E-10
  validation loss:		9.665387E-10

Epoch 37 of 500
  training loss:		9.231464E-10
  validation loss:		9.429122E-10

Epoch 38 of 500
  training loss:		9.021257E-10
  validation loss:		9.225027E-10

Epoch 39 of 500
  training loss:		8.839105E-10
  validation loss:		9.059766E-10

Epoch 40 of 500
  training loss:		8.625222E-10
  validation loss:		8.820300E-10

Epoch 41 of 500
  training loss:		8.415112E-10
  validation loss:		8.624832E-10

Epoch 42 of 500
  training loss:		8.217869E-10
  validation loss:		8.387218E-10

Epoch 43 of 500
  training loss:		7.999856E-10
  validation loss:		8.149406E-10

Epoch 44 of 500
  training loss:		7.794696E-10
  validation loss:		7.914650E-10

Epoch 45 of 500
  training loss:		7.552051E-10
  validation loss:		7.683073E-10

Epoch 46 of 500
  training loss:		7.321083E-10
  validation loss:		7.471794E-10

Epoch 47 of 500
  training loss:		7.094316E-10
  validation loss:		7.200012E-10

Epoch 48 of 500
  training loss:		6.903367E-10
  validation loss:		6.943851E-10

Epoch 49 of 500
  training loss:		6.634731E-10
  validation loss:		6.709781E-10

Epoch 50 of 500
  training loss:		6.394042E-10
  validation loss:		6.493744E-10

Epoch 51 of 500
  training loss:		6.168662E-10
  validation loss:		6.228459E-10

Epoch 52 of 500
  training loss:		5.914371E-10
  validation loss:		6.010533E-10

Epoch 53 of 500
  training loss:		5.718703E-10
  validation loss:		5.737177E-10

Epoch 54 of 500
  training loss:		5.457140E-10
  validation loss:		5.521215E-10

Epoch 55 of 500
  training loss:		5.227540E-10
  validation loss:		5.271184E-10

Epoch 56 of 500
  training loss:		5.006721E-10
  validation loss:		5.052026E-10

Epoch 57 of 500
  training loss:		4.809172E-10
  validation loss:		4.824578E-10

Epoch 58 of 500
  training loss:		4.567538E-10
  validation loss:		4.693626E-10

Epoch 59 of 500
  training loss:		4.393319E-10
  validation loss:		4.467727E-10

Epoch 60 of 500
  training loss:		4.212452E-10
  validation loss:		4.151993E-10

Epoch 61 of 500
  training loss:		3.956101E-10
  validation loss:		3.976954E-10

Epoch 62 of 500
  training loss:		3.761883E-10
  validation loss:		3.759018E-10

Epoch 63 of 500
  training loss:		3.585774E-10
  validation loss:		3.553283E-10

Epoch 64 of 500
  training loss:		3.446882E-10
  validation loss:		3.369374E-10

Epoch 65 of 500
  training loss:		3.262907E-10
  validation loss:		3.187971E-10

Epoch 66 of 500
  training loss:		3.040267E-10
  validation loss:		3.044330E-10

Epoch 67 of 500
  training loss:		2.956099E-10
  validation loss:		3.025330E-10

Epoch 68 of 500
  training loss:		2.760020E-10
  validation loss:		2.692141E-10

Epoch 69 of 500
  training loss:		2.574855E-10
  validation loss:		2.582039E-10

Epoch 70 of 500
  training loss:		2.539185E-10
  validation loss:		2.707738E-10

Epoch 71 of 500
  training loss:		2.346056E-10
  validation loss:		2.347102E-10

Epoch 72 of 500
  training loss:		2.252111E-10
  validation loss:		2.727302E-10

Epoch 73 of 500
  training loss:		2.094904E-10
  validation loss:		2.016471E-10

Epoch 74 of 500
  training loss:		2.011961E-10
  validation loss:		1.898755E-10

Epoch 75 of 500
  training loss:		1.840887E-10
  validation loss:		2.125601E-10

Epoch 76 of 500
  training loss:		1.819095E-10
  validation loss:		2.192617E-10

Epoch 77 of 500
  training loss:		1.676031E-10
  validation loss:		1.749340E-10

Epoch 78 of 500
  training loss:		1.704230E-10
  validation loss:		1.482335E-10

Epoch 79 of 500
  training loss:		1.633804E-10
  validation loss:		1.409130E-10

Epoch 80 of 500
  training loss:		1.491830E-10
  validation loss:		1.326351E-10

Epoch 81 of 500
  training loss:		1.664512E-10
  validation loss:		1.890636E-10

Epoch 82 of 500
  training loss:		1.867937E-10
  validation loss:		3.346439E-10

Epoch 83 of 500
  training loss:		5.738785E-10
  validation loss:		5.108373E-10

Epoch 84 of 500
  training loss:		1.205106E-08
  validation loss:		6.097076E-09

Epoch 85 of 500
  training loss:		7.074062E-09
  validation loss:		1.600415E-09

Early stopping, val-loss increased over the last 5 epochs from 7.18060028948e-09 to 7.68419188842e-08
Training-set, RMSE: 7.80563432328e-05
Validation-set, RMSE: 7.80705740767e-05
