Epoch 1 of 500
  training loss:		2.290876E-03
  validation loss:		1.220417E-07

Epoch 2 of 500
  training loss:		3.038154E-08
  validation loss:		7.077358E-09

Epoch 3 of 500
  training loss:		3.999293E-09
  validation loss:		2.123177E-09

Epoch 4 of 500
  training loss:		1.505044E-09
  validation loss:		1.347798E-09

Epoch 5 of 500
  training loss:		1.510321E-09
  validation loss:		4.183911E-09

Epoch 6 of 500
  training loss:		3.653842E-05
  validation loss:		1.512124E-09

Epoch 7 of 500
  training loss:		1.926293E-05
  validation loss:		5.219037E-08

Epoch 8 of 500
  training loss:		2.027407E-05
  validation loss:		2.381913E-08

Epoch 9 of 500
  training loss:		1.898302E-05
  validation loss:		5.440495E-08

Epoch 10 of 500
  training loss:		1.342721E-05
  validation loss:		2.328339E-08

Epoch 11 of 500
  training loss:		8.581939E-06
  validation loss:		1.684403E-07

Epoch 12 of 500
  training loss:		8.861443E-06
  validation loss:		3.012363E-05

Epoch 13 of 500
  training loss:		8.497567E-06
  validation loss:		1.008791E-06

Epoch 14 of 500
  training loss:		4.097987E-06
  validation loss:		6.129349E-08

Epoch 15 of 500
  training loss:		5.718787E-06
  validation loss:		1.949621E-05

Epoch 16 of 500
  training loss:		3.573448E-06
  validation loss:		5.807029E-09

Epoch 17 of 500
  training loss:		6.326138E-06
  validation loss:		2.516521E-05

Epoch 18 of 500
  training loss:		5.008944E-06
  validation loss:		1.035533E-09

Epoch 19 of 500
  training loss:		4.918533E-06
  validation loss:		5.598485E-11

Epoch 20 of 500
  training loss:		4.924037E-06
  validation loss:		2.591550E-12

Early stopping, val-loss increased over the last 10 epochs from 5.13891638679e-06 to 0.00133813624441
Training-set, RMSE: 7.48678938929e-06
Validation-set, RMSE: 7.48243230285e-06
