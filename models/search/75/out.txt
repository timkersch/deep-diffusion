Epoch 1 of 500
  training loss:		2.276437E-02
  validation loss:		3.595327E-06

Epoch 2 of 500
  training loss:		2.668983E-06
  validation loss:		2.289205E-06

Epoch 3 of 500
  training loss:		1.902626E-06
  validation loss:		1.599000E-06

Epoch 4 of 500
  training loss:		1.311111E-06
  validation loss:		1.096866E-06

Epoch 5 of 500
  training loss:		9.043259E-07
  validation loss:		7.659550E-07

Epoch 6 of 500
  training loss:		6.224697E-07
  validation loss:		5.099714E-07

Epoch 7 of 500
  training loss:		4.143423E-07
  validation loss:		3.315233E-07

Epoch 8 of 500
  training loss:		2.851149E-07
  validation loss:		2.271310E-07

Epoch 9 of 500
  training loss:		1.935634E-07
  validation loss:		1.488697E-07

Epoch 10 of 500
  training loss:		1.364420E-07
  validation loss:		1.165390E-07

Epoch 11 of 500
  training loss:		9.613009E-08
  validation loss:		6.590881E-08

Epoch 12 of 500
  training loss:		7.165416E-08
  validation loss:		4.636459E-08

Epoch 13 of 500
  training loss:		4.756125E-08
  validation loss:		3.314080E-08

Epoch 14 of 500
  training loss:		5.169475E-05
  validation loss:		3.018272E-05

Epoch 15 of 500
  training loss:		5.257586E-05
  validation loss:		2.054800E-06

Epoch 16 of 500
  training loss:		9.066845E-05
  validation loss:		2.395303E-04

Epoch 17 of 500
  training loss:		5.127374E-05
  validation loss:		1.141260E-04

Epoch 18 of 500
  training loss:		6.941180E-05
  validation loss:		1.908228E-06

Epoch 19 of 500
  training loss:		7.505241E-05
  validation loss:		4.178275E-06

Epoch 20 of 500
  training loss:		7.968489E-05
  validation loss:		5.058317E-05

Early stopping, val-loss increased over the last 10 epochs from 9.39874165864e-05 to 0.00389583828059
Training-set, RMSE: 0.00204719322354
Validation-set, RMSE: 0.0020442000519
