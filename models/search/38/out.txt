Epoch 1 of 500
  training loss:		9.005435E-02
  validation loss:		7.754363E-02

Epoch 2 of 500
  training loss:		6.667379E-02
  validation loss:		5.473696E-02

Epoch 3 of 500
  training loss:		4.079882E-02
  validation loss:		2.749656E-02

Epoch 4 of 500
  training loss:		1.885254E-02
  validation loss:		1.270047E-02

Epoch 5 of 500
  training loss:		1.035207E-02
  validation loss:		8.997814E-03

Epoch 6 of 500
  training loss:		8.416813E-03
  validation loss:		8.131874E-03

Epoch 7 of 500
  training loss:		7.689430E-03
  validation loss:		7.451246E-03

Epoch 8 of 500
  training loss:		7.151759E-03
  validation loss:		6.897340E-03

Epoch 9 of 500
  training loss:		6.658378E-03
  validation loss:		6.420779E-03

Epoch 10 of 500
  training loss:		6.149097E-03
  validation loss:		6.291966E-03

Epoch 11 of 500
  training loss:		5.646289E-03
  validation loss:		5.370395E-03

Epoch 12 of 500
  training loss:		5.109062E-03
  validation loss:		4.866855E-03

Epoch 13 of 500
  training loss:		4.614682E-03
  validation loss:		4.391319E-03

Epoch 14 of 500
  training loss:		4.173189E-03
  validation loss:		4.015622E-03

Epoch 15 of 500
  training loss:		3.734703E-03
  validation loss:		3.571516E-03

Epoch 16 of 500
  training loss:		3.390826E-03
  validation loss:		3.158841E-03

Epoch 17 of 500
  training loss:		3.045390E-03
  validation loss:		2.928796E-03

Epoch 18 of 500
  training loss:		2.746512E-03
  validation loss:		2.626129E-03

Epoch 19 of 500
  training loss:		2.514737E-03
  validation loss:		2.354235E-03

Epoch 20 of 500
  training loss:		2.310054E-03
  validation loss:		2.122554E-03

Epoch 21 of 500
  training loss:		2.029868E-03
  validation loss:		1.823627E-03

Epoch 22 of 500
  training loss:		1.680048E-03
  validation loss:		1.492777E-03

Epoch 23 of 500
  training loss:		1.387016E-03
  validation loss:		1.291710E-03

Epoch 24 of 500
  training loss:		1.144616E-03
  validation loss:		1.062937E-03

Epoch 25 of 500
  training loss:		9.826810E-04
  validation loss:		9.053529E-04

Epoch 26 of 500
  training loss:		8.491417E-04
  validation loss:		7.839158E-04

Epoch 27 of 500
  training loss:		7.378866E-04
  validation loss:		6.833892E-04

Epoch 28 of 500
  training loss:		6.520454E-04
  validation loss:		6.147246E-04

Epoch 29 of 500
  training loss:		5.785315E-04
  validation loss:		5.554882E-04

Epoch 30 of 500
  training loss:		5.194849E-04
  validation loss:		4.799481E-04

Epoch 31 of 500
  training loss:		4.663283E-04
  validation loss:		4.592572E-04

Epoch 32 of 500
  training loss:		4.276242E-04
  validation loss:		4.114638E-04

Epoch 33 of 500
  training loss:		3.822092E-04
  validation loss:		3.631557E-04

Epoch 34 of 500
  training loss:		3.476563E-04
  validation loss:		3.177473E-04

Epoch 35 of 500
  training loss:		3.137514E-04
  validation loss:		2.901937E-04

Epoch 36 of 500
  training loss:		2.921118E-04
  validation loss:		2.660526E-04

Epoch 37 of 500
  training loss:		2.695069E-04
  validation loss:		2.457921E-04

Epoch 38 of 500
  training loss:		2.434382E-04
  validation loss:		2.263399E-04

Epoch 39 of 500
  training loss:		2.259515E-04
  validation loss:		2.162828E-04

Epoch 40 of 500
  training loss:		2.202167E-04
  validation loss:		1.956675E-04

Epoch 41 of 500
  training loss:		2.022606E-04
  validation loss:		1.831223E-04

Epoch 42 of 500
  training loss:		1.870271E-04
  validation loss:		2.038685E-04

Epoch 43 of 500
  training loss:		1.768727E-04
  validation loss:		1.585933E-04

Epoch 44 of 500
  training loss:		1.707858E-04
  validation loss:		1.507076E-04

Epoch 45 of 500
  training loss:		1.512787E-04
  validation loss:		1.406127E-04

Epoch 46 of 500
  training loss:		1.491845E-04
  validation loss:		1.408482E-04

Epoch 47 of 500
  training loss:		1.363802E-04
  validation loss:		1.259443E-04

Epoch 48 of 500
  training loss:		1.287724E-04
  validation loss:		1.510220E-04

Epoch 49 of 500
  training loss:		1.247281E-04
  validation loss:		1.280571E-04

Epoch 50 of 500
  training loss:		1.179737E-04
  validation loss:		1.101101E-04

Epoch 51 of 500
  training loss:		1.103647E-04
  validation loss:		1.175159E-04

Epoch 52 of 500
  training loss:		1.058630E-04
  validation loss:		1.132241E-04

Epoch 53 of 500
  training loss:		1.009871E-04
  validation loss:		9.271178E-05

Epoch 54 of 500
  training loss:		1.018265E-04
  validation loss:		9.271183E-05

Epoch 55 of 500
  training loss:		9.747894E-05
  validation loss:		8.372307E-05

Epoch 56 of 500
  training loss:		9.349729E-05
  validation loss:		7.980007E-05

Epoch 57 of 500
  training loss:		8.549539E-05
  validation loss:		7.873945E-05

Epoch 58 of 500
  training loss:		7.921066E-05
  validation loss:		8.707869E-05

Epoch 59 of 500
  training loss:		7.838432E-05
  validation loss:		7.257432E-05

Epoch 60 of 500
  training loss:		7.646147E-05
  validation loss:		7.904077E-05

Epoch 61 of 500
  training loss:		8.508086E-05
  validation loss:		6.617173E-05

Epoch 62 of 500
  training loss:		7.121570E-05
  validation loss:		6.399924E-05

Epoch 63 of 500
  training loss:		7.217471E-05
  validation loss:		7.677818E-05

Epoch 64 of 500
  training loss:		6.978913E-05
  validation loss:		6.347238E-05

Epoch 65 of 500
  training loss:		6.502691E-05
  validation loss:		5.941375E-05

Epoch 66 of 500
  training loss:		6.509204E-05
  validation loss:		5.704595E-05

Epoch 67 of 500
  training loss:		5.957495E-05
  validation loss:		6.519060E-05

Epoch 68 of 500
  training loss:		6.169025E-05
  validation loss:		6.003897E-05

Epoch 69 of 500
  training loss:		5.885365E-05
  validation loss:		8.532621E-05

Epoch 70 of 500
  training loss:		6.005639E-05
  validation loss:		5.045941E-05

Epoch 71 of 500
  training loss:		5.939185E-05
  validation loss:		5.287948E-05

Epoch 72 of 500
  training loss:		5.289293E-05
  validation loss:		4.833568E-05

Epoch 73 of 500
  training loss:		5.469701E-05
  validation loss:		5.377560E-05

Epoch 74 of 500
  training loss:		5.148470E-05
  validation loss:		6.083959E-05

Epoch 75 of 500
  training loss:		5.373009E-05
  validation loss:		4.762801E-05

Epoch 76 of 500
  training loss:		5.338238E-05
  validation loss:		5.486872E-05

Epoch 77 of 500
  training loss:		4.858932E-05
  validation loss:		4.286059E-05

Epoch 78 of 500
  training loss:		4.644151E-05
  validation loss:		4.464622E-05

Epoch 79 of 500
  training loss:		4.609474E-05
  validation loss:		4.091156E-05

Epoch 80 of 500
  training loss:		4.658983E-05
  validation loss:		4.201350E-05

Epoch 81 of 500
  training loss:		4.735650E-05
  validation loss:		4.035073E-05

Epoch 82 of 500
  training loss:		4.350954E-05
  validation loss:		3.835204E-05

Epoch 83 of 500
  training loss:		4.406249E-05
  validation loss:		3.734371E-05

Epoch 84 of 500
  training loss:		4.283613E-05
  validation loss:		4.567421E-05

Epoch 85 of 500
  training loss:		4.373161E-05
  validation loss:		3.560147E-05

Epoch 86 of 500
  training loss:		4.061875E-05
  validation loss:		3.729327E-05

Epoch 87 of 500
  training loss:		3.836115E-05
  validation loss:		3.488722E-05

Epoch 88 of 500
  training loss:		4.035156E-05
  validation loss:		3.586216E-05

Epoch 89 of 500
  training loss:		3.649959E-05
  validation loss:		3.968758E-05

Epoch 90 of 500
  training loss:		4.263584E-05
  validation loss:		5.859037E-05

Epoch 91 of 500
  training loss:		3.790382E-05
  validation loss:		3.288869E-05

Epoch 92 of 500
  training loss:		3.807527E-05
  validation loss:		6.000731E-05

Epoch 93 of 500
  training loss:		3.422789E-05
  validation loss:		3.071527E-05

Epoch 94 of 500
  training loss:		3.224997E-05
  validation loss:		5.770543E-05

Epoch 95 of 500
  training loss:		3.476339E-05
  validation loss:		2.952261E-05

Epoch 96 of 500
  training loss:		3.292561E-05
  validation loss:		2.893963E-05

Epoch 97 of 500
  training loss:		3.544484E-05
  validation loss:		3.076183E-05

Epoch 98 of 500
  training loss:		3.239344E-05
  validation loss:		2.752208E-05

Epoch 99 of 500
  training loss:		3.070143E-05
  validation loss:		4.469361E-05

Epoch 100 of 500
  training loss:		3.240505E-05
  validation loss:		5.466639E-05

Epoch 101 of 500
  training loss:		2.999926E-05
  validation loss:		2.574803E-05

Epoch 102 of 500
  training loss:		3.004991E-05
  validation loss:		6.576955E-05

Epoch 103 of 500
  training loss:		3.070355E-05
  validation loss:		2.508924E-05

Epoch 104 of 500
  training loss:		2.937259E-05
  validation loss:		3.225203E-05

Epoch 105 of 500
  training loss:		2.942998E-05
  validation loss:		2.743668E-05

Epoch 106 of 500
  training loss:		2.639873E-05
  validation loss:		2.983136E-05

Epoch 107 of 500
  training loss:		2.855432E-05
  validation loss:		3.020134E-05

Epoch 108 of 500
  training loss:		2.627736E-05
  validation loss:		2.329766E-05

Epoch 109 of 500
  training loss:		2.614632E-05
  validation loss:		2.207557E-05

Epoch 110 of 500
  training loss:		2.664197E-05
  validation loss:		2.876458E-05

Epoch 111 of 500
  training loss:		2.598178E-05
  validation loss:		2.099878E-05

Epoch 112 of 500
  training loss:		2.415098E-05
  validation loss:		2.401302E-05

Epoch 113 of 500
  training loss:		2.610058E-05
  validation loss:		3.542987E-05

Epoch 114 of 500
  training loss:		2.576886E-05
  validation loss:		1.987971E-05

Epoch 115 of 500
  training loss:		2.541254E-05
  validation loss:		2.164516E-05

Epoch 116 of 500
  training loss:		2.563252E-05
  validation loss:		2.133974E-05

Epoch 117 of 500
  training loss:		2.234736E-05
  validation loss:		2.097856E-05

Epoch 118 of 500
  training loss:		2.206380E-05
  validation loss:		2.538393E-05

Epoch 119 of 500
  training loss:		2.480665E-05
  validation loss:		1.915796E-05

Epoch 120 of 500
  training loss:		2.111231E-05
  validation loss:		1.790273E-05

Epoch 121 of 500
  training loss:		2.263932E-05
  validation loss:		3.792621E-05

Epoch 122 of 500
  training loss:		2.503235E-05
  validation loss:		2.382512E-05

Epoch 123 of 500
  training loss:		2.285302E-05
  validation loss:		1.728070E-05

Epoch 124 of 500
  training loss:		1.949124E-05
  validation loss:		1.818846E-05

Epoch 125 of 500
  training loss:		1.964283E-05
  validation loss:		1.668407E-05

Epoch 126 of 500
  training loss:		1.937818E-05
  validation loss:		3.104708E-05

Epoch 127 of 500
  training loss:		2.194890E-05
  validation loss:		1.598988E-05

Epoch 128 of 500
  training loss:		1.932356E-05
  validation loss:		2.137534E-05

Epoch 129 of 500
  training loss:		2.017549E-05
  validation loss:		2.504664E-05

Epoch 130 of 500
  training loss:		1.902920E-05
  validation loss:		3.031883E-05

Early stopping, val-loss increased over the last 10 epochs from 0.000997609696246 to 0.00104580228598
Training-set, RMSE: 4.95010195482e-09
Validation-set, RMSE: 4.90497871805e-09
