Training network with 45280 training samples and 8490 validation samples
Epoch 1 of 500
  training loss:		4.390330E-02
  validation loss:		9.469318E-03

Epoch 2 of 500
  training loss:		7.730069E-03
  validation loss:		6.315619E-03

Epoch 3 of 500
  training loss:		4.963903E-03
  validation loss:		4.269042E-03

Epoch 4 of 500
  training loss:		2.898012E-03
  validation loss:		1.969323E-03

Epoch 5 of 500
  training loss:		1.327210E-03
  validation loss:		7.804081E-04

Epoch 6 of 500
  training loss:		6.132342E-04
  validation loss:		5.575984E-04

Epoch 7 of 500
  training loss:		3.578820E-04
  validation loss:		2.783031E-04

Epoch 8 of 500
  training loss:		2.248404E-04
  validation loss:		2.626683E-04

Epoch 9 of 500
  training loss:		1.601454E-04
  validation loss:		1.207554E-04

Epoch 10 of 500
  training loss:		1.165976E-04
  validation loss:		9.379253E-05

Epoch 11 of 500
  training loss:		9.702723E-05
  validation loss:		6.835117E-05

Epoch 12 of 500
  training loss:		7.677925E-05
  validation loss:		6.315664E-05

Epoch 13 of 500
  training loss:		6.675971E-05
  validation loss:		4.964993E-05

Epoch 14 of 500
  training loss:		5.801879E-05
  validation loss:		4.811420E-05

Epoch 15 of 500
  training loss:		5.165373E-05
  validation loss:		4.683896E-05

Epoch 16 of 500
  training loss:		4.625407E-05
  validation loss:		4.787586E-05

Epoch 17 of 500
  training loss:		3.969470E-05
  validation loss:		3.204915E-05

Epoch 18 of 500
  training loss:		3.739345E-05
  validation loss:		2.404414E-05

Epoch 19 of 500
  training loss:		3.411476E-05
  validation loss:		2.379925E-05

Epoch 20 of 500
  training loss:		3.182224E-05
  validation loss:		2.310790E-05

Epoch 21 of 500
  training loss:		2.925745E-05
  validation loss:		2.508136E-05

Epoch 22 of 500
  training loss:		2.476761E-05
  validation loss:		2.143788E-05

Epoch 23 of 500
  training loss:		2.473016E-05
  validation loss:		2.097418E-05

Epoch 24 of 500
  training loss:		2.268994E-05
  validation loss:		1.673076E-05

Epoch 25 of 500
  training loss:		2.020491E-05
  validation loss:		1.384149E-05

Epoch 26 of 500
  training loss:		1.955603E-05
  validation loss:		1.293074E-05

Epoch 27 of 500
  training loss:		1.815533E-05
  validation loss:		1.127965E-05

Epoch 28 of 500
  training loss:		1.816512E-05
  validation loss:		1.000918E-05

Epoch 29 of 500
  training loss:		1.592422E-05
  validation loss:		2.651922E-05

Epoch 30 of 500
  training loss:		1.534560E-05
  validation loss:		1.985390E-05

Epoch 31 of 500
  training loss:		1.495007E-05
  validation loss:		1.277053E-05

Epoch 32 of 500
  training loss:		1.378469E-05
  validation loss:		1.132644E-05

Epoch 33 of 500
  training loss:		1.393721E-05
  validation loss:		1.459576E-05

Epoch 34 of 500
  training loss:		1.297602E-05
  validation loss:		8.788856E-06

Epoch 35 of 500
  training loss:		1.171018E-05
  validation loss:		1.145974E-05

Epoch 36 of 500
  training loss:		1.194073E-05
  validation loss:		7.130485E-06

Epoch 37 of 500
  training loss:		1.037853E-05
  validation loss:		5.475197E-06

Epoch 38 of 500
  training loss:		1.050083E-05
  validation loss:		5.933718E-06

Epoch 39 of 500
  training loss:		9.676547E-06
  validation loss:		1.086290E-05

Epoch 40 of 500
  training loss:		9.528802E-06
  validation loss:		6.495274E-06

Epoch 41 of 500
  training loss:		9.437334E-06
  validation loss:		4.511806E-06

Epoch 42 of 500
  training loss:		8.321653E-06
  validation loss:		4.340856E-06

Epoch 43 of 500
  training loss:		8.212504E-06
  validation loss:		3.946587E-06

Epoch 44 of 500
  training loss:		8.150659E-06
  validation loss:		4.827809E-06

Epoch 45 of 500
  training loss:		7.754889E-06
  validation loss:		4.665098E-06

Epoch 46 of 500
  training loss:		7.268837E-06
  validation loss:		2.451692E-05

Epoch 47 of 500
  training loss:		7.151353E-06
  validation loss:		6.068723E-06

Epoch 48 of 500
  training loss:		7.249335E-06
  validation loss:		5.301883E-06

Epoch 49 of 500
  training loss:		6.462125E-06
  validation loss:		5.895975E-06

Epoch 50 of 500
  training loss:		6.105932E-06
  validation loss:		4.245364E-06

Epoch 51 of 500
  training loss:		5.891246E-06
  validation loss:		2.785587E-06

Epoch 52 of 500
  training loss:		6.039050E-06
  validation loss:		2.208268E-06

Epoch 53 of 500
  training loss:		5.978008E-06
  validation loss:		1.239978E-05

Epoch 54 of 500
  training loss:		5.647743E-06
  validation loss:		2.600482E-06

Epoch 55 of 500
  training loss:		5.495219E-06
  validation loss:		1.718898E-05

Epoch 56 of 500
  training loss:		5.347002E-06
  validation loss:		5.035233E-06

Epoch 57 of 500
  training loss:		5.283932E-06
  validation loss:		1.798310E-06

Epoch 58 of 500
  training loss:		5.400487E-06
  validation loss:		1.859047E-06

Epoch 59 of 500
  training loss:		4.334568E-06
  validation loss:		1.678665E-06

Epoch 60 of 500
  training loss:		4.799434E-06
  validation loss:		1.621782E-05

Epoch 61 of 500
  training loss:		4.618648E-06
  validation loss:		2.926674E-05

Epoch 62 of 500
  training loss:		4.758937E-06
  validation loss:		2.348870E-06

Epoch 63 of 500
  training loss:		4.557492E-06
  validation loss:		1.128454E-05

Epoch 64 of 500
  training loss:		4.240754E-06
  validation loss:		2.057119E-06

Epoch 65 of 500
  training loss:		4.127362E-06
  validation loss:		1.161936E-06

Epoch 66 of 500
  training loss:		4.699550E-06
  validation loss:		1.238113E-06

Epoch 67 of 500
  training loss:		3.883471E-06
  validation loss:		5.521564E-06

Epoch 68 of 500
  training loss:		3.987307E-06
  validation loss:		1.124073E-06

Epoch 69 of 500
  training loss:		4.206787E-06
  validation loss:		4.085588E-06

Epoch 70 of 500
  training loss:		3.994532E-06
  validation loss:		1.016842E-06

Epoch 71 of 500
  training loss:		3.874434E-06
  validation loss:		1.026210E-06

Epoch 72 of 500
  training loss:		4.049726E-06
  validation loss:		7.325683E-06

Epoch 73 of 500
  training loss:		3.599785E-06
  validation loss:		2.153681E-05

Epoch 74 of 500
  training loss:		3.966992E-06
  validation loss:		2.286324E-06

Epoch 75 of 500
  training loss:		3.493525E-06
  validation loss:		8.103836E-06

Epoch 76 of 500
  training loss:		3.969919E-06
  validation loss:		1.070355E-06

Epoch 77 of 500
  training loss:		3.951647E-06
  validation loss:		3.371758E-06

Epoch 78 of 500
  training loss:		3.538794E-06
  validation loss:		1.490032E-05

Epoch 79 of 500
  training loss:		3.534259E-06
  validation loss:		2.507978E-06

Epoch 80 of 500
  training loss:		3.393351E-06
  validation loss:		1.237972E-06

Epoch 81 of 500
  training loss:		3.576943E-06
  validation loss:		1.351489E-06

Epoch 82 of 500
  training loss:		3.504157E-06
  validation loss:		1.251341E-06

Epoch 83 of 500
  training loss:		3.600843E-06
  validation loss:		2.517192E-06

Epoch 84 of 500
  training loss:		3.406279E-06
  validation loss:		6.960054E-07

Epoch 85 of 500
  training loss:		3.544766E-06
  validation loss:		6.346852E-07

Epoch 86 of 500
  training loss:		3.171035E-06
  validation loss:		2.653480E-06

Epoch 87 of 500
  training loss:		3.426761E-06
  validation loss:		8.814077E-07

Epoch 88 of 500
  training loss:		3.263656E-06
  validation loss:		7.491392E-07

Epoch 89 of 500
  training loss:		3.301161E-06
  validation loss:		5.593550E-07

Epoch 90 of 500
  training loss:		2.996220E-06
  validation loss:		1.921317E-06

Epoch 91 of 500
  training loss:		3.592410E-06
  validation loss:		6.438337E-07

Epoch 92 of 500
  training loss:		3.134176E-06
  validation loss:		5.354869E-06

Epoch 93 of 500
  training loss:		3.396837E-06
  validation loss:		2.618249E-06

Epoch 94 of 500
  training loss:		2.871098E-06
  validation loss:		1.225875E-06

Epoch 95 of 500
  training loss:		3.413471E-06
  validation loss:		8.829772E-06

Epoch 96 of 500
  training loss:		3.105111E-06
  validation loss:		2.163778E-06

Epoch 97 of 500
  training loss:		3.170544E-06
  validation loss:		8.629341E-06

Epoch 98 of 500
  training loss:		2.949930E-06
  validation loss:		8.429033E-06

Epoch 99 of 500
  training loss:		3.251345E-06
  validation loss:		4.497236E-07

Epoch 100 of 500
  training loss:		2.955874E-06
  validation loss:		3.906959E-06

Epoch 101 of 500
  training loss:		3.148244E-06
  validation loss:		8.740357E-07

Epoch 102 of 500
  training loss:		3.032054E-06
  validation loss:		1.144328E-06

Epoch 103 of 500
  training loss:		2.686147E-06
  validation loss:		4.655165E-06

Epoch 104 of 500
  training loss:		3.330278E-06
  validation loss:		3.001372E-06

Epoch 105 of 500
  training loss:		2.998359E-06
  validation loss:		1.888950E-06

Epoch 106 of 500
  training loss:		2.855871E-06
  validation loss:		3.180564E-06

Epoch 107 of 500
  training loss:		2.952481E-06
  validation loss:		2.161757E-06

Epoch 108 of 500
  training loss:		2.829259E-06
  validation loss:		2.400272E-06

Epoch 109 of 500
  training loss:		2.956887E-06
  validation loss:		1.196141E-06

Epoch 110 of 500
  training loss:		3.006792E-06
  validation loss:		4.570566E-06

Epoch 111 of 500
  training loss:		2.775529E-06
  validation loss:		7.573524E-07

Epoch 112 of 500
  training loss:		2.947811E-06
  validation loss:		9.322541E-07

Epoch 113 of 500
  training loss:		2.843253E-06
  validation loss:		1.247169E-06

Epoch 114 of 500
  training loss:		2.665869E-06
  validation loss:		4.316102E-07

Epoch 115 of 500
  training loss:		2.817548E-06
  validation loss:		2.045809E-05

Epoch 116 of 500
  training loss:		3.128571E-06
  validation loss:		4.024899E-07

Epoch 117 of 500
  training loss:		2.656077E-06
  validation loss:		5.178148E-07

Epoch 118 of 500
  training loss:		2.904736E-06
  validation loss:		7.252660E-06

Epoch 119 of 500
  training loss:		2.860959E-06
  validation loss:		7.737341E-06

Epoch 120 of 500
  training loss:		2.750803E-06
  validation loss:		2.540116E-06

Early stopping, val-loss increased over the last 20 epochs from 0.000734935698695 to 0.000892388085368
Training RMSE: 2.7302625387e-09
Validation RMSE: 2.72578187216e-09
