Training network with 45280 training samples and 8490 validation samples
Epoch 1 of 100
  training loss:		8.039254E-03
  validation loss:		6.468670E-04

Epoch 2 of 100
  training loss:		3.697452E-04
  validation loss:		2.190491E-04

Epoch 3 of 100
  training loss:		1.682792E-04
  validation loss:		1.267326E-04

Epoch 4 of 100
  training loss:		1.090745E-04
  validation loss:		9.017105E-05

Epoch 5 of 100
  training loss:		7.503474E-05
  validation loss:		6.247264E-05

Epoch 6 of 100
  training loss:		5.133153E-05
  validation loss:		3.889760E-05

Epoch 7 of 100
  training loss:		3.322256E-05
  validation loss:		2.747483E-05

Epoch 8 of 100
  training loss:		2.249720E-05
  validation loss:		1.983880E-05

Epoch 9 of 100
  training loss:		1.488249E-05
  validation loss:		1.127628E-05

Epoch 10 of 100
  training loss:		9.775980E-06
  validation loss:		7.488765E-06

Epoch 11 of 100
  training loss:		6.515768E-06
  validation loss:		5.141667E-06

Epoch 12 of 100
  training loss:		5.191224E-06
  validation loss:		5.414212E-06

Epoch 13 of 100
  training loss:		3.328277E-06
  validation loss:		2.455032E-06

Epoch 14 of 100
  training loss:		2.570909E-06
  validation loss:		1.637388E-06

Epoch 15 of 100
  training loss:		2.011091E-06
  validation loss:		1.380492E-06

Epoch 16 of 100
  training loss:		1.709327E-06
  validation loss:		9.530612E-07

Epoch 17 of 100
  training loss:		1.326668E-06
  validation loss:		9.420017E-07

Epoch 18 of 100
  training loss:		1.135954E-06
  validation loss:		6.789040E-07

Epoch 19 of 100
  training loss:		8.222423E-07
  validation loss:		8.928918E-06

Epoch 20 of 100
  training loss:		1.012732E-06
  validation loss:		8.325489E-07

Epoch 21 of 100
  training loss:		9.210424E-07
  validation loss:		2.888572E-07

Epoch 22 of 100
  training loss:		4.121783E-06
  validation loss:		1.810687E-07

Epoch 23 of 100
  training loss:		2.848254E-07
  validation loss:		2.923940E-07

Epoch 24 of 100
  training loss:		2.824691E-07
  validation loss:		8.976887E-08

Epoch 25 of 100
  training loss:		3.214120E-06
  validation loss:		4.323340E-07

Epoch 26 of 100
  training loss:		4.214691E-07
  validation loss:		2.257910E-07

Epoch 27 of 100
  training loss:		3.595361E-06
  validation loss:		1.708928E-07

Epoch 28 of 100
  training loss:		3.186934E-07
  validation loss:		1.242477E-06

Epoch 29 of 100
  training loss:		1.048968E-06
  validation loss:		5.537133E-07

Epoch 30 of 100
  training loss:		1.931778E-06
  validation loss:		6.461062E-07

Epoch 31 of 100
  training loss:		1.234907E-06
  validation loss:		5.445981E-06

Epoch 32 of 100
  training loss:		3.693869E-06
  validation loss:		4.623864E-08

Epoch 33 of 100
  training loss:		2.400545E-08
  validation loss:		2.477637E-09

Epoch 34 of 100
  training loss:		1.396494E-06
  validation loss:		6.103262E-06

Epoch 35 of 100
  training loss:		1.034437E-06
  validation loss:		3.648581E-06

Epoch 36 of 100
  training loss:		2.540838E-06
  validation loss:		4.656329E-09

Epoch 37 of 100
  training loss:		3.570654E-09
  validation loss:		2.429949E-09

Epoch 38 of 100
  training loss:		3.777466E-06
  validation loss:		1.226503E-07

Epoch 39 of 100
  training loss:		2.745692E-07
  validation loss:		8.153351E-07

Epoch 40 of 100
  training loss:		1.682391E-06
  validation loss:		1.855750E-07

Epoch 41 of 100
  training loss:		4.778471E-07
  validation loss:		4.175879E-08

Epoch 42 of 100
  training loss:		1.636296E-06
  validation loss:		6.403783E-08

Epoch 43 of 100
  training loss:		9.410778E-07
  validation loss:		2.298442E-10

Epoch 44 of 100
  training loss:		6.824451E-06
  validation loss:		1.433617E-06

Epoch 45 of 100
  training loss:		6.994696E-08
  validation loss:		1.118007E-09

Epoch 46 of 100
  training loss:		6.396299E-10
  validation loss:		9.238336E-11

Epoch 47 of 100
  training loss:		8.879103E-08
  validation loss:		7.741184E-09

Epoch 48 of 100
  training loss:		8.129198E-06
  validation loss:		1.486131E-08

Epoch 49 of 100
  training loss:		6.728779E-09
  validation loss:		5.379347E-10

Epoch 50 of 100
  training loss:		2.283875E-09
  validation loss:		2.588979E-09

Epoch 51 of 100
  training loss:		1.306671E-07
  validation loss:		2.568048E-07

Epoch 52 of 100
  training loss:		8.137337E-06
  validation loss:		2.621656E-06

Epoch 53 of 100
  training loss:		6.906547E-08
  validation loss:		1.226193E-09

Epoch 54 of 100
  training loss:		1.405751E-10
  validation loss:		7.175964E-13

Epoch 55 of 100
  training loss:		3.111914E-13
  validation loss:		1.307935E-13

Epoch 56 of 100
  training loss:		6.308573E-14
  validation loss:		2.844588E-14

Epoch 57 of 100
  training loss:		1.864937E-14
  validation loss:		8.990412E-15

Epoch 58 of 100
  training loss:		6.354445E-15
  validation loss:		3.911051E-15

Epoch 59 of 100
  training loss:		2.691108E-15
  validation loss:		1.759249E-15

Epoch 60 of 100
  training loss:		1.203645E-15
  validation loss:		7.692390E-16

Epoch 61 of 100
  training loss:		5.397126E-16
  validation loss:		3.240449E-16

Epoch 62 of 100
  training loss:		2.435061E-16
  validation loss:		1.582033E-16

Epoch 63 of 100
  training loss:		1.213468E-16
  validation loss:		7.229732E-17

Epoch 64 of 100
  training loss:		5.645132E-17
  validation loss:		3.642489E-17

Epoch 65 of 100
  training loss:		2.771761E-17
  validation loss:		1.911137E-17

Epoch 66 of 100
  training loss:		1.445842E-17
  validation loss:		9.189379E-18

Epoch 67 of 100
  training loss:		7.204214E-18
  validation loss:		5.311587E-18

Epoch 68 of 100
  training loss:		3.698805E-18
  validation loss:		2.483074E-18

Epoch 69 of 100
  training loss:		1.798719E-18
  validation loss:		1.178773E-18

Epoch 70 of 100
  training loss:		9.185884E-19
  validation loss:		6.177203E-19

Epoch 71 of 100
  training loss:		4.836558E-19
  validation loss:		3.069511E-19

Epoch 72 of 100
  training loss:		2.526991E-19
  validation loss:		1.732449E-19

Epoch 73 of 100
  training loss:		1.372849E-19
  validation loss:		1.009864E-19

Epoch 74 of 100
  training loss:		7.048175E-20
  validation loss:		4.815688E-20

Epoch 75 of 100
  training loss:		3.714236E-20
  validation loss:		2.533092E-20

Epoch 76 of 100
  training loss:		1.954970E-20
  validation loss:		1.298604E-20

Epoch 77 of 100
  training loss:		1.096494E-20
  validation loss:		7.847600E-21

Epoch 78 of 100
  training loss:		6.035860E-21
  validation loss:		4.002154E-21

Epoch 79 of 100
  training loss:		3.464906E-21
  validation loss:		3.093257E-21

Epoch 80 of 100
  training loss:		1.960206E-21
  validation loss:		1.400337E-21

Epoch 81 of 100
  training loss:		1.202084E-21
  validation loss:		8.480749E-22

Epoch 82 of 100
  training loss:		7.802560E-22
  validation loss:		5.402680E-22

Epoch 83 of 100
  training loss:		4.831648E-22
  validation loss:		3.277815E-22

Epoch 84 of 100
  training loss:		3.095067E-22
  validation loss:		2.181660E-22

Epoch 85 of 100
  training loss:		2.016876E-22
  validation loss:		1.341557E-22

Epoch 86 of 100
  training loss:		1.310834E-22
  validation loss:		9.510720E-23

Epoch 87 of 100
  training loss:		8.889446E-23
  validation loss:		6.840179E-23

Epoch 88 of 100
  training loss:		6.171522E-23
  validation loss:		4.685252E-23

Epoch 89 of 100
  training loss:		4.202715E-23
  validation loss:		2.791175E-23

Epoch 90 of 100
  training loss:		1.212062E-04
  validation loss:		5.861475E-06

Epoch 91 of 100
  training loss:		4.021042E-06
  validation loss:		2.504673E-06

Epoch 92 of 100
  training loss:		2.132884E-06
  validation loss:		1.288509E-06

Epoch 93 of 100
  training loss:		1.187149E-06
  validation loss:		7.486555E-07

Epoch 94 of 100
  training loss:		6.568244E-07
  validation loss:		5.346246E-07

Epoch 95 of 100
  training loss:		5.149090E-07
  validation loss:		2.898706E-07

Epoch 96 of 100
  training loss:		3.471674E-07
  validation loss:		1.795367E-07

Epoch 97 of 100
  training loss:		2.527934E-07
  validation loss:		1.773304E-07

Epoch 98 of 100
  training loss:		2.264910E-07
  validation loss:		7.747164E-08

Epoch 99 of 100
  training loss:		1.491049E-07
  validation loss:		3.818772E-08

Epoch 100 of 100
  training loss:		1.100177E-07
  validation loss:		1.141197E-07

Training RMSE: 3.39994731877e-10
Validation RMSE: 3.32655643091e-10
