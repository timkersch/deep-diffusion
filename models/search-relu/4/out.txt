Training network with 45280 training samples and 8490 validation samples
Epoch 1 of 100
  training loss:		6.757307E-04
  validation loss:		3.242540E-05

Epoch 2 of 100
  training loss:		4.129574E-05
  validation loss:		1.926007E-05

Epoch 3 of 100
  training loss:		2.942338E-05
  validation loss:		7.915282E-04

Epoch 4 of 100
  training loss:		6.430917E-05
  validation loss:		7.803524E-06

Epoch 5 of 100
  training loss:		1.308224E-05
  validation loss:		1.031118E-04

Epoch 6 of 100
  training loss:		8.448089E-05
  validation loss:		4.273203E-06

Epoch 7 of 100
  training loss:		3.801750E-05
  validation loss:		5.907495E-06

Epoch 8 of 100
  training loss:		2.500743E-05
  validation loss:		2.872777E-06

Epoch 9 of 100
  training loss:		2.949244E-05
  validation loss:		1.704194E-05

Epoch 10 of 100
  training loss:		3.090539E-05
  validation loss:		4.728114E-06

Epoch 11 of 100
  training loss:		1.587563E-05
  validation loss:		1.008866E-05

Epoch 12 of 100
  training loss:		4.941622E-05
  validation loss:		2.838121E-06

Epoch 13 of 100
  training loss:		5.943483E-06
  validation loss:		5.856765E-07

Epoch 14 of 100
  training loss:		6.888163E-05
  validation loss:		7.846498E-07

Epoch 15 of 100
  training loss:		1.363658E-06
  validation loss:		3.829937E-07

Epoch 16 of 100
  training loss:		4.085715E-06
  validation loss:		1.762217E-06

Epoch 17 of 100
  training loss:		2.986534E-05
  validation loss:		1.667618E-06

Epoch 18 of 100
  training loss:		1.976560E-05
  validation loss:		2.316617E-06

Epoch 19 of 100
  training loss:		3.659944E-06
  validation loss:		3.405942E-06

Epoch 20 of 100
  training loss:		2.398673E-05
  validation loss:		8.140965E-06

Epoch 21 of 100
  training loss:		2.886795E-05
  validation loss:		7.948117E-05

Epoch 22 of 100
  training loss:		2.031550E-06
  validation loss:		3.408395E-06

Epoch 23 of 100
  training loss:		2.046608E-05
  validation loss:		2.954651E-07

Epoch 24 of 100
  training loss:		2.419372E-06
  validation loss:		2.543206E-07

Epoch 25 of 100
  training loss:		1.702084E-05
  validation loss:		1.475712E-06

Epoch 26 of 100
  training loss:		7.205351E-06
  validation loss:		1.375469E-06

Epoch 27 of 100
  training loss:		4.347518E-06
  validation loss:		4.945862E-06

Epoch 28 of 100
  training loss:		2.109376E-05
  validation loss:		7.116578E-07

Epoch 29 of 100
  training loss:		6.110803E-07
  validation loss:		3.954160E-07

Epoch 30 of 100
  training loss:		1.254650E-05
  validation loss:		6.698277E-07

Epoch 31 of 100
  training loss:		6.192078E-06
  validation loss:		3.406775E-06

Epoch 32 of 100
  training loss:		3.367528E-06
  validation loss:		4.833533E-05

Epoch 33 of 100
  training loss:		1.102146E-05
  validation loss:		5.020386E-06

Epoch 34 of 100
  training loss:		3.692251E-06
  validation loss:		1.508114E-06

Epoch 35 of 100
  training loss:		9.563483E-06
  validation loss:		1.544094E-05

Epoch 36 of 100
  training loss:		2.705438E-06
  validation loss:		6.773557E-09

Epoch 37 of 100
  training loss:		9.201873E-06
  validation loss:		6.213377E-08

Epoch 38 of 100
  training loss:		1.534546E-06
  validation loss:		8.342503E-06

Epoch 39 of 100
  training loss:		9.995106E-06
  validation loss:		2.517174E-07

Epoch 40 of 100
  training loss:		1.065004E-06
  validation loss:		9.424997E-08

Epoch 41 of 100
  training loss:		4.835575E-06
  validation loss:		1.545619E-06

Epoch 42 of 100
  training loss:		7.016493E-07
  validation loss:		1.582649E-05

Epoch 43 of 100
  training loss:		9.707137E-06
  validation loss:		7.980678E-06

Epoch 44 of 100
  training loss:		1.233853E-06
  validation loss:		2.484274E-08

Epoch 45 of 100
  training loss:		8.757611E-06
  validation loss:		5.519446E-07

Epoch 46 of 100
  training loss:		7.593682E-07
  validation loss:		9.946375E-08

Epoch 47 of 100
  training loss:		6.119935E-06
  validation loss:		1.417521E-09

Epoch 48 of 100
  training loss:		1.797280E-06
  validation loss:		3.568170E-06

Epoch 49 of 100
  training loss:		2.193713E-06
  validation loss:		3.836522E-07

Epoch 50 of 100
  training loss:		3.448723E-06
  validation loss:		2.871242E-05

Epoch 51 of 100
  training loss:		8.013148E-06
  validation loss:		2.771015E-09

Epoch 52 of 100
  training loss:		2.592855E-06
  validation loss:		3.220738E-06

Epoch 53 of 100
  training loss:		8.673487E-06
  validation loss:		6.386048E-07

Epoch 54 of 100
  training loss:		4.357724E-08
  validation loss:		1.428237E-11

Epoch 55 of 100
  training loss:		5.298855E-10
  validation loss:		9.788956E-09

Epoch 56 of 100
  training loss:		3.660956E-06
  validation loss:		8.788017E-07

Epoch 57 of 100
  training loss:		4.476791E-06
  validation loss:		2.574726E-07

Epoch 58 of 100
  training loss:		2.440495E-07
  validation loss:		3.813867E-07

Epoch 59 of 100
  training loss:		5.736370E-06
  validation loss:		4.253087E-07

Epoch 60 of 100
  training loss:		2.398523E-08
  validation loss:		1.927696E-08

Epoch 61 of 100
  training loss:		4.977073E-06
  validation loss:		6.025532E-07

Epoch 62 of 100
  training loss:		2.127214E-06
  validation loss:		3.015583E-05

Epoch 63 of 100
  training loss:		3.620320E-06
  validation loss:		8.560176E-11

Epoch 64 of 100
  training loss:		5.207692E-07
  validation loss:		2.252411E-06

Epoch 65 of 100
  training loss:		3.371203E-06
  validation loss:		1.975040E-08

Epoch 66 of 100
  training loss:		1.982205E-06
  validation loss:		4.091497E-06

Epoch 67 of 100
  training loss:		1.190144E-06
  validation loss:		4.564976E-09

Epoch 68 of 100
  training loss:		2.964785E-06
  validation loss:		2.955027E-08

Epoch 69 of 100
  training loss:		3.296453E-06
  validation loss:		1.679026E-05

Epoch 70 of 100
  training loss:		2.814454E-06
  validation loss:		8.473848E-08

Epoch 71 of 100
  training loss:		4.326463E-08
  validation loss:		2.293703E-07

Epoch 72 of 100
  training loss:		3.375244E-06
  validation loss:		2.941830E-07

Epoch 73 of 100
  training loss:		2.704503E-06
  validation loss:		9.253895E-05

Epoch 74 of 100
  training loss:		2.118464E-06
  validation loss:		1.111012E-08

Epoch 75 of 100
  training loss:		9.416194E-07
  validation loss:		3.204358E-05

Epoch 76 of 100
  training loss:		4.121263E-06
  validation loss:		3.414337E-10

Epoch 77 of 100
  training loss:		4.282444E-08
  validation loss:		1.211648E-06

Epoch 78 of 100
  training loss:		3.794640E-06
  validation loss:		1.446835E-11

Epoch 79 of 100
  training loss:		8.508452E-13
  validation loss:		8.041780E-16

Epoch 80 of 100
  training loss:		4.119495E-16
  validation loss:		1.094768E-16

Epoch 81 of 100
  training loss:		5.877080E-17
  validation loss:		2.057477E-17

Epoch 82 of 100
  training loss:		1.221288E-17
  validation loss:		4.881867E-18

Epoch 83 of 100
  training loss:		2.967579E-18
  validation loss:		1.243616E-18

Epoch 84 of 100
  training loss:		7.544981E-19
  validation loss:		3.082452E-19

Epoch 85 of 100
  training loss:		2.006140E-19
  validation loss:		9.218936E-20

Epoch 86 of 100
  training loss:		5.611832E-20
  validation loss:		2.488148E-20

Epoch 87 of 100
  training loss:		1.632885E-20
  validation loss:		7.763567E-21

Epoch 88 of 100
  training loss:		4.661966E-21
  validation loss:		2.361701E-21

Epoch 89 of 100
  training loss:		1.516331E-21
  validation loss:		7.611773E-22

Epoch 90 of 100
  training loss:		4.876489E-22
  validation loss:		2.763611E-22

Epoch 91 of 100
  training loss:		1.811648E-22
  validation loss:		1.021829E-22

Epoch 92 of 100
  training loss:		7.293192E-23
  validation loss:		3.866494E-23

Epoch 93 of 100
  training loss:		3.025049E-23
  validation loss:		1.727514E-23

Epoch 94 of 100
  training loss:		1.555710E-23
  validation loss:		9.758875E-24

Epoch 95 of 100
  training loss:		3.134285E-05
  validation loss:		5.265204E-06

Epoch 96 of 100
  training loss:		9.225284E-07
  validation loss:		2.058273E-07

Epoch 97 of 100
  training loss:		1.197447E-07
  validation loss:		4.024554E-08

Epoch 98 of 100
  training loss:		1.397599E-07
  validation loss:		2.782148E-08

Epoch 99 of 100
  training loss:		1.158863E-06
  validation loss:		2.992048E-07

Epoch 100 of 100
  training loss:		1.042069E-07
  validation loss:		4.032438E-07

Training RMSE: 6.19413067059e-10
Validation RMSE: 6.21945031376e-10
