Training network with 45280 training samples and 8490 validation samples
Epoch 1 of 100
  training loss:		5.945285E-02
  validation loss:		1.775734E-03

Epoch 2 of 100
  training loss:		9.943276E-04
  validation loss:		5.168588E-04

Epoch 3 of 100
  training loss:		3.511979E-04
  validation loss:		2.322514E-04

Epoch 4 of 100
  training loss:		2.021505E-04
  validation loss:		1.614092E-04

Epoch 5 of 100
  training loss:		1.420209E-04
  validation loss:		1.135180E-04

Epoch 6 of 100
  training loss:		1.072461E-04
  validation loss:		8.624880E-05

Epoch 7 of 100
  training loss:		8.121137E-05
  validation loss:		6.516404E-05

Epoch 8 of 100
  training loss:		6.184610E-05
  validation loss:		4.833639E-05

Epoch 9 of 100
  training loss:		4.637542E-05
  validation loss:		4.544443E-05

Epoch 10 of 100
  training loss:		3.433573E-05
  validation loss:		2.530625E-05

Epoch 11 of 100
  training loss:		2.422175E-05
  validation loss:		2.011732E-05

Epoch 12 of 100
  training loss:		1.888323E-05
  validation loss:		1.537059E-05

Epoch 13 of 100
  training loss:		1.485123E-05
  validation loss:		1.365830E-05

Epoch 14 of 100
  training loss:		1.129483E-05
  validation loss:		9.133593E-06

Epoch 15 of 100
  training loss:		9.053532E-06
  validation loss:		7.309934E-06

Epoch 16 of 100
  training loss:		6.812680E-06
  validation loss:		5.735161E-06

Epoch 17 of 100
  training loss:		5.297508E-06
  validation loss:		3.866250E-06

Epoch 18 of 100
  training loss:		3.977238E-06
  validation loss:		3.166022E-06

Epoch 19 of 100
  training loss:		3.057400E-06
  validation loss:		2.487318E-06

Epoch 20 of 100
  training loss:		2.407874E-06
  validation loss:		2.397519E-06

Epoch 21 of 100
  training loss:		1.822203E-06
  validation loss:		1.191319E-06

Epoch 22 of 100
  training loss:		1.148187E-06
  validation loss:		6.792801E-07

Epoch 23 of 100
  training loss:		9.706253E-07
  validation loss:		4.933046E-07

Epoch 24 of 100
  training loss:		7.566904E-07
  validation loss:		2.943342E-07

Epoch 25 of 100
  training loss:		4.619083E-07
  validation loss:		1.549858E-06

Epoch 26 of 100
  training loss:		6.309031E-07
  validation loss:		1.821900E-07

Epoch 27 of 100
  training loss:		1.057401E-06
  validation loss:		5.397964E-07

Epoch 28 of 100
  training loss:		1.190923E-07
  validation loss:		2.718892E-08

Epoch 29 of 100
  training loss:		5.105418E-07
  validation loss:		6.018099E-06

Epoch 30 of 100
  training loss:		2.453214E-06
  validation loss:		6.698562E-06

Epoch 31 of 100
  training loss:		2.003608E-06
  validation loss:		1.798191E-07

Epoch 32 of 100
  training loss:		2.161446E-08
  validation loss:		7.284901E-09

Epoch 33 of 100
  training loss:		1.312180E-06
  validation loss:		3.678231E-08

Epoch 34 of 100
  training loss:		3.423116E-06
  validation loss:		2.999037E-06

Epoch 35 of 100
  training loss:		2.503813E-07
  validation loss:		7.061705E-08

Epoch 36 of 100
  training loss:		3.581034E-06
  validation loss:		1.119071E-05

Epoch 37 of 100
  training loss:		9.048999E-07
  validation loss:		1.157149E-09

Epoch 38 of 100
  training loss:		4.832746E-08
  validation loss:		3.555520E-07

Epoch 39 of 100
  training loss:		6.028793E-06
  validation loss:		4.244323E-08

Epoch 40 of 100
  training loss:		3.206553E-08
  validation loss:		1.638437E-09

Epoch 41 of 100
  training loss:		2.159480E-09
  validation loss:		1.473325E-10

Epoch 42 of 100
  training loss:		5.265869E-08
  validation loss:		1.553481E-07

Epoch 43 of 100
  training loss:		3.072778E-06
  validation loss:		8.564517E-07

Epoch 44 of 100
  training loss:		3.390428E-07
  validation loss:		2.332411E-07

Epoch 45 of 100
  training loss:		3.002606E-06
  validation loss:		3.138892E-08

Epoch 46 of 100
  training loss:		5.105210E-08
  validation loss:		2.646910E-09

Epoch 47 of 100
  training loss:		3.683368E-08
  validation loss:		1.303063E-07

Epoch 48 of 100
  training loss:		4.537183E-06
  validation loss:		3.934893E-09

Epoch 49 of 100
  training loss:		2.205534E-09
  validation loss:		5.429572E-10

Epoch 50 of 100
  training loss:		5.626345E-10
  validation loss:		2.820376E-09

Epoch 51 of 100
  training loss:		2.935592E-06
  validation loss:		5.144146E-08

Epoch 52 of 100
  training loss:		5.311931E-08
  validation loss:		2.948688E-06

Epoch 53 of 100
  training loss:		3.553656E-06
  validation loss:		6.200331E-08

Epoch 54 of 100
  training loss:		2.837708E-09
  validation loss:		2.839650E-12

Epoch 55 of 100
  training loss:		9.032739E-13
  validation loss:		2.070988E-13

Epoch 56 of 100
  training loss:		1.309848E-13
  validation loss:		6.003287E-14

Epoch 57 of 100
  training loss:		4.127539E-14
  validation loss:		2.043952E-14

Epoch 58 of 100
  training loss:		1.524321E-14
  validation loss:		7.783910E-15

Epoch 59 of 100
  training loss:		5.540267E-15
  validation loss:		2.934442E-15

Epoch 60 of 100
  training loss:		2.354689E-15
  validation loss:		1.406454E-15

Early stopping, val-loss increased over the last 10 epochs from 9.35106952346e-06 to 2.02100962409e-05
Training RMSE: 6.03476853182e-14
Validation RMSE: 5.79430092968e-14
