Training network with 45280 training samples and 8490 validation samples
Epoch 1 of 100
  training loss:		1.394893E-02
  validation loss:		2.023527E-03

Epoch 2 of 100
  training loss:		1.363326E-03
  validation loss:		8.802881E-04

Epoch 3 of 100
  training loss:		6.472119E-04
  validation loss:		4.745855E-04

Epoch 4 of 100
  training loss:		3.691207E-04
  validation loss:		2.907765E-04

Epoch 5 of 100
  training loss:		2.501987E-04
  validation loss:		2.242119E-04

Epoch 6 of 100
  training loss:		1.950358E-04
  validation loss:		1.837786E-04

Epoch 7 of 100
  training loss:		1.586881E-04
  validation loss:		1.468090E-04

Epoch 8 of 100
  training loss:		1.323009E-04
  validation loss:		1.259028E-04

Epoch 9 of 100
  training loss:		1.129888E-04
  validation loss:		1.071697E-04

Epoch 10 of 100
  training loss:		9.596936E-05
  validation loss:		9.109812E-05

Epoch 11 of 100
  training loss:		8.138753E-05
  validation loss:		7.526419E-05

Epoch 12 of 100
  training loss:		6.851732E-05
  validation loss:		6.611183E-05

Epoch 13 of 100
  training loss:		5.790878E-05
  validation loss:		5.522883E-05

Epoch 14 of 100
  training loss:		5.083886E-05
  validation loss:		4.898288E-05

Epoch 15 of 100
  training loss:		4.386708E-05
  validation loss:		4.199500E-05

Epoch 16 of 100
  training loss:		3.849739E-05
  validation loss:		3.660216E-05

Epoch 17 of 100
  training loss:		3.369045E-05
  validation loss:		3.054692E-05

Epoch 18 of 100
  training loss:		3.004679E-05
  validation loss:		2.815607E-05

Epoch 19 of 100
  training loss:		2.602393E-05
  validation loss:		2.525265E-05

Epoch 20 of 100
  training loss:		2.257615E-05
  validation loss:		2.056644E-05

Epoch 21 of 100
  training loss:		1.914407E-05
  validation loss:		1.952075E-05

Epoch 22 of 100
  training loss:		1.647621E-05
  validation loss:		1.591625E-05

Epoch 23 of 100
  training loss:		1.449877E-05
  validation loss:		1.263006E-05

Epoch 24 of 100
  training loss:		1.250242E-05
  validation loss:		1.090850E-05

Epoch 25 of 100
  training loss:		1.066300E-05
  validation loss:		9.325657E-06

Epoch 26 of 100
  training loss:		9.155393E-06
  validation loss:		8.563242E-06

Epoch 27 of 100
  training loss:		7.547859E-06
  validation loss:		9.410738E-06

Epoch 28 of 100
  training loss:		6.803785E-06
  validation loss:		7.128060E-06

Epoch 29 of 100
  training loss:		5.790132E-06
  validation loss:		4.852788E-06

Epoch 30 of 100
  training loss:		5.348387E-06
  validation loss:		5.180489E-06

Epoch 31 of 100
  training loss:		4.679970E-06
  validation loss:		5.076223E-06

Epoch 32 of 100
  training loss:		3.771594E-06
  validation loss:		3.080798E-06

Epoch 33 of 100
  training loss:		3.460578E-06
  validation loss:		2.358123E-06

Epoch 34 of 100
  training loss:		2.588028E-06
  validation loss:		2.047534E-06

Epoch 35 of 100
  training loss:		2.256602E-06
  validation loss:		1.933980E-06

Epoch 36 of 100
  training loss:		2.018665E-06
  validation loss:		1.694431E-06

Epoch 37 of 100
  training loss:		1.890635E-06
  validation loss:		2.074279E-06

Epoch 38 of 100
  training loss:		1.342432E-06
  validation loss:		1.651686E-06

Epoch 39 of 100
  training loss:		1.201349E-06
  validation loss:		7.740638E-07

Epoch 40 of 100
  training loss:		9.823085E-07
  validation loss:		6.103128E-07

Epoch 41 of 100
  training loss:		8.345003E-07
  validation loss:		8.777602E-07

Epoch 42 of 100
  training loss:		7.313857E-07
  validation loss:		3.092030E-06

Epoch 43 of 100
  training loss:		7.036068E-07
  validation loss:		2.899973E-07

Epoch 44 of 100
  training loss:		4.845050E-07
  validation loss:		5.743055E-07

Epoch 45 of 100
  training loss:		5.007490E-07
  validation loss:		2.557830E-07

Epoch 46 of 100
  training loss:		3.737423E-07
  validation loss:		8.685389E-07

Epoch 47 of 100
  training loss:		5.623283E-07
  validation loss:		4.991063E-07

Epoch 48 of 100
  training loss:		3.940747E-07
  validation loss:		2.876897E-07

Epoch 49 of 100
  training loss:		6.857939E-07
  validation loss:		1.408164E-07

Epoch 50 of 100
  training loss:		7.635982E-07
  validation loss:		5.195527E-08

Epoch 51 of 100
  training loss:		4.428708E-07
  validation loss:		1.775996E-06

Epoch 52 of 100
  training loss:		7.631113E-07
  validation loss:		5.686565E-07

Epoch 53 of 100
  training loss:		3.717827E-07
  validation loss:		7.266453E-08

Epoch 54 of 100
  training loss:		1.155329E-07
  validation loss:		3.417697E-06

Epoch 55 of 100
  training loss:		2.179880E-06
  validation loss:		1.110643E-05

Epoch 56 of 100
  training loss:		7.901101E-07
  validation loss:		3.702205E-08

Epoch 57 of 100
  training loss:		3.304125E-08
  validation loss:		3.883251E-08

Epoch 58 of 100
  training loss:		1.675308E-08
  validation loss:		3.159128E-09

Epoch 59 of 100
  training loss:		1.069334E-07
  validation loss:		2.433382E-07

Epoch 60 of 100
  training loss:		8.689845E-06
  validation loss:		2.566703E-07

Early stopping, val-loss increased over the last 10 epochs from 2.28953420185e-05 to 5.78175433441e-05
Training RMSE: 4.83866837667e-10
Validation RMSE: 4.83779334028e-10
