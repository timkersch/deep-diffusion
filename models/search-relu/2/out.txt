Training network with 45280 training samples and 8490 validation samples
Epoch 1 of 100
  training loss:		6.162644E-03
  validation loss:		4.578829E-04

Epoch 2 of 100
  training loss:		2.512420E-04
  validation loss:		1.366084E-04

Epoch 3 of 100
  training loss:		1.008859E-04
  validation loss:		7.547216E-05

Epoch 4 of 100
  training loss:		5.676418E-05
  validation loss:		4.123718E-05

Epoch 5 of 100
  training loss:		3.424380E-05
  validation loss:		2.587393E-05

Epoch 6 of 100
  training loss:		2.186497E-05
  validation loss:		1.513266E-05

Epoch 7 of 100
  training loss:		1.390837E-05
  validation loss:		9.368183E-06

Epoch 8 of 100
  training loss:		9.059030E-06
  validation loss:		6.090088E-06

Epoch 9 of 100
  training loss:		6.396186E-06
  validation loss:		6.073849E-06

Epoch 10 of 100
  training loss:		4.443700E-06
  validation loss:		2.562273E-06

Epoch 11 of 100
  training loss:		3.480784E-06
  validation loss:		2.018171E-05

Epoch 12 of 100
  training loss:		2.651595E-06
  validation loss:		3.857053E-06

Epoch 13 of 100
  training loss:		2.022018E-06
  validation loss:		7.649092E-07

Epoch 14 of 100
  training loss:		2.243044E-06
  validation loss:		7.118391E-07

Epoch 15 of 100
  training loss:		2.562918E-06
  validation loss:		3.112623E-07

Epoch 16 of 100
  training loss:		1.583289E-06
  validation loss:		3.259789E-07

Epoch 17 of 100
  training loss:		7.386407E-07
  validation loss:		3.665863E-07

Epoch 18 of 100
  training loss:		9.790184E-07
  validation loss:		1.052804E-06

Epoch 19 of 100
  training loss:		7.640822E-07
  validation loss:		1.571003E-05

Epoch 20 of 100
  training loss:		2.885960E-06
  validation loss:		6.850847E-08

Epoch 21 of 100
  training loss:		6.385271E-08
  validation loss:		1.782088E-08

Epoch 22 of 100
  training loss:		1.078367E-06
  validation loss:		1.700140E-06

Epoch 23 of 100
  training loss:		1.011329E-06
  validation loss:		2.054787E-08

Epoch 24 of 100
  training loss:		1.506284E-07
  validation loss:		1.955774E-08

Epoch 25 of 100
  training loss:		2.010492E-06
  validation loss:		5.339236E-09

Epoch 26 of 100
  training loss:		3.802982E-07
  validation loss:		6.933733E-07

Epoch 27 of 100
  training loss:		8.097577E-07
  validation loss:		6.178616E-07

Epoch 28 of 100
  training loss:		1.431667E-06
  validation loss:		4.723076E-09

Epoch 29 of 100
  training loss:		4.975578E-07
  validation loss:		2.684701E-07

Epoch 30 of 100
  training loss:		3.486997E-06
  validation loss:		1.893237E-08

Epoch 31 of 100
  training loss:		1.597068E-08
  validation loss:		1.869182E-09

Epoch 32 of 100
  training loss:		8.603344E-07
  validation loss:		2.811734E-08

Epoch 33 of 100
  training loss:		1.802346E-07
  validation loss:		2.776745E-07

Epoch 34 of 100
  training loss:		7.836887E-07
  validation loss:		5.167738E-10

Epoch 35 of 100
  training loss:		2.691003E-06
  validation loss:		4.034377E-08

Epoch 36 of 100
  training loss:		6.463175E-09
  validation loss:		1.880718E-10

Epoch 37 of 100
  training loss:		1.348671E-06
  validation loss:		3.468249E-06

Epoch 38 of 100
  training loss:		7.974246E-08
  validation loss:		2.830233E-10

Epoch 39 of 100
  training loss:		1.993066E-06
  validation loss:		3.752853E-11

Epoch 40 of 100
  training loss:		3.080637E-11
  validation loss:		1.252319E-12

Epoch 41 of 100
  training loss:		5.399973E-13
  validation loss:		1.537002E-13

Epoch 42 of 100
  training loss:		7.393259E-14
  validation loss:		2.421492E-14

Epoch 43 of 100
  training loss:		1.440419E-14
  validation loss:		4.880697E-15

Epoch 44 of 100
  training loss:		2.669832E-15
  validation loss:		9.780728E-16

Epoch 45 of 100
  training loss:		5.395107E-16
  validation loss:		2.180978E-16

Epoch 46 of 100
  training loss:		1.211003E-16
  validation loss:		5.498503E-17

Epoch 47 of 100
  training loss:		3.360253E-17
  validation loss:		1.645508E-17

Epoch 48 of 100
  training loss:		8.038013E-18
  validation loss:		3.966449E-18

Epoch 49 of 100
  training loss:		2.420770E-18
  validation loss:		9.998293E-19

Epoch 50 of 100
  training loss:		7.114872E-19
  validation loss:		3.542265E-19

Epoch 51 of 100
  training loss:		1.837339E-19
  validation loss:		6.361744E-20

Epoch 52 of 100
  training loss:		3.955764E-20
  validation loss:		1.929658E-20

Epoch 53 of 100
  training loss:		1.564450E-20
  validation loss:		4.371706E-21

Epoch 54 of 100
  training loss:		3.847828E-21
  validation loss:		1.826834E-21

Epoch 55 of 100
  training loss:		1.217270E-21
  validation loss:		4.996065E-22

Epoch 56 of 100
  training loss:		3.870563E-13
  validation loss:		9.423578E-05

Epoch 57 of 100
  training loss:		2.371364E-05
  validation loss:		6.544674E-08

Epoch 58 of 100
  training loss:		2.371923E-08
  validation loss:		7.266186E-09

Epoch 59 of 100
  training loss:		4.958913E-09
  validation loss:		2.000144E-09

Epoch 60 of 100
  training loss:		7.004534E-10
  validation loss:		8.217925E-11

Epoch 61 of 100
  training loss:		3.970706E-11
  validation loss:		4.995384E-12

Epoch 62 of 100
  training loss:		1.987814E-06
  validation loss:		6.592948E-09

Epoch 63 of 100
  training loss:		2.101245E-09
  validation loss:		7.101447E-11

Epoch 64 of 100
  training loss:		9.179481E-07
  validation loss:		4.859868E-07

Epoch 65 of 100
  training loss:		3.467537E-07
  validation loss:		2.620155E-06

Epoch 66 of 100
  training loss:		1.114969E-06
  validation loss:		7.512126E-09

Epoch 67 of 100
  training loss:		1.417454E-06
  validation loss:		2.502396E-09

Epoch 68 of 100
  training loss:		4.360450E-09
  validation loss:		4.682348E-09

Epoch 69 of 100
  training loss:		3.865431E-06
  validation loss:		1.191046E-08

Epoch 70 of 100
  training loss:		4.888960E-09
  validation loss:		3.570872E-10

Epoch 71 of 100
  training loss:		2.649317E-08
  validation loss:		1.177269E-07

Epoch 72 of 100
  training loss:		2.278383E-06
  validation loss:		2.445107E-09

Epoch 73 of 100
  training loss:		2.405061E-09
  validation loss:		8.557893E-10

Epoch 74 of 100
  training loss:		9.996016E-07
  validation loss:		1.285218E-09

Epoch 75 of 100
  training loss:		2.184382E-06
  validation loss:		1.661766E-06

Epoch 76 of 100
  training loss:		4.355075E-07
  validation loss:		4.529840E-08

Epoch 77 of 100
  training loss:		9.336153E-08
  validation loss:		3.186028E-06

Epoch 78 of 100
  training loss:		1.700472E-06
  validation loss:		8.153967E-09

Epoch 79 of 100
  training loss:		7.258180E-07
  validation loss:		5.974089E-08

Epoch 80 of 100
  training loss:		7.531604E-08
  validation loss:		1.784446E-07

Epoch 81 of 100
  training loss:		1.778834E-06
  validation loss:		1.295728E-08

Epoch 82 of 100
  training loss:		1.832167E-08
  validation loss:		4.459664E-10

Epoch 83 of 100
  training loss:		1.579598E-06
  validation loss:		1.202284E-09

Epoch 84 of 100
  training loss:		5.407169E-09
  validation loss:		4.042161E-09

Epoch 85 of 100
  training loss:		9.243319E-07
  validation loss:		5.884914E-07

Epoch 86 of 100
  training loss:		1.505938E-06
  validation loss:		2.138999E-09

Epoch 87 of 100
  training loss:		3.611455E-07
  validation loss:		3.858327E-06

Epoch 88 of 100
  training loss:		1.496919E-06
  validation loss:		4.427573E-09

Epoch 89 of 100
  training loss:		7.946647E-09
  validation loss:		8.349667E-09

Epoch 90 of 100
  training loss:		1.878373E-06
  validation loss:		7.435415E-09

Epoch 91 of 100
  training loss:		1.132040E-09
  validation loss:		3.647237E-09

Epoch 92 of 100
  training loss:		3.393316E-06
  validation loss:		8.682108E-08

Epoch 93 of 100
  training loss:		5.719217E-08
  validation loss:		7.331186E-09

Epoch 94 of 100
  training loss:		2.079347E-07
  validation loss:		7.080282E-08

Epoch 95 of 100
  training loss:		1.654388E-06
  validation loss:		7.974722E-08

Epoch 96 of 100
  training loss:		1.315689E-07
  validation loss:		8.757203E-09

Epoch 97 of 100
  training loss:		8.127909E-07
  validation loss:		1.445498E-06

Epoch 98 of 100
  training loss:		1.475002E-07
  validation loss:		2.345533E-07

Epoch 99 of 100
  training loss:		2.427180E-06
  validation loss:		3.672811E-10

Epoch 100 of 100
  training loss:		2.473733E-10
  validation loss:		1.766919E-11

Training RMSE: 4.17029100791e-12
Validation RMSE: 4.12382486804e-12
