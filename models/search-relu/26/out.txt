Training network with 45280 training samples and 8490 validation samples
Epoch 1 of 100
  training loss:		1.074610E-01
  validation loss:		2.299852E-02

Epoch 2 of 100
  training loss:		1.083512E-02
  validation loss:		6.774458E-03

Epoch 3 of 100
  training loss:		5.866703E-03
  validation loss:		5.539322E-03

Epoch 4 of 100
  training loss:		4.893346E-03
  validation loss:		4.632676E-03

Epoch 5 of 100
  training loss:		4.075088E-03
  validation loss:		3.842158E-03

Epoch 6 of 100
  training loss:		3.363614E-03
  validation loss:		3.170392E-03

Epoch 7 of 100
  training loss:		2.785286E-03
  validation loss:		2.634095E-03

Epoch 8 of 100
  training loss:		2.326774E-03
  validation loss:		2.214121E-03

Epoch 9 of 100
  training loss:		1.956616E-03
  validation loss:		1.853911E-03

Epoch 10 of 100
  training loss:		1.650507E-03
  validation loss:		1.570809E-03

Epoch 11 of 100
  training loss:		1.389692E-03
  validation loss:		1.320291E-03

Epoch 12 of 100
  training loss:		1.167801E-03
  validation loss:		1.111161E-03

Epoch 13 of 100
  training loss:		9.820819E-04
  validation loss:		9.356618E-04

Epoch 14 of 100
  training loss:		8.299441E-04
  validation loss:		7.885954E-04

Epoch 15 of 100
  training loss:		6.997878E-04
  validation loss:		6.641434E-04

Epoch 16 of 100
  training loss:		5.842924E-04
  validation loss:		5.542482E-04

Epoch 17 of 100
  training loss:		4.896018E-04
  validation loss:		4.679070E-04

Epoch 18 of 100
  training loss:		4.183839E-04
  validation loss:		4.016904E-04

Epoch 19 of 100
  training loss:		3.597225E-04
  validation loss:		3.468405E-04

Epoch 20 of 100
  training loss:		3.038682E-04
  validation loss:		2.848250E-04

Epoch 21 of 100
  training loss:		2.513068E-04
  validation loss:		2.425538E-04

Epoch 22 of 100
  training loss:		2.160831E-04
  validation loss:		2.086639E-04

Epoch 23 of 100
  training loss:		1.861309E-04
  validation loss:		1.783917E-04

Epoch 24 of 100
  training loss:		1.615216E-04
  validation loss:		1.583381E-04

Epoch 25 of 100
  training loss:		1.435131E-04
  validation loss:		1.418606E-04

Epoch 26 of 100
  training loss:		1.297536E-04
  validation loss:		1.290848E-04

Epoch 27 of 100
  training loss:		1.184945E-04
  validation loss:		1.183895E-04

Epoch 28 of 100
  training loss:		1.090094E-04
  validation loss:		1.100462E-04

Epoch 29 of 100
  training loss:		1.012172E-04
  validation loss:		1.017151E-04

Epoch 30 of 100
  training loss:		9.389786E-05
  validation loss:		9.417217E-05

Epoch 31 of 100
  training loss:		8.682948E-05
  validation loss:		8.714990E-05

Epoch 32 of 100
  training loss:		8.051591E-05
  validation loss:		8.086358E-05

Epoch 33 of 100
  training loss:		7.454424E-05
  validation loss:		7.514240E-05

Epoch 34 of 100
  training loss:		6.929369E-05
  validation loss:		7.006785E-05

Epoch 35 of 100
  training loss:		6.422645E-05
  validation loss:		6.446060E-05

Epoch 36 of 100
  training loss:		5.926429E-05
  validation loss:		6.001654E-05

Epoch 37 of 100
  training loss:		5.493878E-05
  validation loss:		5.637420E-05

Epoch 38 of 100
  training loss:		5.153042E-05
  validation loss:		5.245858E-05

Epoch 39 of 100
  training loss:		4.811795E-05
  validation loss:		4.883883E-05

Epoch 40 of 100
  training loss:		4.488989E-05
  validation loss:		4.598516E-05

Epoch 41 of 100
  training loss:		4.191077E-05
  validation loss:		4.290917E-05

Epoch 42 of 100
  training loss:		3.883947E-05
  validation loss:		3.922560E-05

Epoch 43 of 100
  training loss:		3.628213E-05
  validation loss:		3.714064E-05

Epoch 44 of 100
  training loss:		3.363538E-05
  validation loss:		3.567707E-05

Epoch 45 of 100
  training loss:		3.134526E-05
  validation loss:		3.225180E-05

Epoch 46 of 100
  training loss:		2.915314E-05
  validation loss:		3.046847E-05

Epoch 47 of 100
  training loss:		2.703929E-05
  validation loss:		2.819173E-05

Epoch 48 of 100
  training loss:		2.520355E-05
  validation loss:		2.593933E-05

Epoch 49 of 100
  training loss:		2.324757E-05
  validation loss:		2.362903E-05

Epoch 50 of 100
  training loss:		2.166817E-05
  validation loss:		2.237214E-05

Epoch 51 of 100
  training loss:		2.005138E-05
  validation loss:		2.045168E-05

Epoch 52 of 100
  training loss:		1.816719E-05
  validation loss:		1.873679E-05

Epoch 53 of 100
  training loss:		1.684970E-05
  validation loss:		1.726551E-05

Epoch 54 of 100
  training loss:		1.563307E-05
  validation loss:		1.588091E-05

Epoch 55 of 100
  training loss:		1.458541E-05
  validation loss:		1.457664E-05

Epoch 56 of 100
  training loss:		1.355926E-05
  validation loss:		1.362323E-05

Epoch 57 of 100
  training loss:		1.258622E-05
  validation loss:		1.258167E-05

Epoch 58 of 100
  training loss:		1.186504E-05
  validation loss:		1.206271E-05

Epoch 59 of 100
  training loss:		1.086356E-05
  validation loss:		1.090645E-05

Epoch 60 of 100
  training loss:		1.004183E-05
  validation loss:		1.000438E-05

Epoch 61 of 100
  training loss:		9.430027E-06
  validation loss:		9.492556E-06

Epoch 62 of 100
  training loss:		8.658103E-06
  validation loss:		8.624050E-06

Epoch 63 of 100
  training loss:		8.080035E-06
  validation loss:		7.878983E-06

Epoch 64 of 100
  training loss:		7.549466E-06
  validation loss:		7.256355E-06

Epoch 65 of 100
  training loss:		6.901092E-06
  validation loss:		6.373947E-06

Epoch 66 of 100
  training loss:		6.326356E-06
  validation loss:		5.993188E-06

Epoch 67 of 100
  training loss:		5.772000E-06
  validation loss:		5.423497E-06

Epoch 68 of 100
  training loss:		5.264156E-06
  validation loss:		5.109559E-06

Epoch 69 of 100
  training loss:		4.842927E-06
  validation loss:		4.594092E-06

Epoch 70 of 100
  training loss:		4.448316E-06
  validation loss:		4.190776E-06

Epoch 71 of 100
  training loss:		4.261717E-06
  validation loss:		3.962798E-06

Epoch 72 of 100
  training loss:		3.756527E-06
  validation loss:		3.471956E-06

Epoch 73 of 100
  training loss:		3.390361E-06
  validation loss:		3.156044E-06

Epoch 74 of 100
  training loss:		3.169516E-06
  validation loss:		2.813865E-06

Epoch 75 of 100
  training loss:		2.820870E-06
  validation loss:		2.543464E-06

Epoch 76 of 100
  training loss:		2.598322E-06
  validation loss:		2.831460E-06

Epoch 77 of 100
  training loss:		2.378448E-06
  validation loss:		2.207041E-06

Epoch 78 of 100
  training loss:		2.148282E-06
  validation loss:		1.945244E-06

Epoch 79 of 100
  training loss:		1.934467E-06
  validation loss:		1.765952E-06

Epoch 80 of 100
  training loss:		1.816240E-06
  validation loss:		1.658391E-06

Epoch 81 of 100
  training loss:		1.643598E-06
  validation loss:		1.454728E-06

Epoch 82 of 100
  training loss:		1.549346E-06
  validation loss:		1.428600E-06

Epoch 83 of 100
  training loss:		1.398398E-06
  validation loss:		1.226290E-06

Epoch 84 of 100
  training loss:		1.312661E-06
  validation loss:		1.371035E-06

Epoch 85 of 100
  training loss:		1.234371E-06
  validation loss:		1.022502E-06

Epoch 86 of 100
  training loss:		1.134671E-06
  validation loss:		9.614418E-07

Epoch 87 of 100
  training loss:		1.015320E-06
  validation loss:		9.783441E-07

Epoch 88 of 100
  training loss:		9.414932E-07
  validation loss:		8.832645E-07

Epoch 89 of 100
  training loss:		8.518901E-07
  validation loss:		7.689050E-07

Epoch 90 of 100
  training loss:		7.634942E-07
  validation loss:		8.241331E-07

Epoch 91 of 100
  training loss:		6.973583E-07
  validation loss:		6.006277E-07

Epoch 92 of 100
  training loss:		6.387741E-07
  validation loss:		5.238678E-07

Epoch 93 of 100
  training loss:		5.612620E-07
  validation loss:		4.814955E-07

Epoch 94 of 100
  training loss:		5.182956E-07
  validation loss:		4.222502E-07

Epoch 95 of 100
  training loss:		4.550497E-07
  validation loss:		3.938027E-07

Epoch 96 of 100
  training loss:		4.188899E-07
  validation loss:		3.552160E-07

Epoch 97 of 100
  training loss:		3.809142E-07
  validation loss:		3.868435E-07

Epoch 98 of 100
  training loss:		3.724299E-07
  validation loss:		4.587091E-07

Epoch 99 of 100
  training loss:		3.307963E-07
  validation loss:		2.493105E-07

Epoch 100 of 100
  training loss:		2.815493E-07
  validation loss:		2.272908E-07

Training RMSE: 4.74608622083e-10
Validation RMSE: 4.66122188482e-10
