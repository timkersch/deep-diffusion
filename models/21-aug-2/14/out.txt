Training network with 56340 training samples and 18780 validation samples
Epoch 1 of 300
  training loss:		9.381462E-02
  validation loss:		7.335602E-02
Epoch took 1.417s

Epoch 2 of 300
  training loss:		6.903160E-02
  validation loss:		6.321785E-02
Epoch took 1.396s

Epoch 3 of 300
  training loss:		6.060966E-02
  validation loss:		5.614186E-02
Epoch took 1.396s

Epoch 4 of 300
  training loss:		5.567146E-02
  validation loss:		5.222787E-02
Epoch took 1.395s

Epoch 5 of 300
  training loss:		5.213131E-02
  validation loss:		4.911180E-02
Epoch took 1.395s

Epoch 6 of 300
  training loss:		4.915251E-02
  validation loss:		4.644345E-02
Epoch took 1.407s

Epoch 7 of 300
  training loss:		4.677772E-02
  validation loss:		4.407572E-02
Epoch took 1.406s

Epoch 8 of 300
  training loss:		4.474270E-02
  validation loss:		4.220498E-02
Epoch took 1.406s

Epoch 9 of 300
  training loss:		4.299776E-02
  validation loss:		4.085286E-02
Epoch took 1.405s

Epoch 10 of 300
  training loss:		4.138813E-02
  validation loss:		3.927679E-02
Epoch took 1.405s

Epoch 11 of 300
  training loss:		3.983500E-02
  validation loss:		3.759130E-02
Epoch took 1.405s

Epoch 12 of 300
  training loss:		3.862897E-02
  validation loss:		3.668264E-02
Epoch took 1.405s

Epoch 13 of 300
  training loss:		3.756576E-02
  validation loss:		3.541946E-02
Epoch took 1.405s

Epoch 14 of 300
  training loss:		3.659406E-02
  validation loss:		3.493929E-02
Epoch took 1.405s

Epoch 15 of 300
  training loss:		3.559262E-02
  validation loss:		3.391382E-02
Epoch took 1.405s

Epoch 16 of 300
  training loss:		3.497655E-02
  validation loss:		3.307486E-02
Epoch took 1.405s

Epoch 17 of 300
  training loss:		3.432867E-02
  validation loss:		3.277924E-02
Epoch took 1.407s

Epoch 18 of 300
  training loss:		3.350710E-02
  validation loss:		3.202609E-02
Epoch took 1.407s

Epoch 19 of 300
  training loss:		3.301243E-02
  validation loss:		3.123190E-02
Epoch took 1.407s

Epoch 20 of 300
  training loss:		3.247903E-02
  validation loss:		3.071496E-02
Epoch took 1.406s

Epoch 21 of 300
  training loss:		3.207066E-02
  validation loss:		3.008021E-02
Epoch took 1.405s

Epoch 22 of 300
  training loss:		3.146086E-02
  validation loss:		3.042472E-02
Epoch took 1.405s

Epoch 23 of 300
  training loss:		3.127325E-02
  validation loss:		3.010457E-02
Epoch took 1.405s

Epoch 24 of 300
  training loss:		3.081957E-02
  validation loss:		2.917943E-02
Epoch took 1.405s

Epoch 25 of 300
  training loss:		3.049625E-02
  validation loss:		2.910222E-02
Epoch took 1.405s

Epoch 26 of 300
  training loss:		3.022695E-02
  validation loss:		2.889337E-02
Epoch took 1.405s

Epoch 27 of 300
  training loss:		3.010144E-02
  validation loss:		2.880863E-02
Epoch took 1.405s

Epoch 28 of 300
  training loss:		2.959867E-02
  validation loss:		2.873384E-02
Epoch took 1.405s

Epoch 29 of 300
  training loss:		2.951024E-02
  validation loss:		2.875704E-02
Epoch took 1.405s

Epoch 30 of 300
  training loss:		2.921507E-02
  validation loss:		2.822082E-02
Epoch took 1.405s

Epoch 31 of 300
  training loss:		2.913100E-02
  validation loss:		2.773191E-02
Epoch took 1.405s

Epoch 32 of 300
  training loss:		2.900044E-02
  validation loss:		2.805556E-02
Epoch took 1.405s

Epoch 33 of 300
  training loss:		2.878256E-02
  validation loss:		2.715335E-02
Epoch took 1.405s

Epoch 34 of 300
  training loss:		2.865420E-02
  validation loss:		2.777472E-02
Epoch took 1.406s

Epoch 35 of 300
  training loss:		2.857979E-02
  validation loss:		2.769160E-02
Epoch took 1.405s

Epoch 36 of 300
  training loss:		2.846606E-02
  validation loss:		2.723887E-02
Epoch took 1.405s

Epoch 37 of 300
  training loss:		2.832469E-02
  validation loss:		2.710283E-02
Epoch took 1.405s

Epoch 38 of 300
  training loss:		2.826261E-02
  validation loss:		2.678178E-02
Epoch took 1.405s

Epoch 39 of 300
  training loss:		2.806108E-02
  validation loss:		2.658283E-02
Epoch took 1.404s

Epoch 40 of 300
  training loss:		2.795549E-02
  validation loss:		2.710845E-02
Epoch took 1.405s

Epoch 41 of 300
  training loss:		2.775325E-02
  validation loss:		2.639367E-02
Epoch took 1.405s

Epoch 42 of 300
  training loss:		2.780440E-02
  validation loss:		2.658482E-02
Epoch took 1.405s

Epoch 43 of 300
  training loss:		2.767480E-02
  validation loss:		2.632960E-02
Epoch took 1.405s

Epoch 44 of 300
  training loss:		2.778375E-02
  validation loss:		2.783559E-02
Epoch took 1.405s

Epoch 45 of 300
  training loss:		2.782660E-02
  validation loss:		2.684029E-02
Epoch took 1.405s

Epoch 46 of 300
  training loss:		2.765065E-02
  validation loss:		2.810573E-02
Epoch took 1.406s

Epoch 47 of 300
  training loss:		2.747377E-02
  validation loss:		2.644595E-02
Epoch took 1.405s

Epoch 48 of 300
  training loss:		2.744825E-02
  validation loss:		2.620637E-02
Epoch took 1.405s

Epoch 49 of 300
  training loss:		2.756804E-02
  validation loss:		2.649349E-02
Epoch took 1.405s

Epoch 50 of 300
  training loss:		2.760249E-02
  validation loss:		2.649541E-02
Epoch took 1.405s

Epoch 51 of 300
  training loss:		2.738554E-02
  validation loss:		2.597682E-02
Epoch took 1.405s

Epoch 52 of 300
  training loss:		2.726265E-02
  validation loss:		2.596157E-02
Epoch took 1.405s

Epoch 53 of 300
  training loss:		2.749619E-02
  validation loss:		2.590965E-02
Epoch took 1.405s

Epoch 54 of 300
  training loss:		2.722984E-02
  validation loss:		2.612210E-02
Epoch took 1.405s

Epoch 55 of 300
  training loss:		2.723014E-02
  validation loss:		2.602238E-02
Epoch took 1.404s

Epoch 56 of 300
  training loss:		2.751042E-02
  validation loss:		2.656811E-02
Epoch took 1.405s

Epoch 57 of 300
  training loss:		2.731301E-02
  validation loss:		2.613591E-02
Epoch took 1.405s

Epoch 58 of 300
  training loss:		2.721296E-02
  validation loss:		2.602669E-02
Epoch took 1.405s

Epoch 59 of 300
  training loss:		2.720843E-02
  validation loss:		2.617253E-02
Epoch took 1.405s

Epoch 60 of 300
  training loss:		2.717583E-02
  validation loss:		2.596806E-02
Epoch took 1.407s

Epoch 61 of 300
  training loss:		2.722117E-02
  validation loss:		2.629130E-02
Epoch took 1.405s

Epoch 62 of 300
  training loss:		2.726528E-02
  validation loss:		2.621937E-02
Epoch took 1.405s

Epoch 63 of 300
  training loss:		2.727747E-02
  validation loss:		2.584278E-02
Epoch took 1.405s

Epoch 64 of 300
  training loss:		2.722958E-02
  validation loss:		2.679680E-02
Epoch took 1.405s

Epoch 65 of 300
  training loss:		2.719553E-02
  validation loss:		2.580235E-02
Epoch took 1.405s

Epoch 66 of 300
  training loss:		2.698670E-02
  validation loss:		2.581927E-02
Epoch took 1.405s

Epoch 67 of 300
  training loss:		2.701046E-02
  validation loss:		2.599462E-02
Epoch took 1.405s

Epoch 68 of 300
  training loss:		2.702473E-02
  validation loss:		2.559972E-02
Epoch took 1.405s

Epoch 69 of 300
  training loss:		2.718350E-02
  validation loss:		2.605371E-02
Epoch took 1.405s

Epoch 70 of 300
  training loss:		2.720125E-02
  validation loss:		2.647581E-02
Epoch took 1.404s

Epoch 71 of 300
  training loss:		2.711516E-02
  validation loss:		2.624909E-02
Epoch took 1.404s

Epoch 72 of 300
  training loss:		2.709841E-02
  validation loss:		2.621857E-02
Epoch took 1.405s

Epoch 73 of 300
  training loss:		2.697126E-02
  validation loss:		2.595169E-02
Epoch took 1.405s

Epoch 74 of 300
  training loss:		2.694589E-02
  validation loss:		2.583455E-02
Epoch took 1.405s

Epoch 75 of 300
  training loss:		2.698339E-02
  validation loss:		2.565081E-02
Epoch took 1.405s

Epoch 76 of 300
  training loss:		2.703156E-02
  validation loss:		2.607168E-02
Epoch took 1.405s

Epoch 77 of 300
  training loss:		2.725342E-02
  validation loss:		2.639102E-02
Epoch took 1.405s

Epoch 78 of 300
  training loss:		2.717055E-02
  validation loss:		2.571684E-02
Epoch took 1.405s

Epoch 79 of 300
  training loss:		2.722335E-02
  validation loss:		2.562792E-02
Epoch took 1.405s

Epoch 80 of 300
  training loss:		2.706689E-02
  validation loss:		2.651427E-02
Epoch took 1.405s

Epoch 81 of 300
  training loss:		2.699354E-02
  validation loss:		2.590181E-02
Epoch took 1.405s

Epoch 82 of 300
  training loss:		2.711920E-02
  validation loss:		2.608572E-02
Epoch took 1.404s

Epoch 83 of 300
  training loss:		2.703418E-02
  validation loss:		2.662788E-02
Epoch took 1.404s

Epoch 84 of 300
  training loss:		2.715102E-02
  validation loss:		2.594611E-02
Epoch took 1.404s

Epoch 85 of 300
  training loss:		2.702316E-02
  validation loss:		2.608884E-02
Epoch took 1.405s

Epoch 86 of 300
  training loss:		2.699755E-02
  validation loss:		2.567717E-02
Epoch took 1.404s

Epoch 87 of 300
  training loss:		2.726994E-02
  validation loss:		2.549053E-02
Epoch took 1.404s

Epoch 88 of 300
  training loss:		2.690365E-02
  validation loss:		2.604026E-02
Epoch took 1.405s

Epoch 89 of 300
  training loss:		2.699803E-02
  validation loss:		2.543267E-02
Epoch took 1.405s

Epoch 90 of 300
  training loss:		2.693260E-02
  validation loss:		2.629855E-02
Epoch took 1.405s

Epoch 91 of 300
  training loss:		2.709789E-02
  validation loss:		2.559357E-02
Epoch took 1.405s

Epoch 92 of 300
  training loss:		2.707912E-02
  validation loss:		2.554442E-02
Epoch took 1.405s

Epoch 93 of 300
  training loss:		2.706214E-02
  validation loss:		2.568846E-02
Epoch took 1.405s

Epoch 94 of 300
  training loss:		2.700640E-02
  validation loss:		2.555466E-02
Epoch took 1.405s

Epoch 95 of 300
  training loss:		2.694900E-02
  validation loss:		2.540587E-02
Epoch took 1.405s

Epoch 96 of 300
  training loss:		2.686519E-02
  validation loss:		2.570157E-02
Epoch took 1.405s

Epoch 97 of 300
  training loss:		2.707357E-02
  validation loss:		2.643889E-02
Epoch took 1.405s

Epoch 98 of 300
  training loss:		2.691275E-02
  validation loss:		2.562061E-02
Epoch took 1.405s

Epoch 99 of 300
  training loss:		2.697376E-02
  validation loss:		2.572015E-02
Epoch took 1.405s

Epoch 100 of 300
  training loss:		2.684851E-02
  validation loss:		2.540923E-02
Epoch took 1.405s

Epoch 101 of 300
  training loss:		2.695991E-02
  validation loss:		2.550935E-02
Epoch took 1.405s

Epoch 102 of 300
  training loss:		2.705879E-02
  validation loss:		2.616278E-02
Epoch took 1.407s

Epoch 103 of 300
  training loss:		2.691831E-02
  validation loss:		2.584706E-02
Epoch took 1.405s

Epoch 104 of 300
  training loss:		2.690556E-02
  validation loss:		2.575699E-02
Epoch took 1.406s

Epoch 105 of 300
  training loss:		2.695841E-02
  validation loss:		2.536286E-02
Epoch took 1.406s

Epoch 106 of 300
  training loss:		2.691027E-02
  validation loss:		2.570040E-02
Epoch took 1.405s

Epoch 107 of 300
  training loss:		2.688565E-02
  validation loss:		2.623029E-02
Epoch took 1.406s

Epoch 108 of 300
  training loss:		2.707933E-02
  validation loss:		2.590512E-02
Epoch took 1.406s

Epoch 109 of 300
  training loss:		2.698749E-02
  validation loss:		2.571535E-02
Epoch took 1.406s

Epoch 110 of 300
  training loss:		2.686082E-02
  validation loss:		2.591600E-02
Epoch took 1.406s

Epoch 111 of 300
  training loss:		2.688035E-02
  validation loss:		2.579910E-02
Epoch took 1.406s

Epoch 112 of 300
  training loss:		2.697389E-02
  validation loss:		2.576470E-02
Epoch took 1.406s

Epoch 113 of 300
  training loss:		2.700444E-02
  validation loss:		2.650112E-02
Epoch took 1.406s

Epoch 114 of 300
  training loss:		2.684200E-02
  validation loss:		2.565604E-02
Epoch took 1.406s

Epoch 115 of 300
  training loss:		2.687622E-02
  validation loss:		2.593251E-02
Epoch took 1.406s

Epoch 116 of 300
  training loss:		2.680403E-02
  validation loss:		2.568929E-02
Epoch took 1.406s

Epoch 117 of 300
  training loss:		2.695679E-02
  validation loss:		2.646524E-02
Epoch took 1.406s

Epoch 118 of 300
  training loss:		2.687810E-02
  validation loss:		2.562143E-02
Epoch took 1.406s

Epoch 119 of 300
  training loss:		2.694730E-02
  validation loss:		2.544810E-02
Epoch took 1.406s

Epoch 120 of 300
  training loss:		2.700201E-02
  validation loss:		2.714777E-02
Epoch took 1.406s

Early stopping, val-loss increased over the last 20 epochs from 0.025813349041 to 0.0259065748307
Saving model from epoch 100
Training MSE: 2.5246e-14
Validation MSE: 2.43822e-14
Training R2: 0.732487679646
Validation R2: 0.740593109831
