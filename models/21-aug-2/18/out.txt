Training network with 56340 training samples and 18780 validation samples
Epoch 1 of 300
  training loss:		8.203168E-02
  validation loss:		4.993726E-02
Epoch took 1.452s

Epoch 2 of 300
  training loss:		4.764539E-02
  validation loss:		3.917662E-02
Epoch took 1.397s

Epoch 3 of 300
  training loss:		3.905032E-02
  validation loss:		3.230341E-02
Epoch took 1.397s

Epoch 4 of 300
  training loss:		3.323927E-02
  validation loss:		3.018325E-02
Epoch took 1.396s

Epoch 5 of 300
  training loss:		3.118934E-02
  validation loss:		2.942306E-02
Epoch took 1.395s

Epoch 6 of 300
  training loss:		2.978374E-02
  validation loss:		2.741175E-02
Epoch took 1.395s

Epoch 7 of 300
  training loss:		2.910322E-02
  validation loss:		2.891341E-02
Epoch took 1.398s

Epoch 8 of 300
  training loss:		2.863230E-02
  validation loss:		2.742820E-02
Epoch took 1.397s

Epoch 9 of 300
  training loss:		3.067028E-02
  validation loss:		2.860355E-02
Epoch took 1.396s

Epoch 10 of 300
  training loss:		2.831396E-02
  validation loss:		2.733210E-02
Epoch took 1.399s

Epoch 11 of 300
  training loss:		2.877229E-02
  validation loss:		2.823571E-02
Epoch took 1.396s

Epoch 12 of 300
  training loss:		2.918689E-02
  validation loss:		2.755725E-02
Epoch took 1.396s

Epoch 13 of 300
  training loss:		2.897407E-02
  validation loss:		2.675775E-02
Epoch took 1.395s

Epoch 14 of 300
  training loss:		2.883923E-02
  validation loss:		2.715769E-02
Epoch took 1.396s

Epoch 15 of 300
  training loss:		2.753741E-02
  validation loss:		2.612699E-02
Epoch took 1.397s

Epoch 16 of 300
  training loss:		2.815067E-02
  validation loss:		2.642823E-02
Epoch took 1.395s

Epoch 17 of 300
  training loss:		2.816840E-02
  validation loss:		2.668162E-02
Epoch took 1.394s

Epoch 18 of 300
  training loss:		2.734147E-02
  validation loss:		2.605627E-02
Epoch took 1.397s

Epoch 19 of 300
  training loss:		2.831221E-02
  validation loss:		2.814853E-02
Epoch took 1.395s

Epoch 20 of 300
  training loss:		2.754466E-02
  validation loss:		2.577361E-02
Epoch took 1.393s

Epoch 21 of 300
  training loss:		2.782261E-02
  validation loss:		3.626067E-02
Epoch took 1.397s

Epoch 22 of 300
  training loss:		2.761649E-02
  validation loss:		2.671429E-02
Epoch took 1.393s

Epoch 23 of 300
  training loss:		2.779888E-02
  validation loss:		2.577688E-02
Epoch took 1.395s

Epoch 24 of 300
  training loss:		2.674370E-02
  validation loss:		2.635002E-02
Epoch took 1.397s

Epoch 25 of 300
  training loss:		2.713019E-02
  validation loss:		2.588948E-02
Epoch took 1.393s

Epoch 26 of 300
  training loss:		2.694427E-02
  validation loss:		3.531832E-02
Epoch took 1.395s

Epoch 27 of 300
  training loss:		3.022995E-02
  validation loss:		2.557626E-02
Epoch took 1.391s

Epoch 28 of 300
  training loss:		2.651102E-02
  validation loss:		2.530405E-02
Epoch took 1.398s

Epoch 29 of 300
  training loss:		2.668065E-02
  validation loss:		2.636842E-02
Epoch took 1.395s

Epoch 30 of 300
  training loss:		2.675489E-02
  validation loss:		2.792665E-02
Epoch took 1.392s

Epoch 31 of 300
  training loss:		2.859596E-02
  validation loss:		2.555336E-02
Epoch took 1.393s

Epoch 32 of 300
  training loss:		2.641066E-02
  validation loss:		2.573962E-02
Epoch took 1.398s

Epoch 33 of 300
  training loss:		2.793126E-02
  validation loss:		2.745779E-02
Epoch took 1.394s

Epoch 34 of 300
  training loss:		2.718177E-02
  validation loss:		2.534342E-02
Epoch took 1.392s

Epoch 35 of 300
  training loss:		2.847518E-02
  validation loss:		2.718317E-02
Epoch took 1.398s

Epoch 36 of 300
  training loss:		2.723968E-02
  validation loss:		2.530770E-02
Epoch took 1.393s

Epoch 37 of 300
  training loss:		2.629565E-02
  validation loss:		2.531901E-02
Epoch took 1.399s

Epoch 38 of 300
  training loss:		2.629501E-02
  validation loss:		2.549876E-02
Epoch took 1.394s

Epoch 39 of 300
  training loss:		2.729230E-02
  validation loss:		3.254092E-02
Epoch took 1.393s

Epoch 40 of 300
  training loss:		2.846830E-02
  validation loss:		2.552875E-02
Epoch took 1.390s

Epoch 41 of 300
  training loss:		2.629966E-02
  validation loss:		2.523439E-02
Epoch took 1.401s

Epoch 42 of 300
  training loss:		2.716453E-02
  validation loss:		3.080940E-02
Epoch took 1.397s

Epoch 43 of 300
  training loss:		2.678051E-02
  validation loss:		2.524651E-02
Epoch took 1.391s

Epoch 44 of 300
  training loss:		2.622919E-02
  validation loss:		2.524515E-02
Epoch took 1.399s

Epoch 45 of 300
  training loss:		2.626786E-02
  validation loss:		2.529489E-02
Epoch took 1.396s

Epoch 46 of 300
  training loss:		3.127853E-02
  validation loss:		2.682971E-02
Epoch took 1.390s

Epoch 47 of 300
  training loss:		2.676567E-02
  validation loss:		2.530916E-02
Epoch took 1.397s

Epoch 48 of 300
  training loss:		2.624092E-02
  validation loss:		2.539550E-02
Epoch took 1.400s

Epoch 49 of 300
  training loss:		2.621458E-02
  validation loss:		2.535442E-02
Epoch took 1.393s

Epoch 50 of 300
  training loss:		2.620876E-02
  validation loss:		2.533958E-02
Epoch took 1.394s

Epoch 51 of 300
  training loss:		2.690794E-02
  validation loss:		3.332811E-02
Epoch took 1.392s

Epoch 52 of 300
  training loss:		2.866891E-02
  validation loss:		2.543477E-02
Epoch took 1.390s

Epoch 53 of 300
  training loss:		2.625437E-02
  validation loss:		2.529947E-02
Epoch took 1.401s

Epoch 54 of 300
  training loss:		2.619820E-02
  validation loss:		2.522895E-02
Epoch took 1.396s

Epoch 55 of 300
  training loss:		2.862211E-02
  validation loss:		2.564813E-02
Epoch took 1.391s

Epoch 56 of 300
  training loss:		2.634137E-02
  validation loss:		2.517601E-02
Epoch took 1.395s

Epoch 57 of 300
  training loss:		2.624861E-02
  validation loss:		2.557531E-02
Epoch took 1.398s

Epoch 58 of 300
  training loss:		2.620452E-02
  validation loss:		2.550229E-02
Epoch took 1.393s

Epoch 59 of 300
  training loss:		2.621917E-02
  validation loss:		2.526024E-02
Epoch took 1.393s

Epoch 60 of 300
  training loss:		2.975188E-02
  validation loss:		2.639764E-02
Epoch took 1.391s

Epoch 61 of 300
  training loss:		2.637503E-02
  validation loss:		2.535015E-02
Epoch took 1.392s

Epoch 62 of 300
  training loss:		2.621067E-02
  validation loss:		2.527761E-02
Epoch took 1.401s

Epoch 63 of 300
  training loss:		2.620336E-02
  validation loss:		2.534433E-02
Epoch took 1.392s

Epoch 64 of 300
  training loss:		3.010145E-02
  validation loss:		2.669820E-02
Epoch took 1.392s

Epoch 65 of 300
  training loss:		2.633077E-02
  validation loss:		2.526866E-02
Epoch took 1.393s

Epoch 66 of 300
  training loss:		2.627686E-02
  validation loss:		2.524183E-02
Epoch took 1.400s

Epoch 67 of 300
  training loss:		2.620880E-02
  validation loss:		2.521934E-02
Epoch took 1.393s

Epoch 68 of 300
  training loss:		2.618211E-02
  validation loss:		2.519917E-02
Epoch took 1.394s

Epoch 69 of 300
  training loss:		2.621043E-02
  validation loss:		2.529922E-02
Epoch took 1.393s

Epoch 70 of 300
  training loss:		2.826159E-02
  validation loss:		3.712384E-02
Epoch took 1.391s

Epoch 71 of 300
  training loss:		2.793854E-02
  validation loss:		2.526357E-02
Epoch took 1.390s

Epoch 72 of 300
  training loss:		2.630668E-02
  validation loss:		2.549535E-02
Epoch took 1.403s

Epoch 73 of 300
  training loss:		2.628618E-02
  validation loss:		2.520039E-02
Epoch took 1.392s

Epoch 74 of 300
  training loss:		2.615764E-02
  validation loss:		2.521659E-02
Epoch took 1.396s

Epoch 75 of 300
  training loss:		2.751905E-02
  validation loss:		2.620149E-02
Epoch took 1.393s

Epoch 76 of 300
  training loss:		2.705450E-02
  validation loss:		2.528561E-02
Epoch took 1.391s

Epoch 77 of 300
  training loss:		2.630303E-02
  validation loss:		2.562593E-02
Epoch took 1.401s

Epoch 78 of 300
  training loss:		2.652011E-02
  validation loss:		2.528702E-02
Epoch took 1.392s

Epoch 79 of 300
  training loss:		2.618405E-02
  validation loss:		2.527294E-02
Epoch took 1.395s

Epoch 80 of 300
  training loss:		2.616259E-02
  validation loss:		2.525300E-02
Epoch took 1.396s

Epoch 81 of 300
  training loss:		2.615895E-02
  validation loss:		2.523569E-02
Epoch took 1.393s

Epoch 82 of 300
  training loss:		2.855722E-02
  validation loss:		2.797844E-02
Epoch took 1.392s

Epoch 83 of 300
  training loss:		2.652913E-02
  validation loss:		2.520587E-02
Epoch took 1.391s

Epoch 84 of 300
  training loss:		2.615889E-02
  validation loss:		2.532394E-02
Epoch took 1.405s

Epoch 85 of 300
  training loss:		2.617398E-02
  validation loss:		2.542962E-02
Epoch took 1.393s

Epoch 86 of 300
  training loss:		2.619037E-02
  validation loss:		2.527817E-02
Epoch took 1.395s

Epoch 87 of 300
  training loss:		2.941577E-02
  validation loss:		3.726618E-02
Epoch took 1.394s

Epoch 88 of 300
  training loss:		2.706812E-02
  validation loss:		2.544635E-02
Epoch took 1.391s

Epoch 89 of 300
  training loss:		2.618831E-02
  validation loss:		2.524635E-02
Epoch took 1.403s

Epoch 90 of 300
  training loss:		2.615833E-02
  validation loss:		2.559471E-02
Epoch took 1.394s

Epoch 91 of 300
  training loss:		2.643088E-02
  validation loss:		2.525372E-02
Epoch took 1.392s

Epoch 92 of 300
  training loss:		2.621694E-02
  validation loss:		2.521976E-02
Epoch took 1.394s

Epoch 93 of 300
  training loss:		2.618343E-02
  validation loss:		2.539088E-02
Epoch took 1.396s

Epoch 94 of 300
  training loss:		2.617188E-02
  validation loss:		2.520427E-02
Epoch took 1.393s

Epoch 95 of 300
  training loss:		2.967071E-02
  validation loss:		2.602450E-02
Epoch took 1.392s

Epoch 96 of 300
  training loss:		2.631519E-02
  validation loss:		2.530309E-02
Epoch took 1.395s

Epoch 97 of 300
  training loss:		2.616788E-02
  validation loss:		2.527977E-02
Epoch took 1.401s

Epoch 98 of 300
  training loss:		2.624553E-02
  validation loss:		2.530264E-02
Epoch took 1.392s

Epoch 99 of 300
  training loss:		2.615085E-02
  validation loss:		2.518691E-02
Epoch took 1.393s

Epoch 100 of 300
  training loss:		2.614070E-02
  validation loss:		2.509614E-02
Epoch took 1.395s

Early stopping, val-loss increased over the last 20 epochs from 0.0260062112113 to 0.026063349998
Saving model from epoch 80
Training MSE: 2.505e-14
Validation MSE: 2.42387e-14
Training R2: 0.734564103646
Validation R2: 0.742118927922
