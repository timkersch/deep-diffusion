Training network with 56340 training samples and 18780 validation samples
Epoch 1 of 300
  training loss:		3.410044E-01
  validation loss:		1.073911E-01
Epoch took 0.721s

Epoch 2 of 300
  training loss:		8.489550E-02
  validation loss:		7.068418E-02
Epoch took 0.656s

Epoch 3 of 300
  training loss:		6.639274E-02
  validation loss:		5.939225E-02
Epoch took 0.658s

Epoch 4 of 300
  training loss:		5.722453E-02
  validation loss:		5.250575E-02
Epoch took 0.656s

Epoch 5 of 300
  training loss:		5.099476E-02
  validation loss:		4.771823E-02
Epoch took 0.654s

Epoch 6 of 300
  training loss:		4.652076E-02
  validation loss:		4.255385E-02
Epoch took 0.655s

Epoch 7 of 300
  training loss:		4.320364E-02
  validation loss:		3.942092E-02
Epoch took 0.656s

Epoch 8 of 300
  training loss:		3.986728E-02
  validation loss:		3.658702E-02
Epoch took 0.655s

Epoch 9 of 300
  training loss:		4.006033E-02
  validation loss:		3.577113E-02
Epoch took 0.656s

Epoch 10 of 300
  training loss:		3.636199E-02
  validation loss:		3.350977E-02
Epoch took 0.656s

Epoch 11 of 300
  training loss:		3.618259E-02
  validation loss:		3.257978E-02
Epoch took 0.657s

Epoch 12 of 300
  training loss:		3.568847E-02
  validation loss:		3.147396E-02
Epoch took 0.657s

Epoch 13 of 300
  training loss:		3.260109E-02
  validation loss:		2.972145E-02
Epoch took 0.659s

Epoch 14 of 300
  training loss:		3.213346E-02
  validation loss:		3.315197E-02
Epoch took 0.655s

Epoch 15 of 300
  training loss:		3.292074E-02
  validation loss:		3.712327E-02
Epoch took 0.655s

Epoch 16 of 300
  training loss:		3.345699E-02
  validation loss:		2.859856E-02
Epoch took 0.658s

Epoch 17 of 300
  training loss:		3.037453E-02
  validation loss:		2.926520E-02
Epoch took 0.655s

Epoch 18 of 300
  training loss:		3.012710E-02
  validation loss:		2.794575E-02
Epoch took 0.657s

Epoch 19 of 300
  training loss:		2.974517E-02
  validation loss:		2.898182E-02
Epoch took 0.657s

Epoch 20 of 300
  training loss:		3.105440E-02
  validation loss:		2.763379E-02
Epoch took 0.657s

Epoch 21 of 300
  training loss:		2.964054E-02
  validation loss:		2.684090E-02
Epoch took 0.660s

Epoch 22 of 300
  training loss:		2.940012E-02
  validation loss:		2.747612E-02
Epoch took 0.662s

Epoch 23 of 300
  training loss:		2.961302E-02
  validation loss:		2.678357E-02
Epoch took 0.661s

Epoch 24 of 300
  training loss:		2.793480E-02
  validation loss:		2.665184E-02
Epoch took 0.662s

Epoch 25 of 300
  training loss:		2.828133E-02
  validation loss:		2.696943E-02
Epoch took 0.662s

Epoch 26 of 300
  training loss:		2.793752E-02
  validation loss:		3.014146E-02
Epoch took 0.662s

Epoch 27 of 300
  training loss:		2.933685E-02
  validation loss:		2.640213E-02
Epoch took 0.665s

Epoch 28 of 300
  training loss:		2.830268E-02
  validation loss:		2.657350E-02
Epoch took 0.661s

Epoch 29 of 300
  training loss:		2.812166E-02
  validation loss:		2.621566E-02
Epoch took 0.663s

Epoch 30 of 300
  training loss:		2.765728E-02
  validation loss:		2.649237E-02
Epoch took 0.662s

Epoch 31 of 300
  training loss:		2.844044E-02
  validation loss:		2.694770E-02
Epoch took 0.662s

Epoch 32 of 300
  training loss:		2.822118E-02
  validation loss:		2.787684E-02
Epoch took 0.661s

Epoch 33 of 300
  training loss:		2.771606E-02
  validation loss:		2.704733E-02
Epoch took 0.658s

Epoch 34 of 300
  training loss:		2.720582E-02
  validation loss:		3.030821E-02
Epoch took 0.656s

Epoch 35 of 300
  training loss:		2.838895E-02
  validation loss:		2.693969E-02
Epoch took 0.659s

Epoch 36 of 300
  training loss:		2.769917E-02
  validation loss:		2.690389E-02
Epoch took 0.659s

Epoch 37 of 300
  training loss:		2.735978E-02
  validation loss:		2.686301E-02
Epoch took 0.660s

Epoch 38 of 300
  training loss:		2.803555E-02
  validation loss:		2.591366E-02
Epoch took 0.659s

Epoch 39 of 300
  training loss:		2.777730E-02
  validation loss:		2.693199E-02
Epoch took 0.657s

Epoch 40 of 300
  training loss:		2.778900E-02
  validation loss:		2.676039E-02
Epoch took 0.659s

Epoch 41 of 300
  training loss:		2.735800E-02
  validation loss:		2.742991E-02
Epoch took 0.660s

Epoch 42 of 300
  training loss:		2.792325E-02
  validation loss:		2.844858E-02
Epoch took 0.660s

Epoch 43 of 300
  training loss:		2.916357E-02
  validation loss:		2.597826E-02
Epoch took 0.656s

Epoch 44 of 300
  training loss:		2.690607E-02
  validation loss:		2.592218E-02
Epoch took 0.659s

Epoch 45 of 300
  training loss:		2.719969E-02
  validation loss:		2.611696E-02
Epoch took 0.656s

Epoch 46 of 300
  training loss:		2.756621E-02
  validation loss:		3.098197E-02
Epoch took 0.659s

Epoch 47 of 300
  training loss:		2.794221E-02
  validation loss:		2.682992E-02
Epoch took 0.660s

Epoch 48 of 300
  training loss:		2.748766E-02
  validation loss:		2.563712E-02
Epoch took 0.662s

Epoch 49 of 300
  training loss:		2.739842E-02
  validation loss:		2.758991E-02
Epoch took 0.658s

Epoch 50 of 300
  training loss:		2.739415E-02
  validation loss:		2.603264E-02
Epoch took 0.656s

Epoch 51 of 300
  training loss:		2.748073E-02
  validation loss:		2.641315E-02
Epoch took 0.658s

Epoch 52 of 300
  training loss:		2.749425E-02
  validation loss:		2.575128E-02
Epoch took 0.659s

Epoch 53 of 300
  training loss:		2.701757E-02
  validation loss:		2.647534E-02
Epoch took 0.657s

Epoch 54 of 300
  training loss:		2.751584E-02
  validation loss:		2.649254E-02
Epoch took 0.654s

Epoch 55 of 300
  training loss:		2.759276E-02
  validation loss:		2.755261E-02
Epoch took 0.658s

Epoch 56 of 300
  training loss:		2.818432E-02
  validation loss:		2.788077E-02
Epoch took 0.661s

Epoch 57 of 300
  training loss:		2.800038E-02
  validation loss:		2.690305E-02
Epoch took 0.654s

Epoch 58 of 300
  training loss:		2.748743E-02
  validation loss:		2.633529E-02
Epoch took 0.660s

Epoch 59 of 300
  training loss:		2.834488E-02
  validation loss:		2.752057E-02
Epoch took 0.653s

Epoch 60 of 300
  training loss:		2.787215E-02
  validation loss:		2.684540E-02
Epoch took 0.659s

Epoch 61 of 300
  training loss:		2.735315E-02
  validation loss:		2.602743E-02
Epoch took 0.658s

Epoch 62 of 300
  training loss:		2.700695E-02
  validation loss:		2.601972E-02
Epoch took 0.658s

Epoch 63 of 300
  training loss:		2.731571E-02
  validation loss:		2.541597E-02
Epoch took 0.659s

Epoch 64 of 300
  training loss:		2.682345E-02
  validation loss:		2.577117E-02
Epoch took 0.658s

Epoch 65 of 300
  training loss:		2.683267E-02
  validation loss:		2.569273E-02
Epoch took 0.658s

Epoch 66 of 300
  training loss:		2.698501E-02
  validation loss:		2.580932E-02
Epoch took 0.659s

Epoch 67 of 300
  training loss:		2.680038E-02
  validation loss:		2.578842E-02
Epoch took 0.658s

Epoch 68 of 300
  training loss:		2.725524E-02
  validation loss:		2.685424E-02
Epoch took 0.658s

Epoch 69 of 300
  training loss:		2.801435E-02
  validation loss:		2.594627E-02
Epoch took 0.659s

Epoch 70 of 300
  training loss:		2.701186E-02
  validation loss:		2.782918E-02
Epoch took 0.656s

Epoch 71 of 300
  training loss:		2.752277E-02
  validation loss:		2.601605E-02
Epoch took 0.657s

Epoch 72 of 300
  training loss:		2.700175E-02
  validation loss:		2.611800E-02
Epoch took 0.657s

Epoch 73 of 300
  training loss:		2.694747E-02
  validation loss:		2.586738E-02
Epoch took 0.658s

Epoch 74 of 300
  training loss:		2.708617E-02
  validation loss:		2.734106E-02
Epoch took 0.657s

Epoch 75 of 300
  training loss:		2.746769E-02
  validation loss:		2.620362E-02
Epoch took 0.657s

Epoch 76 of 300
  training loss:		2.732643E-02
  validation loss:		2.574341E-02
Epoch took 0.660s

Epoch 77 of 300
  training loss:		2.698830E-02
  validation loss:		2.594340E-02
Epoch took 0.657s

Epoch 78 of 300
  training loss:		2.723168E-02
  validation loss:		2.577262E-02
Epoch took 0.656s

Epoch 79 of 300
  training loss:		2.747775E-02
  validation loss:		2.767128E-02
Epoch took 0.658s

Epoch 80 of 300
  training loss:		2.711901E-02
  validation loss:		2.582046E-02
Epoch took 0.657s

Epoch 81 of 300
  training loss:		2.710883E-02
  validation loss:		2.628694E-02
Epoch took 0.657s

Epoch 82 of 300
  training loss:		2.690474E-02
  validation loss:		2.559425E-02
Epoch took 0.656s

Epoch 83 of 300
  training loss:		2.706554E-02
  validation loss:		2.594041E-02
Epoch took 0.659s

Epoch 84 of 300
  training loss:		2.709537E-02
  validation loss:		2.650394E-02
Epoch took 0.657s

Epoch 85 of 300
  training loss:		2.744430E-02
  validation loss:		2.670919E-02
Epoch took 0.657s

Epoch 86 of 300
  training loss:		2.758145E-02
  validation loss:		2.655281E-02
Epoch took 0.659s

Epoch 87 of 300
  training loss:		2.695935E-02
  validation loss:		2.602102E-02
Epoch took 0.658s

Epoch 88 of 300
  training loss:		2.700408E-02
  validation loss:		2.681001E-02
Epoch took 0.656s

Epoch 89 of 300
  training loss:		2.685173E-02
  validation loss:		2.568658E-02
Epoch took 0.660s

Epoch 90 of 300
  training loss:		2.672969E-02
  validation loss:		2.586042E-02
Epoch took 0.658s

Epoch 91 of 300
  training loss:		2.685994E-02
  validation loss:		2.671722E-02
Epoch took 0.658s

Epoch 92 of 300
  training loss:		2.750584E-02
  validation loss:		2.727779E-02
Epoch took 0.657s

Epoch 93 of 300
  training loss:		2.759025E-02
  validation loss:		2.654338E-02
Epoch took 0.657s

Epoch 94 of 300
  training loss:		2.688462E-02
  validation loss:		2.581442E-02
Epoch took 0.660s

Epoch 95 of 300
  training loss:		2.666823E-02
  validation loss:		2.558408E-02
Epoch took 0.657s

Epoch 96 of 300
  training loss:		2.689421E-02
  validation loss:		2.679971E-02
Epoch took 0.654s

Epoch 97 of 300
  training loss:		2.730998E-02
  validation loss:		2.657124E-02
Epoch took 0.657s

Epoch 98 of 300
  training loss:		2.738634E-02
  validation loss:		2.615293E-02
Epoch took 0.658s

Epoch 99 of 300
  training loss:		2.689120E-02
  validation loss:		2.597319E-02
Epoch took 0.656s

Epoch 100 of 300
  training loss:		2.693306E-02
  validation loss:		2.595544E-02
Epoch took 0.658s

Early stopping, val-loss increased over the last 20 epochs from 0.0261825864617 to 0.0262677477259
Saving model from epoch 80
Training MSE: 2.56721e-14
Validation MSE: 2.48403e-14
Training R2: 0.727972463545
Validation R2: 0.735719080649
