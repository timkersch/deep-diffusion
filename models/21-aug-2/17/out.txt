Training network with 56340 training samples and 18780 validation samples
Epoch 1 of 300
  training loss:		6.734184E-02
  validation loss:		5.352674E-02
Epoch took 1.444s

Epoch 2 of 300
  training loss:		4.278968E-02
  validation loss:		3.439722E-02
Epoch took 1.395s

Epoch 3 of 300
  training loss:		3.629169E-02
  validation loss:		3.257224E-02
Epoch took 1.398s

Epoch 4 of 300
  training loss:		3.267401E-02
  validation loss:		2.899944E-02
Epoch took 1.398s

Epoch 5 of 300
  training loss:		3.053084E-02
  validation loss:		2.856519E-02
Epoch took 1.398s

Epoch 6 of 300
  training loss:		3.006151E-02
  validation loss:		2.876665E-02
Epoch took 1.397s

Epoch 7 of 300
  training loss:		2.962682E-02
  validation loss:		2.806222E-02
Epoch took 1.397s

Epoch 8 of 300
  training loss:		2.944691E-02
  validation loss:		2.851313E-02
Epoch took 1.396s

Epoch 9 of 300
  training loss:		2.939924E-02
  validation loss:		2.839455E-02
Epoch took 1.397s

Epoch 10 of 300
  training loss:		2.851666E-02
  validation loss:		2.656978E-02
Epoch took 1.396s

Epoch 11 of 300
  training loss:		2.840070E-02
  validation loss:		2.763137E-02
Epoch took 1.397s

Epoch 12 of 300
  training loss:		2.848145E-02
  validation loss:		2.709581E-02
Epoch took 1.396s

Epoch 13 of 300
  training loss:		2.797404E-02
  validation loss:		2.805916E-02
Epoch took 1.396s

Epoch 14 of 300
  training loss:		2.830060E-02
  validation loss:		2.728297E-02
Epoch took 1.396s

Epoch 15 of 300
  training loss:		2.807878E-02
  validation loss:		2.757158E-02
Epoch took 1.395s

Epoch 16 of 300
  training loss:		2.813666E-02
  validation loss:		2.658524E-02
Epoch took 1.396s

Epoch 17 of 300
  training loss:		2.793240E-02
  validation loss:		2.603148E-02
Epoch took 1.396s

Epoch 18 of 300
  training loss:		2.755392E-02
  validation loss:		2.620298E-02
Epoch took 1.396s

Epoch 19 of 300
  training loss:		2.789499E-02
  validation loss:		2.756664E-02
Epoch took 1.394s

Epoch 20 of 300
  training loss:		2.751307E-02
  validation loss:		2.621362E-02
Epoch took 1.395s

Epoch 21 of 300
  training loss:		2.741671E-02
  validation loss:		2.643972E-02
Epoch took 1.395s

Epoch 22 of 300
  training loss:		2.735670E-02
  validation loss:		2.605106E-02
Epoch took 1.394s

Epoch 23 of 300
  training loss:		2.747796E-02
  validation loss:		2.674285E-02
Epoch took 1.395s

Epoch 24 of 300
  training loss:		2.752410E-02
  validation loss:		2.596327E-02
Epoch took 1.394s

Epoch 25 of 300
  training loss:		2.708402E-02
  validation loss:		2.619218E-02
Epoch took 1.395s

Epoch 26 of 300
  training loss:		2.696920E-02
  validation loss:		2.611200E-02
Epoch took 1.394s

Epoch 27 of 300
  training loss:		2.721129E-02
  validation loss:		2.610064E-02
Epoch took 1.395s

Epoch 28 of 300
  training loss:		2.701319E-02
  validation loss:		2.577660E-02
Epoch took 1.394s

Epoch 29 of 300
  training loss:		2.695838E-02
  validation loss:		2.612601E-02
Epoch took 1.394s

Epoch 30 of 300
  training loss:		2.689262E-02
  validation loss:		2.594712E-02
Epoch took 1.394s

Epoch 31 of 300
  training loss:		2.703185E-02
  validation loss:		2.615834E-02
Epoch took 1.394s

Epoch 32 of 300
  training loss:		2.681590E-02
  validation loss:		2.578845E-02
Epoch took 1.393s

Epoch 33 of 300
  training loss:		2.685716E-02
  validation loss:		2.630198E-02
Epoch took 1.393s

Epoch 34 of 300
  training loss:		2.681290E-02
  validation loss:		2.598040E-02
Epoch took 1.393s

Epoch 35 of 300
  training loss:		2.684046E-02
  validation loss:		2.559837E-02
Epoch took 1.394s

Epoch 36 of 300
  training loss:		2.742386E-02
  validation loss:		2.569617E-02
Epoch took 1.392s

Epoch 37 of 300
  training loss:		2.662446E-02
  validation loss:		2.603384E-02
Epoch took 1.395s

Epoch 38 of 300
  training loss:		2.657109E-02
  validation loss:		2.553249E-02
Epoch took 1.392s

Epoch 39 of 300
  training loss:		2.679600E-02
  validation loss:		2.552210E-02
Epoch took 1.392s

Epoch 40 of 300
  training loss:		2.714576E-02
  validation loss:		2.571494E-02
Epoch took 1.391s

Epoch 41 of 300
  training loss:		2.661915E-02
  validation loss:		2.568796E-02
Epoch took 1.393s

Epoch 42 of 300
  training loss:		2.662180E-02
  validation loss:		2.561418E-02
Epoch took 1.394s

Epoch 43 of 300
  training loss:		2.658122E-02
  validation loss:		2.580948E-02
Epoch took 1.392s

Epoch 44 of 300
  training loss:		2.652885E-02
  validation loss:		2.532730E-02
Epoch took 1.392s

Epoch 45 of 300
  training loss:		2.712192E-02
  validation loss:		2.551091E-02
Epoch took 1.392s

Epoch 46 of 300
  training loss:		2.652652E-02
  validation loss:		2.571049E-02
Epoch took 1.393s

Epoch 47 of 300
  training loss:		2.649634E-02
  validation loss:		2.547764E-02
Epoch took 1.393s

Epoch 48 of 300
  training loss:		2.648174E-02
  validation loss:		2.556461E-02
Epoch took 1.391s

Epoch 49 of 300
  training loss:		2.659271E-02
  validation loss:		3.028447E-02
Epoch took 1.392s

Epoch 50 of 300
  training loss:		2.728276E-02
  validation loss:		2.544466E-02
Epoch took 1.391s

Epoch 51 of 300
  training loss:		2.634073E-02
  validation loss:		2.532494E-02
Epoch took 1.395s

Epoch 52 of 300
  training loss:		2.632548E-02
  validation loss:		2.534821E-02
Epoch took 1.391s

Epoch 53 of 300
  training loss:		2.636110E-02
  validation loss:		2.566285E-02
Epoch took 1.391s

Epoch 54 of 300
  training loss:		2.637528E-02
  validation loss:		2.530420E-02
Epoch took 1.391s

Epoch 55 of 300
  training loss:		2.693063E-02
  validation loss:		2.554667E-02
Epoch took 1.392s

Epoch 56 of 300
  training loss:		2.642326E-02
  validation loss:		2.558968E-02
Epoch took 1.392s

Epoch 57 of 300
  training loss:		2.633739E-02
  validation loss:		2.540663E-02
Epoch took 1.394s

Epoch 58 of 300
  training loss:		2.632354E-02
  validation loss:		2.550191E-02
Epoch took 1.392s

Epoch 59 of 300
  training loss:		2.724281E-02
  validation loss:		2.572523E-02
Epoch took 1.391s

Epoch 60 of 300
  training loss:		2.630901E-02
  validation loss:		2.543719E-02
Epoch took 1.395s

Epoch 61 of 300
  training loss:		2.627212E-02
  validation loss:		2.524686E-02
Epoch took 1.394s

Epoch 62 of 300
  training loss:		2.626726E-02
  validation loss:		2.551541E-02
Epoch took 1.391s

Epoch 63 of 300
  training loss:		2.641812E-02
  validation loss:		2.528650E-02
Epoch took 1.390s

Epoch 64 of 300
  training loss:		2.723455E-02
  validation loss:		2.617379E-02
Epoch took 1.391s

Epoch 65 of 300
  training loss:		2.634146E-02
  validation loss:		2.521644E-02
Epoch took 1.391s

Epoch 66 of 300
  training loss:		2.633300E-02
  validation loss:		2.538608E-02
Epoch took 1.394s

Epoch 67 of 300
  training loss:		2.627889E-02
  validation loss:		2.529346E-02
Epoch took 1.391s

Epoch 68 of 300
  training loss:		2.680848E-02
  validation loss:		2.534184E-02
Epoch took 1.391s

Epoch 69 of 300
  training loss:		2.624540E-02
  validation loss:		2.520553E-02
Epoch took 1.391s

Epoch 70 of 300
  training loss:		2.619936E-02
  validation loss:		2.527187E-02
Epoch took 1.392s

Epoch 71 of 300
  training loss:		2.635950E-02
  validation loss:		2.587128E-02
Epoch took 1.390s

Epoch 72 of 300
  training loss:		2.660270E-02
  validation loss:		2.537359E-02
Epoch took 1.389s

Epoch 73 of 300
  training loss:		2.632154E-02
  validation loss:		2.538140E-02
Epoch took 1.394s

Epoch 74 of 300
  training loss:		2.640701E-02
  validation loss:		2.521953E-02
Epoch took 1.390s

Epoch 75 of 300
  training loss:		2.628269E-02
  validation loss:		2.521879E-02
Epoch took 1.391s

Epoch 76 of 300
  training loss:		2.622106E-02
  validation loss:		2.523015E-02
Epoch took 1.393s

Epoch 77 of 300
  training loss:		2.717478E-02
  validation loss:		2.542889E-02
Epoch took 1.390s

Epoch 78 of 300
  training loss:		2.625511E-02
  validation loss:		2.562052E-02
Epoch took 1.391s

Epoch 79 of 300
  training loss:		2.621538E-02
  validation loss:		2.537865E-02
Epoch took 1.392s

Epoch 80 of 300
  training loss:		2.625811E-02
  validation loss:		2.542684E-02
Epoch took 1.391s

Epoch 81 of 300
  training loss:		2.623685E-02
  validation loss:		2.528091E-02
Epoch took 1.391s

Epoch 82 of 300
  training loss:		2.709576E-02
  validation loss:		2.580510E-02
Epoch took 1.391s

Epoch 83 of 300
  training loss:		2.631780E-02
  validation loss:		2.528449E-02
Epoch took 1.390s

Epoch 84 of 300
  training loss:		2.617458E-02
  validation loss:		2.512058E-02
Epoch took 1.395s

Epoch 85 of 300
  training loss:		2.618855E-02
  validation loss:		2.522311E-02
Epoch took 1.390s

Epoch 86 of 300
  training loss:		2.620433E-02
  validation loss:		2.850696E-02
Epoch took 1.390s

Epoch 87 of 300
  training loss:		2.681965E-02
  validation loss:		2.524539E-02
Epoch took 1.389s

Epoch 88 of 300
  training loss:		2.617281E-02
  validation loss:		2.523385E-02
Epoch took 1.395s

Epoch 89 of 300
  training loss:		2.616208E-02
  validation loss:		2.517974E-02
Epoch took 1.392s

Epoch 90 of 300
  training loss:		2.620697E-02
  validation loss:		2.511471E-02
Epoch took 1.390s

Epoch 91 of 300
  training loss:		2.623037E-02
  validation loss:		2.519969E-02
Epoch took 1.390s

Epoch 92 of 300
  training loss:		2.682043E-02
  validation loss:		2.698053E-02
Epoch took 1.391s

Epoch 93 of 300
  training loss:		2.632244E-02
  validation loss:		2.525194E-02
Epoch took 1.390s

Epoch 94 of 300
  training loss:		2.618102E-02
  validation loss:		2.522600E-02
Epoch took 1.395s

Epoch 95 of 300
  training loss:		2.617837E-02
  validation loss:		2.523488E-02
Epoch took 1.391s

Epoch 96 of 300
  training loss:		2.619567E-02
  validation loss:		2.524265E-02
Epoch took 1.390s

Epoch 97 of 300
  training loss:		2.705148E-02
  validation loss:		2.525119E-02
Epoch took 1.389s

Epoch 98 of 300
  training loss:		2.619175E-02
  validation loss:		2.519082E-02
Epoch took 1.394s

Epoch 99 of 300
  training loss:		2.615406E-02
  validation loss:		2.528354E-02
Epoch took 1.391s

Epoch 100 of 300
  training loss:		2.615533E-02
  validation loss:		2.519893E-02
Epoch took 1.391s

Early stopping, val-loss increased over the last 20 epochs from 0.0254043701762 to 0.0255027503901
Saving model from epoch 80
Training MSE: 2.51702e-14
Validation MSE: 2.44195e-14
Training R2: 0.73329101337
Validation R2: 0.740195804408
