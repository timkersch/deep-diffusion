Training network with 56340 training samples and 18780 validation samples
Epoch 1 of 300
  training loss:		3.332947E-01
  validation loss:		8.637800E-02
Epoch took 0.708s

Epoch 2 of 300
  training loss:		7.472231E-02
  validation loss:		6.333563E-02
Epoch took 0.653s

Epoch 3 of 300
  training loss:		5.955260E-02
  validation loss:		5.644280E-02
Epoch took 0.655s

Epoch 4 of 300
  training loss:		5.518158E-02
  validation loss:		5.675008E-02
Epoch took 0.654s

Epoch 5 of 300
  training loss:		4.797796E-02
  validation loss:		4.385672E-02
Epoch took 0.654s

Epoch 6 of 300
  training loss:		4.738035E-02
  validation loss:		4.710537E-02
Epoch took 0.654s

Epoch 7 of 300
  training loss:		4.476937E-02
  validation loss:		4.197370E-02
Epoch took 0.655s

Epoch 8 of 300
  training loss:		3.981595E-02
  validation loss:		3.845709E-02
Epoch took 0.654s

Epoch 9 of 300
  training loss:		3.778467E-02
  validation loss:		3.502570E-02
Epoch took 0.654s

Epoch 10 of 300
  training loss:		3.857265E-02
  validation loss:		3.768198E-02
Epoch took 0.653s

Epoch 11 of 300
  training loss:		3.779365E-02
  validation loss:		3.222211E-02
Epoch took 0.653s

Epoch 12 of 300
  training loss:		3.386135E-02
  validation loss:		3.260850E-02
Epoch took 0.656s

Epoch 13 of 300
  training loss:		3.499408E-02
  validation loss:		3.489237E-02
Epoch took 0.653s

Epoch 14 of 300
  training loss:		3.187609E-02
  validation loss:		2.913512E-02
Epoch took 0.654s

Epoch 15 of 300
  training loss:		3.029608E-02
  validation loss:		2.919803E-02
Epoch took 0.655s

Epoch 16 of 300
  training loss:		3.046924E-02
  validation loss:		2.841266E-02
Epoch took 0.653s

Epoch 17 of 300
  training loss:		3.284203E-02
  validation loss:		3.077768E-02
Epoch took 0.655s

Epoch 18 of 300
  training loss:		3.045493E-02
  validation loss:		2.890735E-02
Epoch took 0.656s

Epoch 19 of 300
  training loss:		2.951721E-02
  validation loss:		2.797576E-02
Epoch took 0.658s

Epoch 20 of 300
  training loss:		2.837100E-02
  validation loss:		2.718818E-02
Epoch took 0.658s

Epoch 21 of 300
  training loss:		2.843890E-02
  validation loss:		2.725354E-02
Epoch took 0.663s

Epoch 22 of 300
  training loss:		2.842115E-02
  validation loss:		2.702506E-02
Epoch took 0.658s

Epoch 23 of 300
  training loss:		2.749593E-02
  validation loss:		2.659957E-02
Epoch took 0.662s

Epoch 24 of 300
  training loss:		2.731500E-02
  validation loss:		2.605338E-02
Epoch took 0.662s

Epoch 25 of 300
  training loss:		2.768382E-02
  validation loss:		2.744057E-02
Epoch took 0.661s

Epoch 26 of 300
  training loss:		3.075877E-02
  validation loss:		2.869115E-02
Epoch took 0.661s

Epoch 27 of 300
  training loss:		2.879117E-02
  validation loss:		2.653818E-02
Epoch took 0.662s

Epoch 28 of 300
  training loss:		2.739647E-02
  validation loss:		2.616638E-02
Epoch took 0.661s

Epoch 29 of 300
  training loss:		2.748582E-02
  validation loss:		2.691875E-02
Epoch took 0.663s

Epoch 30 of 300
  training loss:		2.718726E-02
  validation loss:		2.607230E-02
Epoch took 0.662s

Epoch 31 of 300
  training loss:		2.725657E-02
  validation loss:		2.613010E-02
Epoch took 0.660s

Epoch 32 of 300
  training loss:		2.780525E-02
  validation loss:		2.643607E-02
Epoch took 0.659s

Epoch 33 of 300
  training loss:		2.741662E-02
  validation loss:		2.714631E-02
Epoch took 0.656s

Epoch 34 of 300
  training loss:		2.755022E-02
  validation loss:		2.638056E-02
Epoch took 0.658s

Epoch 35 of 300
  training loss:		2.722080E-02
  validation loss:		2.639254E-02
Epoch took 0.658s

Epoch 36 of 300
  training loss:		2.712385E-02
  validation loss:		2.639059E-02
Epoch took 0.659s

Epoch 37 of 300
  training loss:		2.705178E-02
  validation loss:		2.588107E-02
Epoch took 0.657s

Epoch 38 of 300
  training loss:		2.716829E-02
  validation loss:		2.618187E-02
Epoch took 0.657s

Epoch 39 of 300
  training loss:		2.747343E-02
  validation loss:		2.606722E-02
Epoch took 0.654s

Epoch 40 of 300
  training loss:		2.784021E-02
  validation loss:		2.626022E-02
Epoch took 0.655s

Epoch 41 of 300
  training loss:		2.705714E-02
  validation loss:		2.701733E-02
Epoch took 0.656s

Epoch 42 of 300
  training loss:		2.745995E-02
  validation loss:		2.662187E-02
Epoch took 0.657s

Epoch 43 of 300
  training loss:		2.788315E-02
  validation loss:		2.810278E-02
Epoch took 0.657s

Epoch 44 of 300
  training loss:		2.754382E-02
  validation loss:		2.634136E-02
Epoch took 0.657s

Epoch 45 of 300
  training loss:		2.732710E-02
  validation loss:		2.568313E-02
Epoch took 0.658s

Epoch 46 of 300
  training loss:		2.712971E-02
  validation loss:		2.550747E-02
Epoch took 0.657s

Epoch 47 of 300
  training loss:		2.714336E-02
  validation loss:		2.609727E-02
Epoch took 0.659s

Epoch 48 of 300
  training loss:		2.709722E-02
  validation loss:		2.592350E-02
Epoch took 0.656s

Epoch 49 of 300
  training loss:		2.690249E-02
  validation loss:		2.556831E-02
Epoch took 0.657s

Epoch 50 of 300
  training loss:		2.770148E-02
  validation loss:		2.609011E-02
Epoch took 0.654s

Epoch 51 of 300
  training loss:		2.714772E-02
  validation loss:		2.621923E-02
Epoch took 0.658s

Epoch 52 of 300
  training loss:		2.733327E-02
  validation loss:		2.671379E-02
Epoch took 0.657s

Epoch 53 of 300
  training loss:		2.748169E-02
  validation loss:		2.628702E-02
Epoch took 0.658s

Epoch 54 of 300
  training loss:		2.705592E-02
  validation loss:		2.571068E-02
Epoch took 0.655s

Epoch 55 of 300
  training loss:		2.665617E-02
  validation loss:		2.609625E-02
Epoch took 0.656s

Epoch 56 of 300
  training loss:		2.693392E-02
  validation loss:		2.566263E-02
Epoch took 0.656s

Epoch 57 of 300
  training loss:		2.722674E-02
  validation loss:		2.611993E-02
Epoch took 0.655s

Epoch 58 of 300
  training loss:		2.692442E-02
  validation loss:		2.592100E-02
Epoch took 0.655s

Epoch 59 of 300
  training loss:		2.678688E-02
  validation loss:		2.585757E-02
Epoch took 0.655s

Epoch 60 of 300
  training loss:		2.678112E-02
  validation loss:		2.621512E-02
Epoch took 0.656s

Epoch 61 of 300
  training loss:		2.686651E-02
  validation loss:		2.575527E-02
Epoch took 0.656s

Epoch 62 of 300
  training loss:		2.690644E-02
  validation loss:		2.568811E-02
Epoch took 0.654s

Epoch 63 of 300
  training loss:		2.689338E-02
  validation loss:		2.547774E-02
Epoch took 0.656s

Epoch 64 of 300
  training loss:		2.686333E-02
  validation loss:		2.574529E-02
Epoch took 0.656s

Epoch 65 of 300
  training loss:		2.671217E-02
  validation loss:		2.545340E-02
Epoch took 0.658s

Epoch 66 of 300
  training loss:		2.681031E-02
  validation loss:		2.562537E-02
Epoch took 0.657s

Epoch 67 of 300
  training loss:		2.727239E-02
  validation loss:		2.573697E-02
Epoch took 0.655s

Epoch 68 of 300
  training loss:		2.690769E-02
  validation loss:		2.568432E-02
Epoch took 0.657s

Epoch 69 of 300
  training loss:		2.671450E-02
  validation loss:		2.555980E-02
Epoch took 0.653s

Epoch 70 of 300
  training loss:		2.679428E-02
  validation loss:		2.579915E-02
Epoch took 0.655s

Epoch 71 of 300
  training loss:		2.703024E-02
  validation loss:		2.668712E-02
Epoch took 0.658s

Epoch 72 of 300
  training loss:		2.693497E-02
  validation loss:		2.570959E-02
Epoch took 0.657s

Epoch 73 of 300
  training loss:		2.662347E-02
  validation loss:		2.586094E-02
Epoch took 0.657s

Epoch 74 of 300
  training loss:		2.680623E-02
  validation loss:		2.564134E-02
Epoch took 0.655s

Epoch 75 of 300
  training loss:		2.681078E-02
  validation loss:		2.575187E-02
Epoch took 0.657s

Epoch 76 of 300
  training loss:		2.700624E-02
  validation loss:		2.630444E-02
Epoch took 0.655s

Epoch 77 of 300
  training loss:		2.682616E-02
  validation loss:		2.583270E-02
Epoch took 0.656s

Epoch 78 of 300
  training loss:		2.672638E-02
  validation loss:		2.559000E-02
Epoch took 0.654s

Epoch 79 of 300
  training loss:		2.699298E-02
  validation loss:		2.591548E-02
Epoch took 0.656s

Epoch 80 of 300
  training loss:		2.671722E-02
  validation loss:		2.554518E-02
Epoch took 0.656s

Epoch 81 of 300
  training loss:		2.684358E-02
  validation loss:		2.610592E-02
Epoch took 0.655s

Epoch 82 of 300
  training loss:		2.716066E-02
  validation loss:		2.578362E-02
Epoch took 0.653s

Epoch 83 of 300
  training loss:		2.696583E-02
  validation loss:		2.560207E-02
Epoch took 0.653s

Epoch 84 of 300
  training loss:		2.687845E-02
  validation loss:		2.575419E-02
Epoch took 0.655s

Epoch 85 of 300
  training loss:		2.666707E-02
  validation loss:		2.601557E-02
Epoch took 0.655s

Epoch 86 of 300
  training loss:		2.667657E-02
  validation loss:		2.590248E-02
Epoch took 0.654s

Epoch 87 of 300
  training loss:		2.703354E-02
  validation loss:		2.609035E-02
Epoch took 0.656s

Epoch 88 of 300
  training loss:		2.697559E-02
  validation loss:		2.611923E-02
Epoch took 0.654s

Epoch 89 of 300
  training loss:		2.687736E-02
  validation loss:		2.618881E-02
Epoch took 0.657s

Epoch 90 of 300
  training loss:		2.689611E-02
  validation loss:		2.564209E-02
Epoch took 0.659s

Epoch 91 of 300
  training loss:		2.675198E-02
  validation loss:		2.558481E-02
Epoch took 0.658s

Epoch 92 of 300
  training loss:		2.674743E-02
  validation loss:		2.592209E-02
Epoch took 0.655s

Epoch 93 of 300
  training loss:		2.669047E-02
  validation loss:		2.592075E-02
Epoch took 0.657s

Epoch 94 of 300
  training loss:		2.708060E-02
  validation loss:		2.617882E-02
Epoch took 0.656s

Epoch 95 of 300
  training loss:		2.696389E-02
  validation loss:		2.611054E-02
Epoch took 0.658s

Epoch 96 of 300
  training loss:		2.683375E-02
  validation loss:		2.592594E-02
Epoch took 0.656s

Epoch 97 of 300
  training loss:		2.680323E-02
  validation loss:		2.617787E-02
Epoch took 0.658s

Epoch 98 of 300
  training loss:		2.680694E-02
  validation loss:		2.568288E-02
Epoch took 0.656s

Epoch 99 of 300
  training loss:		2.674430E-02
  validation loss:		2.573926E-02
Epoch took 0.654s

Epoch 100 of 300
  training loss:		2.670812E-02
  validation loss:		2.578442E-02
Epoch took 0.656s

Early stopping, val-loss increased over the last 20 epochs from 0.025768204737 to 0.0259115855416
Saving model from epoch 80
Training MSE: 2.53573e-14
Validation MSE: 2.45733e-14
Training R2: 0.731308570356
Validation R2: 0.738559557398
