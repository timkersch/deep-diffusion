Training network with 56340 training samples and 18780 validation samples
Epoch 1 of 300
  training loss:		1.916106E-01
  validation loss:		1.011333E-01
Epoch took 0.704s

Epoch 2 of 300
  training loss:		8.464914E-02
  validation loss:		7.405269E-02
Epoch took 0.654s

Epoch 3 of 300
  training loss:		7.031853E-02
  validation loss:		6.497076E-02
Epoch took 0.656s

Epoch 4 of 300
  training loss:		6.322114E-02
  validation loss:		5.904817E-02
Epoch took 0.654s

Epoch 5 of 300
  training loss:		5.766629E-02
  validation loss:		5.402913E-02
Epoch took 0.651s

Epoch 6 of 300
  training loss:		5.331197E-02
  validation loss:		5.008977E-02
Epoch took 0.654s

Epoch 7 of 300
  training loss:		4.971018E-02
  validation loss:		4.691783E-02
Epoch took 0.654s

Epoch 8 of 300
  training loss:		4.679704E-02
  validation loss:		4.416774E-02
Epoch took 0.653s

Epoch 9 of 300
  training loss:		4.403860E-02
  validation loss:		4.148576E-02
Epoch took 0.652s

Epoch 10 of 300
  training loss:		4.223678E-02
  validation loss:		3.968621E-02
Epoch took 0.653s

Epoch 11 of 300
  training loss:		4.045940E-02
  validation loss:		3.919485E-02
Epoch took 0.655s

Epoch 12 of 300
  training loss:		3.894446E-02
  validation loss:		3.732904E-02
Epoch took 0.654s

Epoch 13 of 300
  training loss:		3.737731E-02
  validation loss:		3.524886E-02
Epoch took 0.654s

Epoch 14 of 300
  training loss:		3.630327E-02
  validation loss:		3.465649E-02
Epoch took 0.653s

Epoch 15 of 300
  training loss:		3.509461E-02
  validation loss:		3.305955E-02
Epoch took 0.657s

Epoch 16 of 300
  training loss:		3.423981E-02
  validation loss:		3.194589E-02
Epoch took 0.652s

Epoch 17 of 300
  training loss:		3.348145E-02
  validation loss:		3.174995E-02
Epoch took 0.657s

Epoch 18 of 300
  training loss:		3.301996E-02
  validation loss:		3.304328E-02
Epoch took 0.656s

Epoch 19 of 300
  training loss:		3.315815E-02
  validation loss:		3.340883E-02
Epoch took 0.657s

Epoch 20 of 300
  training loss:		3.122210E-02
  validation loss:		2.948743E-02
Epoch took 0.654s

Epoch 21 of 300
  training loss:		3.123469E-02
  validation loss:		2.968567E-02
Epoch took 0.657s

Epoch 22 of 300
  training loss:		3.045380E-02
  validation loss:		2.895830E-02
Epoch took 0.657s

Epoch 23 of 300
  training loss:		3.018923E-02
  validation loss:		3.128197E-02
Epoch took 0.660s

Epoch 24 of 300
  training loss:		2.998990E-02
  validation loss:		2.942612E-02
Epoch took 0.660s

Epoch 25 of 300
  training loss:		3.013228E-02
  validation loss:		2.821689E-02
Epoch took 0.660s

Epoch 26 of 300
  training loss:		2.973238E-02
  validation loss:		2.780968E-02
Epoch took 0.659s

Epoch 27 of 300
  training loss:		2.902208E-02
  validation loss:		2.846340E-02
Epoch took 0.658s

Epoch 28 of 300
  training loss:		3.139768E-02
  validation loss:		2.753483E-02
Epoch took 0.661s

Epoch 29 of 300
  training loss:		2.891027E-02
  validation loss:		2.736011E-02
Epoch took 0.659s

Epoch 30 of 300
  training loss:		2.876287E-02
  validation loss:		2.732357E-02
Epoch took 0.659s

Epoch 31 of 300
  training loss:		2.902215E-02
  validation loss:		2.758099E-02
Epoch took 0.658s

Epoch 32 of 300
  training loss:		2.852868E-02
  validation loss:		2.945051E-02
Epoch took 0.659s

Epoch 33 of 300
  training loss:		2.803864E-02
  validation loss:		2.644016E-02
Epoch took 0.660s

Epoch 34 of 300
  training loss:		2.861497E-02
  validation loss:		2.762379E-02
Epoch took 0.657s

Epoch 35 of 300
  training loss:		2.766606E-02
  validation loss:		2.675069E-02
Epoch took 0.657s

Epoch 36 of 300
  training loss:		2.776602E-02
  validation loss:		2.778778E-02
Epoch took 0.660s

Epoch 37 of 300
  training loss:		2.803644E-02
  validation loss:		2.736650E-02
Epoch took 0.655s

Epoch 38 of 300
  training loss:		2.841495E-02
  validation loss:		2.807267E-02
Epoch took 0.657s

Epoch 39 of 300
  training loss:		2.873027E-02
  validation loss:		2.940362E-02
Epoch took 0.657s

Epoch 40 of 300
  training loss:		2.749198E-02
  validation loss:		2.620741E-02
Epoch took 0.653s

Epoch 41 of 300
  training loss:		2.757488E-02
  validation loss:		2.826686E-02
Epoch took 0.657s

Epoch 42 of 300
  training loss:		2.771377E-02
  validation loss:		2.622197E-02
Epoch took 0.658s

Epoch 43 of 300
  training loss:		2.773969E-02
  validation loss:		2.669859E-02
Epoch took 0.655s

Epoch 44 of 300
  training loss:		2.725203E-02
  validation loss:		2.609399E-02
Epoch took 0.659s

Epoch 45 of 300
  training loss:		2.761960E-02
  validation loss:		2.930571E-02
Epoch took 0.659s

Epoch 46 of 300
  training loss:		2.999504E-02
  validation loss:		2.795358E-02
Epoch took 0.660s

Epoch 47 of 300
  training loss:		2.809532E-02
  validation loss:		2.614007E-02
Epoch took 0.660s

Epoch 48 of 300
  training loss:		2.704658E-02
  validation loss:		2.591756E-02
Epoch took 0.655s

Epoch 49 of 300
  training loss:		2.743266E-02
  validation loss:		2.568035E-02
Epoch took 0.658s

Epoch 50 of 300
  training loss:		2.739636E-02
  validation loss:		2.834037E-02
Epoch took 0.656s

Epoch 51 of 300
  training loss:		2.905553E-02
  validation loss:		2.730605E-02
Epoch took 0.656s

Epoch 52 of 300
  training loss:		2.818474E-02
  validation loss:		2.938887E-02
Epoch took 0.657s

Epoch 53 of 300
  training loss:		2.756296E-02
  validation loss:		2.613503E-02
Epoch took 0.655s

Epoch 54 of 300
  training loss:		2.729836E-02
  validation loss:		2.572954E-02
Epoch took 0.656s

Epoch 55 of 300
  training loss:		2.740886E-02
  validation loss:		2.836102E-02
Epoch took 0.657s

Epoch 56 of 300
  training loss:		2.721146E-02
  validation loss:		2.616877E-02
Epoch took 0.653s

Epoch 57 of 300
  training loss:		2.743907E-02
  validation loss:		2.635823E-02
Epoch took 0.654s

Epoch 58 of 300
  training loss:		2.725271E-02
  validation loss:		2.712637E-02
Epoch took 0.656s

Epoch 59 of 300
  training loss:		2.704194E-02
  validation loss:		2.580932E-02
Epoch took 0.658s

Epoch 60 of 300
  training loss:		2.677024E-02
  validation loss:		2.644084E-02
Epoch took 0.658s

Epoch 61 of 300
  training loss:		2.691407E-02
  validation loss:		2.613618E-02
Epoch took 0.655s

Epoch 62 of 300
  training loss:		2.721120E-02
  validation loss:		2.595664E-02
Epoch took 0.656s

Epoch 63 of 300
  training loss:		2.749876E-02
  validation loss:		2.648789E-02
Epoch took 0.654s

Epoch 64 of 300
  training loss:		2.703339E-02
  validation loss:		2.596756E-02
Epoch took 0.656s

Epoch 65 of 300
  training loss:		2.749368E-02
  validation loss:		2.555989E-02
Epoch took 0.655s

Epoch 66 of 300
  training loss:		2.705834E-02
  validation loss:		2.581084E-02
Epoch took 0.656s

Epoch 67 of 300
  training loss:		2.694685E-02
  validation loss:		2.948689E-02
Epoch took 0.656s

Epoch 68 of 300
  training loss:		2.871067E-02
  validation loss:		2.629233E-02
Epoch took 0.659s

Epoch 69 of 300
  training loss:		2.705602E-02
  validation loss:		2.588494E-02
Epoch took 0.656s

Epoch 70 of 300
  training loss:		2.709658E-02
  validation loss:		2.624618E-02
Epoch took 0.653s

Epoch 71 of 300
  training loss:		2.716625E-02
  validation loss:		2.734342E-02
Epoch took 0.656s

Epoch 72 of 300
  training loss:		2.737460E-02
  validation loss:		2.596795E-02
Epoch took 0.656s

Epoch 73 of 300
  training loss:		2.716260E-02
  validation loss:		2.641443E-02
Epoch took 0.658s

Epoch 74 of 300
  training loss:		2.734475E-02
  validation loss:		2.577763E-02
Epoch took 0.655s

Epoch 75 of 300
  training loss:		2.687206E-02
  validation loss:		2.575511E-02
Epoch took 0.654s

Epoch 76 of 300
  training loss:		2.685742E-02
  validation loss:		2.557671E-02
Epoch took 0.656s

Epoch 77 of 300
  training loss:		2.679214E-02
  validation loss:		2.603937E-02
Epoch took 0.657s

Epoch 78 of 300
  training loss:		2.723355E-02
  validation loss:		2.602195E-02
Epoch took 0.657s

Epoch 79 of 300
  training loss:		2.728632E-02
  validation loss:		2.621365E-02
Epoch took 0.654s

Epoch 80 of 300
  training loss:		2.760692E-02
  validation loss:		2.607812E-02
Epoch took 0.656s

Epoch 81 of 300
  training loss:		2.769451E-02
  validation loss:		2.726361E-02
Epoch took 0.656s

Epoch 82 of 300
  training loss:		2.742170E-02
  validation loss:		2.739900E-02
Epoch took 0.658s

Epoch 83 of 300
  training loss:		2.683138E-02
  validation loss:		2.569388E-02
Epoch took 0.653s

Epoch 84 of 300
  training loss:		2.753161E-02
  validation loss:		2.839090E-02
Epoch took 0.655s

Epoch 85 of 300
  training loss:		2.816452E-02
  validation loss:		2.801301E-02
Epoch took 0.657s

Epoch 86 of 300
  training loss:		2.724861E-02
  validation loss:		2.546168E-02
Epoch took 0.656s

Epoch 87 of 300
  training loss:		2.671390E-02
  validation loss:		2.598860E-02
Epoch took 0.654s

Epoch 88 of 300
  training loss:		2.704882E-02
  validation loss:		2.655010E-02
Epoch took 0.662s

Epoch 89 of 300
  training loss:		2.700565E-02
  validation loss:		2.635087E-02
Epoch took 0.657s

Epoch 90 of 300
  training loss:		2.714750E-02
  validation loss:		2.582062E-02
Epoch took 0.656s

Epoch 91 of 300
  training loss:		2.675783E-02
  validation loss:		2.648010E-02
Epoch took 0.658s

Epoch 92 of 300
  training loss:		2.697166E-02
  validation loss:		2.639894E-02
Epoch took 0.656s

Epoch 93 of 300
  training loss:		2.727023E-02
  validation loss:		2.644856E-02
Epoch took 0.653s

Epoch 94 of 300
  training loss:		2.692414E-02
  validation loss:		2.588916E-02
Epoch took 0.654s

Epoch 95 of 300
  training loss:		2.698990E-02
  validation loss:		2.593717E-02
Epoch took 0.657s

Epoch 96 of 300
  training loss:		2.682684E-02
  validation loss:		2.580457E-02
Epoch took 0.656s

Epoch 97 of 300
  training loss:		2.746018E-02
  validation loss:		2.684816E-02
Epoch took 0.654s

Epoch 98 of 300
  training loss:		2.720236E-02
  validation loss:		2.599449E-02
Epoch took 0.657s

Epoch 99 of 300
  training loss:		2.720512E-02
  validation loss:		2.579380E-02
Epoch took 0.655s

Epoch 100 of 300
  training loss:		2.724587E-02
  validation loss:		2.586174E-02
Epoch took 0.656s

Early stopping, val-loss increased over the last 20 epochs from 0.0262508840197 to 0.026419447052
Saving model from epoch 80
Training MSE: 2.58232e-14
Validation MSE: 2.50612e-14
Training R2: 0.726371722675
Validation R2: 0.733368391129
