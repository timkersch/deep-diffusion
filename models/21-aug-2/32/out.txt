Training network with 56340 training samples and 18780 validation samples
Epoch 1 of 300
  training loss:		1.698334E-01
  validation loss:		8.540717E-02
Epoch took 0.721s

Epoch 2 of 300
  training loss:		7.743013E-02
  validation loss:		6.749050E-02
Epoch took 0.654s

Epoch 3 of 300
  training loss:		6.289542E-02
  validation loss:		5.631652E-02
Epoch took 0.658s

Epoch 4 of 300
  training loss:		5.393471E-02
  validation loss:		4.990246E-02
Epoch took 0.658s

Epoch 5 of 300
  training loss:		4.826507E-02
  validation loss:		4.408896E-02
Epoch took 0.658s

Epoch 6 of 300
  training loss:		4.424953E-02
  validation loss:		4.070109E-02
Epoch took 0.660s

Epoch 7 of 300
  training loss:		4.134682E-02
  validation loss:		3.797878E-02
Epoch took 0.656s

Epoch 8 of 300
  training loss:		3.862531E-02
  validation loss:		3.680104E-02
Epoch took 0.655s

Epoch 9 of 300
  training loss:		3.637000E-02
  validation loss:		3.360785E-02
Epoch took 0.655s

Epoch 10 of 300
  training loss:		3.571211E-02
  validation loss:		3.437735E-02
Epoch took 0.656s

Epoch 11 of 300
  training loss:		3.532570E-02
  validation loss:		3.437851E-02
Epoch took 0.657s

Epoch 12 of 300
  training loss:		3.377517E-02
  validation loss:		3.170350E-02
Epoch took 0.656s

Epoch 13 of 300
  training loss:		3.202096E-02
  validation loss:		3.032720E-02
Epoch took 0.655s

Epoch 14 of 300
  training loss:		3.079760E-02
  validation loss:		3.299738E-02
Epoch took 0.657s

Epoch 15 of 300
  training loss:		3.284978E-02
  validation loss:		2.927732E-02
Epoch took 0.658s

Epoch 16 of 300
  training loss:		3.210485E-02
  validation loss:		2.967839E-02
Epoch took 0.659s

Epoch 17 of 300
  training loss:		2.973820E-02
  validation loss:		2.836137E-02
Epoch took 0.661s

Epoch 18 of 300
  training loss:		3.008012E-02
  validation loss:		3.025775E-02
Epoch took 0.660s

Epoch 19 of 300
  training loss:		2.922383E-02
  validation loss:		2.766769E-02
Epoch took 0.660s

Epoch 20 of 300
  training loss:		2.860661E-02
  validation loss:		2.685519E-02
Epoch took 0.661s

Epoch 21 of 300
  training loss:		2.928822E-02
  validation loss:		2.759254E-02
Epoch took 0.667s

Epoch 22 of 300
  training loss:		3.005916E-02
  validation loss:		2.686558E-02
Epoch took 0.661s

Epoch 23 of 300
  training loss:		3.154287E-02
  validation loss:		2.773158E-02
Epoch took 0.663s

Epoch 24 of 300
  training loss:		2.852605E-02
  validation loss:		2.741333E-02
Epoch took 0.662s

Epoch 25 of 300
  training loss:		2.808432E-02
  validation loss:		2.631875E-02
Epoch took 0.662s

Epoch 26 of 300
  training loss:		2.834922E-02
  validation loss:		2.675120E-02
Epoch took 0.661s

Epoch 27 of 300
  training loss:		2.732048E-02
  validation loss:		2.658196E-02
Epoch took 0.662s

Epoch 28 of 300
  training loss:		2.774853E-02
  validation loss:		2.704699E-02
Epoch took 0.657s

Epoch 29 of 300
  training loss:		2.849908E-02
  validation loss:		2.725793E-02
Epoch took 0.664s

Epoch 30 of 300
  training loss:		2.864619E-02
  validation loss:		2.691140E-02
Epoch took 0.662s

Epoch 31 of 300
  training loss:		2.802336E-02
  validation loss:		2.613401E-02
Epoch took 0.660s

Epoch 32 of 300
  training loss:		2.737977E-02
  validation loss:		2.777092E-02
Epoch took 0.663s

Epoch 33 of 300
  training loss:		2.905214E-02
  validation loss:		2.636186E-02
Epoch took 0.658s

Epoch 34 of 300
  training loss:		2.826817E-02
  validation loss:		2.610801E-02
Epoch took 0.660s

Epoch 35 of 300
  training loss:		2.838517E-02
  validation loss:		2.748754E-02
Epoch took 0.661s

Epoch 36 of 300
  training loss:		2.756684E-02
  validation loss:		2.682454E-02
Epoch took 0.657s

Epoch 37 of 300
  training loss:		2.743904E-02
  validation loss:		2.664235E-02
Epoch took 0.659s

Epoch 38 of 300
  training loss:		2.776868E-02
  validation loss:		2.602051E-02
Epoch took 0.660s

Epoch 39 of 300
  training loss:		2.758342E-02
  validation loss:		2.649552E-02
Epoch took 0.658s

Epoch 40 of 300
  training loss:		2.715551E-02
  validation loss:		2.666410E-02
Epoch took 0.658s

Epoch 41 of 300
  training loss:		2.720049E-02
  validation loss:		2.613160E-02
Epoch took 0.657s

Epoch 42 of 300
  training loss:		2.813318E-02
  validation loss:		2.774754E-02
Epoch took 0.659s

Epoch 43 of 300
  training loss:		2.716222E-02
  validation loss:		2.602899E-02
Epoch took 0.660s

Epoch 44 of 300
  training loss:		2.762599E-02
  validation loss:		2.775330E-02
Epoch took 0.659s

Epoch 45 of 300
  training loss:		2.753836E-02
  validation loss:		2.591877E-02
Epoch took 0.659s

Epoch 46 of 300
  training loss:		2.723391E-02
  validation loss:		2.657275E-02
Epoch took 0.658s

Epoch 47 of 300
  training loss:		2.735444E-02
  validation loss:		2.718825E-02
Epoch took 0.659s

Epoch 48 of 300
  training loss:		2.741455E-02
  validation loss:		2.691540E-02
Epoch took 0.657s

Epoch 49 of 300
  training loss:		2.866655E-02
  validation loss:		2.766422E-02
Epoch took 0.659s

Epoch 50 of 300
  training loss:		2.770712E-02
  validation loss:		2.697712E-02
Epoch took 0.655s

Epoch 51 of 300
  training loss:		2.776850E-02
  validation loss:		2.619067E-02
Epoch took 0.657s

Epoch 52 of 300
  training loss:		2.719544E-02
  validation loss:		2.570139E-02
Epoch took 0.658s

Epoch 53 of 300
  training loss:		2.785567E-02
  validation loss:		2.780686E-02
Epoch took 0.657s

Epoch 54 of 300
  training loss:		2.766455E-02
  validation loss:		2.586129E-02
Epoch took 0.658s

Epoch 55 of 300
  training loss:		2.724818E-02
  validation loss:		2.604910E-02
Epoch took 0.660s

Epoch 56 of 300
  training loss:		2.779065E-02
  validation loss:		2.650037E-02
Epoch took 0.656s

Epoch 57 of 300
  training loss:		2.709180E-02
  validation loss:		2.577816E-02
Epoch took 0.656s

Epoch 58 of 300
  training loss:		2.679218E-02
  validation loss:		2.637462E-02
Epoch took 0.661s

Epoch 59 of 300
  training loss:		2.701590E-02
  validation loss:		2.876352E-02
Epoch took 0.659s

Epoch 60 of 300
  training loss:		2.725266E-02
  validation loss:		2.684231E-02
Epoch took 0.657s

Epoch 61 of 300
  training loss:		2.710842E-02
  validation loss:		2.577631E-02
Epoch took 0.657s

Epoch 62 of 300
  training loss:		2.711147E-02
  validation loss:		2.602898E-02
Epoch took 0.659s

Epoch 63 of 300
  training loss:		2.722710E-02
  validation loss:		2.690278E-02
Epoch took 0.659s

Epoch 64 of 300
  training loss:		2.727282E-02
  validation loss:		2.633567E-02
Epoch took 0.657s

Epoch 65 of 300
  training loss:		2.687918E-02
  validation loss:		2.568169E-02
Epoch took 0.659s

Epoch 66 of 300
  training loss:		2.680999E-02
  validation loss:		2.590714E-02
Epoch took 0.656s

Epoch 67 of 300
  training loss:		2.674077E-02
  validation loss:		2.609324E-02
Epoch took 0.659s

Epoch 68 of 300
  training loss:		2.713711E-02
  validation loss:		2.607803E-02
Epoch took 0.659s

Epoch 69 of 300
  training loss:		2.741826E-02
  validation loss:		2.605685E-02
Epoch took 0.659s

Epoch 70 of 300
  training loss:		2.714539E-02
  validation loss:		2.778038E-02
Epoch took 0.660s

Epoch 71 of 300
  training loss:		2.774717E-02
  validation loss:		2.642561E-02
Epoch took 0.659s

Epoch 72 of 300
  training loss:		2.763010E-02
  validation loss:		2.569488E-02
Epoch took 0.661s

Epoch 73 of 300
  training loss:		2.693127E-02
  validation loss:		2.569987E-02
Epoch took 0.657s

Epoch 74 of 300
  training loss:		2.714067E-02
  validation loss:		2.803510E-02
Epoch took 0.663s

Epoch 75 of 300
  training loss:		2.733146E-02
  validation loss:		2.565317E-02
Epoch took 0.658s

Epoch 76 of 300
  training loss:		2.689882E-02
  validation loss:		2.594319E-02
Epoch took 0.661s

Epoch 77 of 300
  training loss:		2.714188E-02
  validation loss:		2.563062E-02
Epoch took 0.657s

Epoch 78 of 300
  training loss:		2.716959E-02
  validation loss:		2.613937E-02
Epoch took 0.664s

Epoch 79 of 300
  training loss:		2.700305E-02
  validation loss:		2.586791E-02
Epoch took 0.658s

Epoch 80 of 300
  training loss:		2.695121E-02
  validation loss:		2.606336E-02
Epoch took 0.657s

Epoch 81 of 300
  training loss:		2.742538E-02
  validation loss:		2.684755E-02
Epoch took 0.660s

Epoch 82 of 300
  training loss:		2.700777E-02
  validation loss:		2.617020E-02
Epoch took 0.660s

Epoch 83 of 300
  training loss:		2.757979E-02
  validation loss:		2.608770E-02
Epoch took 0.659s

Epoch 84 of 300
  training loss:		2.729264E-02
  validation loss:		2.563511E-02
Epoch took 0.654s

Epoch 85 of 300
  training loss:		2.734394E-02
  validation loss:		2.605387E-02
Epoch took 0.657s

Epoch 86 of 300
  training loss:		2.708976E-02
  validation loss:		2.621041E-02
Epoch took 0.658s

Epoch 87 of 300
  training loss:		2.714550E-02
  validation loss:		2.586101E-02
Epoch took 0.662s

Epoch 88 of 300
  training loss:		2.711965E-02
  validation loss:		2.585275E-02
Epoch took 0.656s

Epoch 89 of 300
  training loss:		2.705625E-02
  validation loss:		2.561604E-02
Epoch took 0.661s

Epoch 90 of 300
  training loss:		2.704472E-02
  validation loss:		2.544929E-02
Epoch took 0.660s

Epoch 91 of 300
  training loss:		2.747210E-02
  validation loss:		2.578633E-02
Epoch took 0.658s

Epoch 92 of 300
  training loss:		2.699850E-02
  validation loss:		2.574279E-02
Epoch took 0.659s

Epoch 93 of 300
  training loss:		2.694824E-02
  validation loss:		2.586635E-02
Epoch took 0.658s

Epoch 94 of 300
  training loss:		2.718334E-02
  validation loss:		2.609740E-02
Epoch took 0.659s

Epoch 95 of 300
  training loss:		2.725419E-02
  validation loss:		2.584837E-02
Epoch took 0.659s

Epoch 96 of 300
  training loss:		2.741863E-02
  validation loss:		2.621607E-02
Epoch took 0.658s

Epoch 97 of 300
  training loss:		2.713040E-02
  validation loss:		2.600929E-02
Epoch took 0.660s

Epoch 98 of 300
  training loss:		2.706620E-02
  validation loss:		2.588230E-02
Epoch took 0.661s

Epoch 99 of 300
  training loss:		2.702697E-02
  validation loss:		2.582563E-02
Epoch took 0.658s

Epoch 100 of 300
  training loss:		2.683003E-02
  validation loss:		2.608424E-02
Epoch took 0.658s

Epoch 101 of 300
  training loss:		2.691587E-02
  validation loss:		2.593506E-02
Epoch took 0.658s

Epoch 102 of 300
  training loss:		2.674783E-02
  validation loss:		2.634324E-02
Epoch took 0.659s

Epoch 103 of 300
  training loss:		2.687338E-02
  validation loss:		2.584290E-02
Epoch took 0.658s

Epoch 104 of 300
  training loss:		2.691612E-02
  validation loss:		2.640813E-02
Epoch took 0.657s

Epoch 105 of 300
  training loss:		2.683542E-02
  validation loss:		2.576397E-02
Epoch took 0.659s

Epoch 106 of 300
  training loss:		2.712772E-02
  validation loss:		2.787547E-02
Epoch took 0.660s

Epoch 107 of 300
  training loss:		2.728106E-02
  validation loss:		2.627962E-02
Epoch took 0.658s

Epoch 108 of 300
  training loss:		2.731354E-02
  validation loss:		2.716083E-02
Epoch took 0.659s

Epoch 109 of 300
  training loss:		2.705789E-02
  validation loss:		2.609970E-02
Epoch took 0.657s

Epoch 110 of 300
  training loss:		2.681488E-02
  validation loss:		2.567133E-02
Epoch took 0.661s

Epoch 111 of 300
  training loss:		2.679185E-02
  validation loss:		2.559704E-02
Epoch took 0.660s

Epoch 112 of 300
  training loss:		2.672092E-02
  validation loss:		2.566607E-02
Epoch took 0.657s

Epoch 113 of 300
  training loss:		2.667768E-02
  validation loss:		2.598971E-02
Epoch took 0.659s

Epoch 114 of 300
  training loss:		2.698631E-02
  validation loss:		2.576369E-02
Epoch took 0.663s

Epoch 115 of 300
  training loss:		2.724273E-02
  validation loss:		2.578090E-02
Epoch took 0.663s

Epoch 116 of 300
  training loss:		2.688492E-02
  validation loss:		2.546572E-02
Epoch took 0.658s

Epoch 117 of 300
  training loss:		2.691872E-02
  validation loss:		2.566080E-02
Epoch took 0.659s

Epoch 118 of 300
  training loss:		2.678893E-02
  validation loss:		2.625729E-02
Epoch took 0.660s

Epoch 119 of 300
  training loss:		2.669938E-02
  validation loss:		2.580158E-02
Epoch took 0.657s

Epoch 120 of 300
  training loss:		2.676231E-02
  validation loss:		2.546011E-02
Epoch took 0.660s

Early stopping, val-loss increased over the last 20 epochs from 0.0259571344799 to 0.02604115803
Saving model from epoch 100
Training MSE: 2.57738e-14
Validation MSE: 2.49829e-14
Training R2: 0.726895028551
Validation R2: 0.734201394583
