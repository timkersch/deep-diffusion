Training network with 56340 training samples and 18780 validation samples
Epoch 1 of 300
  training loss:		6.835184E-02
  validation loss:		4.303351E-02
Epoch took 2.466s

Epoch 2 of 300
  training loss:		4.370978E-02
  validation loss:		3.782252E-02
Epoch took 2.442s

Epoch 3 of 300
  training loss:		3.694856E-02
  validation loss:		3.599440E-02
Epoch took 2.444s

Epoch 4 of 300
  training loss:		3.352289E-02
  validation loss:		3.077462E-02
Epoch took 2.442s

Epoch 5 of 300
  training loss:		3.169784E-02
  validation loss:		3.286072E-02
Epoch took 2.439s

Epoch 6 of 300
  training loss:		3.065929E-02
  validation loss:		3.044273E-02
Epoch took 2.439s

Epoch 7 of 300
  training loss:		3.019793E-02
  validation loss:		2.775852E-02
Epoch took 2.438s

Epoch 8 of 300
  training loss:		2.965081E-02
  validation loss:		2.810685E-02
Epoch took 2.439s

Epoch 9 of 300
  training loss:		2.915926E-02
  validation loss:		2.825777E-02
Epoch took 2.437s

Epoch 10 of 300
  training loss:		2.854644E-02
  validation loss:		2.776105E-02
Epoch took 2.437s

Epoch 11 of 300
  training loss:		2.841073E-02
  validation loss:		2.764894E-02
Epoch took 2.435s

Epoch 12 of 300
  training loss:		2.836031E-02
  validation loss:		2.886267E-02
Epoch took 2.432s

Epoch 13 of 300
  training loss:		2.808977E-02
  validation loss:		2.651015E-02
Epoch took 2.433s

Epoch 14 of 300
  training loss:		2.779084E-02
  validation loss:		2.678180E-02
Epoch took 2.432s

Epoch 15 of 300
  training loss:		2.808129E-02
  validation loss:		2.667878E-02
Epoch took 2.432s

Epoch 16 of 300
  training loss:		2.752957E-02
  validation loss:		2.669550E-02
Epoch took 2.433s

Epoch 17 of 300
  training loss:		2.754583E-02
  validation loss:		2.615950E-02
Epoch took 2.434s

Epoch 18 of 300
  training loss:		2.760485E-02
  validation loss:		2.667611E-02
Epoch took 2.434s

Epoch 19 of 300
  training loss:		2.777984E-02
  validation loss:		2.626429E-02
Epoch took 2.430s

Epoch 20 of 300
  training loss:		2.710068E-02
  validation loss:		2.586971E-02
Epoch took 2.431s

Epoch 21 of 300
  training loss:		2.723724E-02
  validation loss:		2.571896E-02
Epoch took 2.429s

Epoch 22 of 300
  training loss:		2.694143E-02
  validation loss:		2.577641E-02
Epoch took 2.429s

Epoch 23 of 300
  training loss:		2.736749E-02
  validation loss:		2.589342E-02
Epoch took 2.432s

Epoch 24 of 300
  training loss:		2.693557E-02
  validation loss:		2.585984E-02
Epoch took 2.436s

Epoch 25 of 300
  training loss:		2.711479E-02
  validation loss:		2.672953E-02
Epoch took 2.434s

Epoch 26 of 300
  training loss:		2.681397E-02
  validation loss:		2.617230E-02
Epoch took 2.434s

Epoch 27 of 300
  training loss:		2.699602E-02
  validation loss:		2.600742E-02
Epoch took 2.430s

Epoch 28 of 300
  training loss:		2.665179E-02
  validation loss:		2.557555E-02
Epoch took 2.433s

Epoch 29 of 300
  training loss:		2.731526E-02
  validation loss:		2.544802E-02
Epoch took 2.431s

Epoch 30 of 300
  training loss:		2.662091E-02
  validation loss:		2.559609E-02
Epoch took 2.430s

Epoch 31 of 300
  training loss:		2.673857E-02
  validation loss:		2.545515E-02
Epoch took 2.431s

Epoch 32 of 300
  training loss:		2.800917E-02
  validation loss:		2.705103E-02
Epoch took 2.431s

Epoch 33 of 300
  training loss:		2.660028E-02
  validation loss:		2.538583E-02
Epoch took 2.428s

Epoch 34 of 300
  training loss:		2.649363E-02
  validation loss:		2.554301E-02
Epoch took 2.427s

Epoch 35 of 300
  training loss:		2.710720E-02
  validation loss:		2.568439E-02
Epoch took 2.429s

Epoch 36 of 300
  training loss:		2.672604E-02
  validation loss:		2.659333E-02
Epoch took 2.430s

Epoch 37 of 300
  training loss:		2.654382E-02
  validation loss:		2.548660E-02
Epoch took 2.428s

Epoch 38 of 300
  training loss:		2.640762E-02
  validation loss:		2.607881E-02
Epoch took 2.427s

Epoch 39 of 300
  training loss:		2.688993E-02
  validation loss:		2.557817E-02
Epoch took 2.429s

Epoch 40 of 300
  training loss:		2.637724E-02
  validation loss:		2.546558E-02
Epoch took 2.429s

Epoch 41 of 300
  training loss:		2.731268E-02
  validation loss:		2.529830E-02
Epoch took 2.427s

Epoch 42 of 300
  training loss:		2.667798E-02
  validation loss:		2.654382E-02
Epoch took 2.429s

Epoch 43 of 300
  training loss:		2.645534E-02
  validation loss:		2.555516E-02
Epoch took 2.427s

Epoch 44 of 300
  training loss:		2.637540E-02
  validation loss:		2.543805E-02
Epoch took 2.428s

Epoch 45 of 300
  training loss:		2.653554E-02
  validation loss:		2.533021E-02
Epoch took 2.434s

Epoch 46 of 300
  training loss:		2.761607E-02
  validation loss:		2.539701E-02
Epoch took 2.432s

Epoch 47 of 300
  training loss:		2.657140E-02
  validation loss:		2.588512E-02
Epoch took 2.432s

Epoch 48 of 300
  training loss:		2.634546E-02
  validation loss:		2.546772E-02
Epoch took 2.431s

Epoch 49 of 300
  training loss:		2.691004E-02
  validation loss:		2.532927E-02
Epoch took 2.508s

Epoch 50 of 300
  training loss:		2.628362E-02
  validation loss:		2.544005E-02
Epoch took 2.434s

Epoch 51 of 300
  training loss:		2.677761E-02
  validation loss:		2.538237E-02
Epoch took 2.428s

Epoch 52 of 300
  training loss:		2.657298E-02
  validation loss:		2.552972E-02
Epoch took 2.430s

Epoch 53 of 300
  training loss:		2.654566E-02
  validation loss:		2.544146E-02
Epoch took 2.432s

Epoch 54 of 300
  training loss:		2.630007E-02
  validation loss:		2.547329E-02
Epoch took 2.430s

Epoch 55 of 300
  training loss:		2.631658E-02
  validation loss:		2.534636E-02
Epoch took 2.429s

Epoch 56 of 300
  training loss:		2.690564E-02
  validation loss:		2.531342E-02
Epoch took 2.427s

Epoch 57 of 300
  training loss:		2.623038E-02
  validation loss:		2.535004E-02
Epoch took 2.438s

Epoch 58 of 300
  training loss:		2.691789E-02
  validation loss:		2.531212E-02
Epoch took 2.426s

Epoch 59 of 300
  training loss:		2.623292E-02
  validation loss:		2.532652E-02
Epoch took 2.436s

Epoch 60 of 300
  training loss:		2.622290E-02
  validation loss:		2.529221E-02
Epoch took 2.429s

Epoch 61 of 300
  training loss:		2.681287E-02
  validation loss:		2.631664E-02
Epoch took 2.430s

Epoch 62 of 300
  training loss:		2.637733E-02
  validation loss:		2.530815E-02
Epoch took 2.434s

Epoch 63 of 300
  training loss:		2.658397E-02
  validation loss:		2.517947E-02
Epoch took 2.430s

Epoch 64 of 300
  training loss:		2.622780E-02
  validation loss:		2.520285E-02
Epoch took 2.433s

Epoch 65 of 300
  training loss:		2.621419E-02
  validation loss:		2.523292E-02
Epoch took 2.447s

Epoch 66 of 300
  training loss:		2.650871E-02
  validation loss:		2.527588E-02
Epoch took 2.432s

Epoch 67 of 300
  training loss:		2.696267E-02
  validation loss:		2.526781E-02
Epoch took 2.435s

Epoch 68 of 300
  training loss:		2.621343E-02
  validation loss:		2.520797E-02
Epoch took 2.430s

Epoch 69 of 300
  training loss:		2.646878E-02
  validation loss:		2.525979E-02
Epoch took 2.539s

Epoch 70 of 300
  training loss:		2.723493E-02
  validation loss:		2.526478E-02
Epoch took 2.547s

Epoch 71 of 300
  training loss:		2.626629E-02
  validation loss:		2.530548E-02
Epoch took 2.552s

Epoch 72 of 300
  training loss:		2.621346E-02
  validation loss:		2.722022E-02
Epoch took 2.542s

Epoch 73 of 300
  training loss:		2.665195E-02
  validation loss:		2.531432E-02
Epoch took 2.452s

Epoch 74 of 300
  training loss:		2.634420E-02
  validation loss:		2.545396E-02
Epoch took 2.494s

Epoch 75 of 300
  training loss:		2.654604E-02
  validation loss:		2.582854E-02
Epoch took 2.433s

Epoch 76 of 300
  training loss:		2.622774E-02
  validation loss:		2.530470E-02
Epoch took 2.429s

Epoch 77 of 300
  training loss:		2.677281E-02
  validation loss:		2.527012E-02
Epoch took 2.427s

Epoch 78 of 300
  training loss:		2.619275E-02
  validation loss:		2.532987E-02
Epoch took 2.431s

Epoch 79 of 300
  training loss:		2.674160E-02
  validation loss:		2.696789E-02
Epoch took 2.425s

Epoch 80 of 300
  training loss:		2.633631E-02
  validation loss:		2.523047E-02
Epoch took 2.433s

Early stopping, val-loss increased over the last 20 epochs from 0.0254726103538 to 0.0255370921951
Saving model from epoch 60
Training MSE: 2.51262e-14
Validation MSE: 2.43056e-14
Training R2: 0.733756703371
Validation R2: 0.741407317262
