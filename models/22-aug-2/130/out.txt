Training network with 56340 training samples and 18780 validation samples
Epoch 1 of 300
  training loss:		3.343199E-01
  validation loss:		3.199436E-01
Epoch took 0.203s

Epoch 2 of 300
  training loss:		3.153652E-01
  validation loss:		3.109417E-01
Epoch took 0.203s

Epoch 3 of 300
  training loss:		3.086307E-01
  validation loss:		3.058793E-01
Epoch took 0.202s

Epoch 4 of 300
  training loss:		3.027261E-01
  validation loss:		2.990236E-01
Epoch took 0.203s

Epoch 5 of 300
  training loss:		2.983875E-01
  validation loss:		2.946347E-01
Epoch took 0.202s

Epoch 6 of 300
  training loss:		2.933042E-01
  validation loss:		2.910779E-01
Epoch took 0.203s

Epoch 7 of 300
  training loss:		2.885437E-01
  validation loss:		2.852722E-01
Epoch took 0.202s

Epoch 8 of 300
  training loss:		2.835221E-01
  validation loss:		2.871282E-01
Epoch took 0.203s

Epoch 9 of 300
  training loss:		2.793586E-01
  validation loss:		2.749098E-01
Epoch took 0.203s

Epoch 10 of 300
  training loss:		2.743076E-01
  validation loss:		2.738475E-01
Epoch took 0.203s

Epoch 11 of 300
  training loss:		2.704204E-01
  validation loss:		2.758695E-01
Epoch took 0.203s

Epoch 12 of 300
  training loss:		2.664919E-01
  validation loss:		2.639249E-01
Epoch took 0.203s

Epoch 13 of 300
  training loss:		2.625903E-01
  validation loss:		2.580814E-01
Epoch took 0.203s

Epoch 14 of 300
  training loss:		2.595727E-01
  validation loss:		2.569597E-01
Epoch took 0.203s

Epoch 15 of 300
  training loss:		2.557343E-01
  validation loss:		2.499091E-01
Epoch took 0.203s

Epoch 16 of 300
  training loss:		2.516451E-01
  validation loss:		2.451116E-01
Epoch took 0.203s

Epoch 17 of 300
  training loss:		2.477489E-01
  validation loss:		2.444752E-01
Epoch took 0.203s

Epoch 18 of 300
  training loss:		2.425997E-01
  validation loss:		2.394120E-01
Epoch took 0.203s

Epoch 19 of 300
  training loss:		2.387802E-01
  validation loss:		2.366342E-01
Epoch took 0.203s

Epoch 20 of 300
  training loss:		2.341983E-01
  validation loss:		2.327881E-01
Epoch took 0.203s

Epoch 21 of 300
  training loss:		2.302259E-01
  validation loss:		2.218320E-01
Epoch took 0.203s

Epoch 22 of 300
  training loss:		2.248072E-01
  validation loss:		2.165423E-01
Epoch took 0.203s

Epoch 23 of 300
  training loss:		2.207192E-01
  validation loss:		2.164732E-01
Epoch took 0.203s

Epoch 24 of 300
  training loss:		2.163671E-01
  validation loss:		2.322299E-01
Epoch took 0.203s

Epoch 25 of 300
  training loss:		2.114410E-01
  validation loss:		2.119750E-01
Epoch took 0.202s

Epoch 26 of 300
  training loss:		2.085902E-01
  validation loss:		2.035530E-01
Epoch took 0.203s

Epoch 27 of 300
  training loss:		2.067051E-01
  validation loss:		2.009671E-01
Epoch took 0.203s

Epoch 28 of 300
  training loss:		2.023071E-01
  validation loss:		2.024416E-01
Epoch took 0.203s

Epoch 29 of 300
  training loss:		2.015213E-01
  validation loss:		2.186178E-01
Epoch took 0.202s

Epoch 30 of 300
  training loss:		1.972197E-01
  validation loss:		2.017526E-01
Epoch took 0.203s

Epoch 31 of 300
  training loss:		1.937778E-01
  validation loss:		2.258418E-01
Epoch took 0.202s

Epoch 32 of 300
  training loss:		1.899594E-01
  validation loss:		2.102259E-01
Epoch took 0.203s

Epoch 33 of 300
  training loss:		1.889229E-01
  validation loss:		1.815162E-01
Epoch took 0.203s

Epoch 34 of 300
  training loss:		1.839763E-01
  validation loss:		1.934895E-01
Epoch took 0.203s

Epoch 35 of 300
  training loss:		1.824263E-01
  validation loss:		1.950114E-01
Epoch took 0.203s

Epoch 36 of 300
  training loss:		1.804761E-01
  validation loss:		1.795505E-01
Epoch took 0.202s

Epoch 37 of 300
  training loss:		1.775519E-01
  validation loss:		1.742289E-01
Epoch took 0.202s

Epoch 38 of 300
  training loss:		1.744686E-01
  validation loss:		1.645734E-01
Epoch took 0.203s

Epoch 39 of 300
  training loss:		1.715976E-01
  validation loss:		2.024601E-01
Epoch took 0.203s

Epoch 40 of 300
  training loss:		1.684348E-01
  validation loss:		1.813222E-01
Epoch took 0.203s

Epoch 41 of 300
  training loss:		1.673601E-01
  validation loss:		1.812333E-01
Epoch took 0.203s

Epoch 42 of 300
  training loss:		1.653755E-01
  validation loss:		1.653922E-01
Epoch took 0.202s

Epoch 43 of 300
  training loss:		1.628492E-01
  validation loss:		1.545559E-01
Epoch took 0.203s

Epoch 44 of 300
  training loss:		1.604320E-01
  validation loss:		1.535634E-01
Epoch took 0.203s

Epoch 45 of 300
  training loss:		1.580142E-01
  validation loss:		1.560749E-01
Epoch took 0.203s

Epoch 46 of 300
  training loss:		1.548536E-01
  validation loss:		1.460155E-01
Epoch took 0.203s

Epoch 47 of 300
  training loss:		1.539689E-01
  validation loss:		1.475829E-01
Epoch took 0.203s

Epoch 48 of 300
  training loss:		1.521309E-01
  validation loss:		1.447363E-01
Epoch took 0.203s

Epoch 49 of 300
  training loss:		1.488158E-01
  validation loss:		1.849825E-01
Epoch took 0.202s

Epoch 50 of 300
  training loss:		1.476674E-01
  validation loss:		1.432794E-01
Epoch took 0.203s

Epoch 51 of 300
  training loss:		1.449193E-01
  validation loss:		1.419152E-01
Epoch took 0.202s

Epoch 52 of 300
  training loss:		1.442181E-01
  validation loss:		1.408066E-01
Epoch took 0.203s

Epoch 53 of 300
  training loss:		1.424587E-01
  validation loss:		1.495793E-01
Epoch took 0.203s

Epoch 54 of 300
  training loss:		1.418764E-01
  validation loss:		1.428218E-01
Epoch took 0.203s

Epoch 55 of 300
  training loss:		1.394258E-01
  validation loss:		1.386000E-01
Epoch took 0.203s

Epoch 56 of 300
  training loss:		1.385579E-01
  validation loss:		1.378861E-01
Epoch took 0.202s

Epoch 57 of 300
  training loss:		1.362202E-01
  validation loss:		1.328107E-01
Epoch took 0.203s

Epoch 58 of 300
  training loss:		1.355361E-01
  validation loss:		1.328383E-01
Epoch took 0.203s

Epoch 59 of 300
  training loss:		1.344785E-01
  validation loss:		1.373938E-01
Epoch took 0.203s

Epoch 60 of 300
  training loss:		1.317890E-01
  validation loss:		1.373944E-01
Epoch took 0.203s

Epoch 61 of 300
  training loss:		1.310537E-01
  validation loss:		1.305661E-01
Epoch took 0.203s

Epoch 62 of 300
  training loss:		1.304819E-01
  validation loss:		1.286045E-01
Epoch took 0.203s

Epoch 63 of 300
  training loss:		1.295643E-01
  validation loss:		1.259995E-01
Epoch took 0.203s

Epoch 64 of 300
  training loss:		1.279458E-01
  validation loss:		1.285082E-01
Epoch took 0.203s

Epoch 65 of 300
  training loss:		1.275261E-01
  validation loss:		1.221107E-01
Epoch took 0.203s

Epoch 66 of 300
  training loss:		1.261204E-01
  validation loss:		1.322450E-01
Epoch took 0.203s

Epoch 67 of 300
  training loss:		1.244455E-01
  validation loss:		1.224259E-01
Epoch took 0.202s

Epoch 68 of 300
  training loss:		1.259040E-01
  validation loss:		1.223755E-01
Epoch took 0.203s

Epoch 69 of 300
  training loss:		1.233140E-01
  validation loss:		1.234689E-01
Epoch took 0.203s

Epoch 70 of 300
  training loss:		1.220040E-01
  validation loss:		1.192091E-01
Epoch took 0.203s

Epoch 71 of 300
  training loss:		1.223125E-01
  validation loss:		1.238850E-01
Epoch took 0.203s

Epoch 72 of 300
  training loss:		1.217753E-01
  validation loss:		1.165916E-01
Epoch took 0.202s

Epoch 73 of 300
  training loss:		1.204805E-01
  validation loss:		1.181011E-01
Epoch took 0.203s

Epoch 74 of 300
  training loss:		1.205869E-01
  validation loss:		1.204070E-01
Epoch took 0.203s

Epoch 75 of 300
  training loss:		1.190680E-01
  validation loss:		1.172963E-01
Epoch took 0.203s

Epoch 76 of 300
  training loss:		1.190370E-01
  validation loss:		1.179348E-01
Epoch took 0.203s

Epoch 77 of 300
  training loss:		1.177785E-01
  validation loss:		1.191025E-01
Epoch took 0.203s

Epoch 78 of 300
  training loss:		1.177417E-01
  validation loss:		1.234408E-01
Epoch took 0.203s

Epoch 79 of 300
  training loss:		1.179531E-01
  validation loss:		1.211891E-01
Epoch took 0.203s

Epoch 80 of 300
  training loss:		1.166614E-01
  validation loss:		1.150741E-01
Epoch took 0.202s

Epoch 81 of 300
  training loss:		1.171137E-01
  validation loss:		1.256394E-01
Epoch took 0.202s

Epoch 82 of 300
  training loss:		1.158725E-01
  validation loss:		1.190950E-01
Epoch took 0.202s

Epoch 83 of 300
  training loss:		1.161302E-01
  validation loss:		1.239285E-01
Epoch took 0.203s

Epoch 84 of 300
  training loss:		1.146665E-01
  validation loss:		1.127897E-01
Epoch took 0.203s

Epoch 85 of 300
  training loss:		1.151829E-01
  validation loss:		1.151517E-01
Epoch took 0.203s

Epoch 86 of 300
  training loss:		1.143515E-01
  validation loss:		1.166306E-01
Epoch took 0.203s

Epoch 87 of 300
  training loss:		1.152669E-01
  validation loss:		1.157179E-01
Epoch took 0.203s

Epoch 88 of 300
  training loss:		1.139135E-01
  validation loss:		1.164513E-01
Epoch took 0.202s

Epoch 89 of 300
  training loss:		1.148320E-01
  validation loss:		1.147733E-01
Epoch took 0.202s

Epoch 90 of 300
  training loss:		1.129070E-01
  validation loss:		1.170663E-01
Epoch took 0.203s

Epoch 91 of 300
  training loss:		1.131008E-01
  validation loss:		1.126757E-01
Epoch took 0.203s

Epoch 92 of 300
  training loss:		1.130758E-01
  validation loss:		1.108478E-01
Epoch took 0.203s

Epoch 93 of 300
  training loss:		1.125905E-01
  validation loss:		1.139161E-01
Epoch took 0.203s

Epoch 94 of 300
  training loss:		1.137437E-01
  validation loss:		1.107499E-01
Epoch took 0.203s

Epoch 95 of 300
  training loss:		1.135917E-01
  validation loss:		1.116261E-01
Epoch took 0.202s

Epoch 96 of 300
  training loss:		1.127422E-01
  validation loss:		1.107408E-01
Epoch took 0.203s

Epoch 97 of 300
  training loss:		1.125852E-01
  validation loss:		1.143005E-01
Epoch took 0.203s

Epoch 98 of 300
  training loss:		1.128801E-01
  validation loss:		1.118395E-01
Epoch took 0.203s

Epoch 99 of 300
  training loss:		1.117695E-01
  validation loss:		1.108098E-01
Epoch took 0.203s

Epoch 100 of 300
  training loss:		1.124262E-01
  validation loss:		1.118537E-01
Epoch took 0.203s

Epoch 101 of 300
  training loss:		1.111394E-01
  validation loss:		1.138388E-01
Epoch took 0.203s

Epoch 102 of 300
  training loss:		1.114071E-01
  validation loss:		1.118686E-01
Epoch took 0.203s

Epoch 103 of 300
  training loss:		1.121200E-01
  validation loss:		1.130644E-01
Epoch took 0.203s

Epoch 104 of 300
  training loss:		1.111719E-01
  validation loss:		1.158201E-01
Epoch took 0.203s

Epoch 105 of 300
  training loss:		1.108653E-01
  validation loss:		1.112101E-01
Epoch took 0.203s

Epoch 106 of 300
  training loss:		1.114154E-01
  validation loss:		1.124147E-01
Epoch took 0.203s

Epoch 107 of 300
  training loss:		1.108301E-01
  validation loss:		1.112115E-01
Epoch took 0.203s

Epoch 108 of 300
  training loss:		1.110457E-01
  validation loss:		1.154011E-01
Epoch took 0.203s

Epoch 109 of 300
  training loss:		1.109521E-01
  validation loss:		1.167302E-01
Epoch took 0.203s

Epoch 110 of 300
  training loss:		1.102328E-01
  validation loss:		1.102983E-01
Epoch took 0.203s

Epoch 111 of 300
  training loss:		1.107615E-01
  validation loss:		1.103337E-01
Epoch took 0.203s

Epoch 112 of 300
  training loss:		1.096373E-01
  validation loss:		1.149445E-01
Epoch took 0.203s

Epoch 113 of 300
  training loss:		1.103222E-01
  validation loss:		1.097501E-01
Epoch took 0.202s

Epoch 114 of 300
  training loss:		1.104123E-01
  validation loss:		1.092227E-01
Epoch took 0.203s

Epoch 115 of 300
  training loss:		1.102282E-01
  validation loss:		1.118651E-01
Epoch took 0.202s

Epoch 116 of 300
  training loss:		1.100522E-01
  validation loss:		1.110124E-01
Epoch took 0.203s

Epoch 117 of 300
  training loss:		1.099947E-01
  validation loss:		1.094630E-01
Epoch took 0.203s

Epoch 118 of 300
  training loss:		1.102679E-01
  validation loss:		1.099199E-01
Epoch took 0.202s

Epoch 119 of 300
  training loss:		1.094710E-01
  validation loss:		1.119216E-01
Epoch took 0.203s

Epoch 120 of 300
  training loss:		1.098823E-01
  validation loss:		1.072935E-01
Epoch took 0.203s

Epoch 121 of 300
  training loss:		1.095273E-01
  validation loss:		1.100754E-01
Epoch took 0.202s

Epoch 122 of 300
  training loss:		1.106358E-01
  validation loss:		1.152639E-01
Epoch took 0.202s

Epoch 123 of 300
  training loss:		1.101208E-01
  validation loss:		1.101852E-01
Epoch took 0.202s

Epoch 124 of 300
  training loss:		1.094470E-01
  validation loss:		1.108548E-01
Epoch took 0.203s

Epoch 125 of 300
  training loss:		1.100549E-01
  validation loss:		1.087302E-01
Epoch took 0.203s

Epoch 126 of 300
  training loss:		1.090938E-01
  validation loss:		1.054812E-01
Epoch took 0.202s

Epoch 127 of 300
  training loss:		1.088482E-01
  validation loss:		1.109825E-01
Epoch took 0.203s

Epoch 128 of 300
  training loss:		1.090982E-01
  validation loss:		1.087330E-01
Epoch took 0.202s

Epoch 129 of 300
  training loss:		1.085246E-01
  validation loss:		1.071033E-01
Epoch took 0.203s

Epoch 130 of 300
  training loss:		1.094448E-01
  validation loss:		1.074309E-01
Epoch took 0.203s

Epoch 131 of 300
  training loss:		1.092793E-01
  validation loss:		1.086925E-01
Epoch took 0.202s

Epoch 132 of 300
  training loss:		1.084459E-01
  validation loss:		1.114921E-01
Epoch took 0.203s

Epoch 133 of 300
  training loss:		1.090063E-01
  validation loss:		1.082725E-01
Epoch took 0.202s

Epoch 134 of 300
  training loss:		1.087737E-01
  validation loss:		1.067480E-01
Epoch took 0.203s

Epoch 135 of 300
  training loss:		1.086585E-01
  validation loss:		1.113544E-01
Epoch took 0.202s

Epoch 136 of 300
  training loss:		1.087480E-01
  validation loss:		1.121271E-01
Epoch took 0.203s

Epoch 137 of 300
  training loss:		1.084001E-01
  validation loss:		1.112803E-01
Epoch took 0.203s

Epoch 138 of 300
  training loss:		1.087051E-01
  validation loss:		1.108128E-01
Epoch took 0.203s

Epoch 139 of 300
  training loss:		1.093461E-01
  validation loss:		1.109505E-01
Epoch took 0.203s

Epoch 140 of 300
  training loss:		1.093699E-01
  validation loss:		1.118541E-01
Epoch took 0.202s

Epoch 141 of 300
  training loss:		1.085238E-01
  validation loss:		1.091136E-01
Epoch took 0.204s

Epoch 142 of 300
  training loss:		1.084136E-01
  validation loss:		1.083204E-01
Epoch took 0.205s

Epoch 143 of 300
  training loss:		1.084742E-01
  validation loss:		1.118128E-01
Epoch took 0.203s

Epoch 144 of 300
  training loss:		1.082298E-01
  validation loss:		1.079884E-01
Epoch took 0.203s

Epoch 145 of 300
  training loss:		1.082584E-01
  validation loss:		1.053622E-01
Epoch took 0.203s

Epoch 146 of 300
  training loss:		1.077746E-01
  validation loss:		1.100412E-01
Epoch took 0.202s

Epoch 147 of 300
  training loss:		1.086625E-01
  validation loss:		1.076139E-01
Epoch took 0.202s

Epoch 148 of 300
  training loss:		1.081529E-01
  validation loss:		1.121729E-01
Epoch took 0.205s

Epoch 149 of 300
  training loss:		1.083457E-01
  validation loss:		1.085868E-01
Epoch took 0.203s

Epoch 150 of 300
  training loss:		1.079250E-01
  validation loss:		1.116133E-01
Epoch took 0.202s

Epoch 151 of 300
  training loss:		1.082634E-01
  validation loss:		1.107957E-01
Epoch took 0.203s

Epoch 152 of 300
  training loss:		1.080411E-01
  validation loss:		1.087864E-01
Epoch took 0.202s

Epoch 153 of 300
  training loss:		1.074559E-01
  validation loss:		1.085475E-01
Epoch took 0.202s

Epoch 154 of 300
  training loss:		1.081848E-01
  validation loss:		1.094730E-01
Epoch took 0.203s

Epoch 155 of 300
  training loss:		1.078680E-01
  validation loss:		1.072475E-01
Epoch took 0.202s

Epoch 156 of 300
  training loss:		1.085304E-01
  validation loss:		1.098449E-01
Epoch took 0.202s

Epoch 157 of 300
  training loss:		1.081276E-01
  validation loss:		1.065835E-01
Epoch took 0.203s

Epoch 158 of 300
  training loss:		1.082354E-01
  validation loss:		1.125545E-01
Epoch took 0.202s

Epoch 159 of 300
  training loss:		1.081389E-01
  validation loss:		1.106761E-01
Epoch took 0.202s

Epoch 160 of 300
  training loss:		1.079714E-01
  validation loss:		1.076433E-01
Epoch took 0.202s

Epoch 161 of 300
  training loss:		1.079752E-01
  validation loss:		1.060770E-01
Epoch took 0.202s

Epoch 162 of 300
  training loss:		1.078397E-01
  validation loss:		1.083700E-01
Epoch took 0.203s

Epoch 163 of 300
  training loss:		1.076722E-01
  validation loss:		1.076300E-01
Epoch took 0.203s

Epoch 164 of 300
  training loss:		1.077851E-01
  validation loss:		1.063518E-01
Epoch took 0.203s

Epoch 165 of 300
  training loss:		1.074102E-01
  validation loss:		1.068438E-01
Epoch took 0.202s

Epoch 166 of 300
  training loss:		1.079142E-01
  validation loss:		1.085884E-01
Epoch took 0.202s

Epoch 167 of 300
  training loss:		1.071984E-01
  validation loss:		1.063776E-01
Epoch took 0.203s

Epoch 168 of 300
  training loss:		1.073164E-01
  validation loss:		1.097217E-01
Epoch took 0.202s

Epoch 169 of 300
  training loss:		1.078486E-01
  validation loss:		1.070332E-01
Epoch took 0.203s

Epoch 170 of 300
  training loss:		1.068799E-01
  validation loss:		1.082960E-01
Epoch took 0.203s

Epoch 171 of 300
  training loss:		1.080525E-01
  validation loss:		1.095431E-01
Epoch took 0.202s

Epoch 172 of 300
  training loss:		1.077217E-01
  validation loss:		1.085962E-01
Epoch took 0.202s

Epoch 173 of 300
  training loss:		1.078059E-01
  validation loss:		1.109231E-01
Epoch took 0.203s

Epoch 174 of 300
  training loss:		1.082765E-01
  validation loss:		1.084447E-01
Epoch took 0.202s

Epoch 175 of 300
  training loss:		1.075070E-01
  validation loss:		1.092222E-01
Epoch took 0.202s

Epoch 176 of 300
  training loss:		1.075650E-01
  validation loss:		1.079223E-01
Epoch took 0.202s

Epoch 177 of 300
  training loss:		1.075989E-01
  validation loss:		1.084361E-01
Epoch took 0.202s

Epoch 178 of 300
  training loss:		1.070617E-01
  validation loss:		1.082673E-01
Epoch took 0.203s

Epoch 179 of 300
  training loss:		1.078963E-01
  validation loss:		1.088580E-01
Epoch took 0.202s

Epoch 180 of 300
  training loss:		1.067421E-01
  validation loss:		1.061371E-01
Epoch took 0.202s

Epoch 181 of 300
  training loss:		1.079487E-01
  validation loss:		1.110798E-01
Epoch took 0.202s

Epoch 182 of 300
  training loss:		1.072387E-01
  validation loss:		1.084363E-01
Epoch took 0.203s

Epoch 183 of 300
  training loss:		1.073583E-01
  validation loss:		1.114442E-01
Epoch took 0.203s

Epoch 184 of 300
  training loss:		1.073409E-01
  validation loss:		1.102171E-01
Epoch took 0.202s

Epoch 185 of 300
  training loss:		1.071439E-01
  validation loss:		1.082003E-01
Epoch took 0.202s

Epoch 186 of 300
  training loss:		1.071686E-01
  validation loss:		1.066069E-01
Epoch took 0.202s

Epoch 187 of 300
  training loss:		1.071779E-01
  validation loss:		1.065895E-01
Epoch took 0.202s

Epoch 188 of 300
  training loss:		1.071477E-01
  validation loss:		1.112585E-01
Epoch took 0.202s

Epoch 189 of 300
  training loss:		1.078585E-01
  validation loss:		1.083409E-01
Epoch took 0.202s

Epoch 190 of 300
  training loss:		1.078301E-01
  validation loss:		1.063549E-01
Epoch took 0.202s

Epoch 191 of 300
  training loss:		1.070668E-01
  validation loss:		1.100671E-01
Epoch took 0.203s

Epoch 192 of 300
  training loss:		1.070956E-01
  validation loss:		1.098479E-01
Epoch took 0.202s

Epoch 193 of 300
  training loss:		1.075339E-01
  validation loss:		1.064684E-01
Epoch took 0.203s

Epoch 194 of 300
  training loss:		1.075515E-01
  validation loss:		1.063897E-01
Epoch took 0.202s

Epoch 195 of 300
  training loss:		1.069474E-01
  validation loss:		1.075916E-01
Epoch took 0.202s

Epoch 196 of 300
  training loss:		1.068274E-01
  validation loss:		1.063816E-01
Epoch took 0.203s

Epoch 197 of 300
  training loss:		1.070656E-01
  validation loss:		1.066867E-01
Epoch took 0.202s

Epoch 198 of 300
  training loss:		1.075189E-01
  validation loss:		1.103871E-01
Epoch took 0.203s

Epoch 199 of 300
  training loss:		1.066114E-01
  validation loss:		1.070628E-01
Epoch took 0.203s

Epoch 200 of 300
  training loss:		1.070846E-01
  validation loss:		1.066523E-01
Epoch took 0.202s

Early stopping, val-loss increased over the last 20 epochs from 0.108081970291 to 0.10830316927
Saving model from epoch 180
Training MSE: 2.53962e-14
Validation MSE: 2.5491e-14
Training R2: 0.730735828082
Validation R2: 0.727861524895
