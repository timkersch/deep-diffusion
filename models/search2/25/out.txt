Epoch 1 of 500
  training loss:		7.374044E-03
  validation loss:		1.087300E-03

Epoch 2 of 500
  training loss:		9.790306E-04
  validation loss:		4.018421E-04

Epoch 3 of 500
  training loss:		1.768628E-04
  validation loss:		6.038828E-05

Epoch 4 of 500
  training loss:		6.243810E-05
  validation loss:		3.955095E-05

Epoch 5 of 500
  training loss:		4.858319E-05
  validation loss:		2.443832E-05

Epoch 6 of 500
  training loss:		4.926844E-05
  validation loss:		3.191270E-05

Epoch 7 of 500
  training loss:		3.863982E-05
  validation loss:		1.132047E-04

Epoch 8 of 500
  training loss:		3.709220E-05
  validation loss:		2.312595E-05

Epoch 9 of 500
  training loss:		3.147848E-05
  validation loss:		7.747552E-05

Epoch 10 of 500
  training loss:		4.286607E-05
  validation loss:		9.493111E-06

Epoch 11 of 500
  training loss:		2.473907E-05
  validation loss:		2.661396E-05

Epoch 12 of 500
  training loss:		2.956179E-05
  validation loss:		8.989200E-06

Epoch 13 of 500
  training loss:		2.928254E-05
  validation loss:		8.472448E-06

Epoch 14 of 500
  training loss:		2.334565E-05
  validation loss:		7.970736E-06

Epoch 15 of 500
  training loss:		2.449993E-05
  validation loss:		2.654399E-05

Epoch 16 of 500
  training loss:		2.326850E-05
  validation loss:		6.214641E-05

Epoch 17 of 500
  training loss:		2.378788E-05
  validation loss:		5.924694E-06

Epoch 18 of 500
  training loss:		2.363908E-05
  validation loss:		1.292037E-04

Epoch 19 of 500
  training loss:		2.301495E-05
  validation loss:		6.388693E-06

Epoch 20 of 500
  training loss:		2.183874E-05
  validation loss:		3.605660E-06

Epoch 21 of 500
  training loss:		2.157699E-05
  validation loss:		1.795739E-05

Epoch 22 of 500
  training loss:		2.592821E-05
  validation loss:		2.990062E-05

Epoch 23 of 500
  training loss:		2.083651E-05
  validation loss:		7.484446E-06

Epoch 24 of 500
  training loss:		1.935148E-05
  validation loss:		9.921689E-06

Epoch 25 of 500
  training loss:		1.976339E-05
  validation loss:		1.606739E-05

Epoch 26 of 500
  training loss:		2.193233E-05
  validation loss:		1.104042E-05

Epoch 27 of 500
  training loss:		1.861052E-05
  validation loss:		1.178600E-05

Epoch 28 of 500
  training loss:		2.181261E-05
  validation loss:		3.781629E-05

Epoch 29 of 500
  training loss:		2.031081E-05
  validation loss:		1.629990E-05

Epoch 30 of 500
  training loss:		2.436540E-05
  validation loss:		2.697760E-06

Epoch 31 of 500
  training loss:		2.011880E-05
  validation loss:		5.665560E-06

Epoch 32 of 500
  training loss:		1.786340E-05
  validation loss:		4.592690E-06

Epoch 33 of 500
  training loss:		1.830869E-05
  validation loss:		2.371549E-06

Epoch 34 of 500
  training loss:		2.387788E-05
  validation loss:		2.219529E-05

Epoch 35 of 500
  training loss:		1.768136E-05
  validation loss:		1.483696E-05

Epoch 36 of 500
  training loss:		1.855581E-05
  validation loss:		7.784678E-06

Epoch 37 of 500
  training loss:		1.975471E-05
  validation loss:		5.541385E-05

Epoch 38 of 500
  training loss:		1.904512E-05
  validation loss:		3.672685E-05

Epoch 39 of 500
  training loss:		1.867418E-05
  validation loss:		5.414473E-06

Epoch 40 of 500
  training loss:		1.856180E-05
  validation loss:		1.559237E-05

Epoch 41 of 500
  training loss:		1.772749E-05
  validation loss:		2.905460E-06

Epoch 42 of 500
  training loss:		1.835827E-05
  validation loss:		8.972277E-06

Epoch 43 of 500
  training loss:		1.882472E-05
  validation loss:		2.358853E-05

Epoch 44 of 500
  training loss:		1.697625E-05
  validation loss:		1.239205E-05

Epoch 45 of 500
  training loss:		1.585738E-05
  validation loss:		1.646811E-05

Epoch 46 of 500
  training loss:		2.184384E-05
  validation loss:		3.045536E-06

Epoch 47 of 500
  training loss:		1.482481E-05
  validation loss:		1.814257E-05

Epoch 48 of 500
  training loss:		1.812711E-05
  validation loss:		9.649106E-05

Epoch 49 of 500
  training loss:		1.782824E-05
  validation loss:		1.775024E-05

Epoch 50 of 500
  training loss:		2.019507E-05
  validation loss:		1.615047E-06

Epoch 51 of 500
  training loss:		1.642773E-05
  validation loss:		1.882209E-06

Epoch 52 of 500
  training loss:		1.870765E-05
  validation loss:		4.772028E-06

Epoch 53 of 500
  training loss:		1.448210E-05
  validation loss:		2.044912E-05

Epoch 54 of 500
  training loss:		1.985067E-05
  validation loss:		3.426239E-05

Epoch 55 of 500
  training loss:		1.646559E-05
  validation loss:		3.134811E-06

Epoch 56 of 500
  training loss:		1.875299E-05
  validation loss:		2.622393E-06

Epoch 57 of 500
  training loss:		1.597047E-05
  validation loss:		8.569087E-06

Epoch 58 of 500
  training loss:		1.863497E-05
  validation loss:		1.454183E-06

Epoch 59 of 500
  training loss:		1.562633E-05
  validation loss:		1.188856E-04

Epoch 60 of 500
  training loss:		1.842001E-05
  validation loss:		4.606789E-06

Early stopping, val-loss increased over the last 15 epochs from 0.00552846711951 to 0.00794680916443
Training RMSE: 1.0734639372e-08
Validation RMSE: 1.06870293826e-08
