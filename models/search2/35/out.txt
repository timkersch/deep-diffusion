Epoch 1 of 500
  training loss:		1.080667E-01
  validation loss:		1.302760E-02

Epoch 2 of 500
  training loss:		8.555714E-03
  validation loss:		6.858808E-03

Epoch 3 of 500
  training loss:		6.221955E-03
  validation loss:		5.462540E-03

Epoch 4 of 500
  training loss:		4.848241E-03
  validation loss:		4.204640E-03

Epoch 5 of 500
  training loss:		3.673869E-03
  validation loss:		3.101635E-03

Epoch 6 of 500
  training loss:		2.818348E-03
  validation loss:		2.520203E-03

Epoch 7 of 500
  training loss:		2.129560E-03
  validation loss:		1.645161E-03

Epoch 8 of 500
  training loss:		1.254651E-03
  validation loss:		9.798068E-04

Epoch 9 of 500
  training loss:		7.712221E-04
  validation loss:		6.467052E-04

Epoch 10 of 500
  training loss:		5.136240E-04
  validation loss:		4.279469E-04

Epoch 11 of 500
  training loss:		3.687249E-04
  validation loss:		3.105704E-04

Epoch 12 of 500
  training loss:		2.803246E-04
  validation loss:		2.749974E-04

Epoch 13 of 500
  training loss:		2.171549E-04
  validation loss:		1.768510E-04

Epoch 14 of 500
  training loss:		1.830888E-04
  validation loss:		1.984942E-04

Epoch 15 of 500
  training loss:		1.446486E-04
  validation loss:		1.159186E-04

Epoch 16 of 500
  training loss:		1.362229E-04
  validation loss:		9.994382E-05

Epoch 17 of 500
  training loss:		1.082772E-04
  validation loss:		9.687487E-05

Epoch 18 of 500
  training loss:		1.151947E-04
  validation loss:		7.698201E-05

Epoch 19 of 500
  training loss:		1.342641E-04
  validation loss:		1.528695E-04

Epoch 20 of 500
  training loss:		9.422313E-05
  validation loss:		7.013231E-05

Epoch 21 of 500
  training loss:		1.083874E-04
  validation loss:		7.012737E-05

Epoch 22 of 500
  training loss:		1.007088E-04
  validation loss:		6.415300E-05

Epoch 23 of 500
  training loss:		1.008489E-04
  validation loss:		8.646855E-05

Epoch 24 of 500
  training loss:		9.390341E-05
  validation loss:		4.289117E-05

Epoch 25 of 500
  training loss:		1.031445E-04
  validation loss:		6.143677E-05

Epoch 26 of 500
  training loss:		8.069970E-05
  validation loss:		4.374824E-05

Epoch 27 of 500
  training loss:		9.402818E-05
  validation loss:		6.728879E-05

Epoch 28 of 500
  training loss:		7.293618E-05
  validation loss:		3.475123E-05

Epoch 29 of 500
  training loss:		9.629225E-05
  validation loss:		1.785244E-04

Epoch 30 of 500
  training loss:		6.077011E-05
  validation loss:		3.085055E-05

Epoch 31 of 500
  training loss:		8.379610E-05
  validation loss:		1.579689E-04

Epoch 32 of 500
  training loss:		7.557400E-05
  validation loss:		8.816656E-05

Epoch 33 of 500
  training loss:		7.545649E-05
  validation loss:		2.948005E-04

Epoch 34 of 500
  training loss:		7.226731E-05
  validation loss:		2.476914E-05

Epoch 35 of 500
  training loss:		8.641740E-05
  validation loss:		2.492017E-05

Epoch 36 of 500
  training loss:		5.831091E-05
  validation loss:		2.080886E-05

Epoch 37 of 500
  training loss:		9.015579E-05
  validation loss:		9.772429E-05

Epoch 38 of 500
  training loss:		5.847376E-05
  validation loss:		4.027245E-05

Epoch 39 of 500
  training loss:		5.815108E-05
  validation loss:		2.130694E-05

Epoch 40 of 500
  training loss:		6.551584E-05
  validation loss:		1.184517E-04

Epoch 41 of 500
  training loss:		6.876065E-05
  validation loss:		1.889495E-05

Epoch 42 of 500
  training loss:		6.936310E-05
  validation loss:		2.480544E-05

Epoch 43 of 500
  training loss:		6.039257E-05
  validation loss:		7.456165E-05

Epoch 44 of 500
  training loss:		7.096391E-05
  validation loss:		6.256481E-05

Epoch 45 of 500
  training loss:		5.837708E-05
  validation loss:		6.715294E-05

Epoch 46 of 500
  training loss:		5.650767E-05
  validation loss:		9.967840E-05

Epoch 47 of 500
  training loss:		6.307316E-05
  validation loss:		3.490370E-05

Epoch 48 of 500
  training loss:		5.833936E-05
  validation loss:		2.186040E-05

Epoch 49 of 500
  training loss:		6.783478E-05
  validation loss:		2.450934E-05

Epoch 50 of 500
  training loss:		5.757961E-05
  validation loss:		2.812112E-04

Epoch 51 of 500
  training loss:		5.525875E-05
  validation loss:		4.988317E-05

Epoch 52 of 500
  training loss:		6.556033E-05
  validation loss:		8.450644E-05

Epoch 53 of 500
  training loss:		5.790826E-05
  validation loss:		1.712440E-05

Epoch 54 of 500
  training loss:		5.406607E-05
  validation loss:		1.151925E-05

Epoch 55 of 500
  training loss:		6.006292E-05
  validation loss:		2.453086E-04

Epoch 56 of 500
  training loss:		5.353716E-05
  validation loss:		2.101950E-05

Epoch 57 of 500
  training loss:		5.815193E-05
  validation loss:		2.181362E-04

Epoch 58 of 500
  training loss:		4.950947E-05
  validation loss:		1.978536E-05

Epoch 59 of 500
  training loss:		6.435951E-05
  validation loss:		1.051671E-05

Epoch 60 of 500
  training loss:		4.685229E-05
  validation loss:		3.142157E-04

Early stopping, val-loss increased over the last 15 epochs from 0.00667139317039 to 0.00853117902733
Training RMSE: 3.07111148983e-09
Validation RMSE: 3.17764839101e-09
