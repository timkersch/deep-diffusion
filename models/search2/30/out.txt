Epoch 1 of 500
  training loss:		1.254549E-02
  validation loss:		1.856214E-03

Epoch 2 of 500
  training loss:		9.288104E-04
  validation loss:		2.597544E-04

Epoch 3 of 500
  training loss:		1.720173E-04
  validation loss:		1.308926E-04

Epoch 4 of 500
  training loss:		1.022558E-04
  validation loss:		7.868308E-05

Epoch 5 of 500
  training loss:		7.187775E-05
  validation loss:		5.996371E-05

Epoch 6 of 500
  training loss:		6.180565E-05
  validation loss:		4.308329E-05

Epoch 7 of 500
  training loss:		5.108471E-05
  validation loss:		3.355891E-05

Epoch 8 of 500
  training loss:		3.710578E-05
  validation loss:		2.266998E-05

Epoch 9 of 500
  training loss:		3.678946E-05
  validation loss:		3.402290E-05

Epoch 10 of 500
  training loss:		3.322857E-05
  validation loss:		1.697906E-05

Epoch 11 of 500
  training loss:		2.881194E-05
  validation loss:		1.985109E-05

Epoch 12 of 500
  training loss:		3.057728E-05
  validation loss:		9.987142E-06

Epoch 13 of 500
  training loss:		2.922985E-05
  validation loss:		1.027669E-05

Epoch 14 of 500
  training loss:		2.534605E-05
  validation loss:		1.046122E-05

Epoch 15 of 500
  training loss:		2.430362E-05
  validation loss:		4.988096E-05

Epoch 16 of 500
  training loss:		2.478989E-05
  validation loss:		1.545364E-05

Epoch 17 of 500
  training loss:		2.140899E-05
  validation loss:		1.407263E-05

Epoch 18 of 500
  training loss:		2.590793E-05
  validation loss:		1.702251E-05

Epoch 19 of 500
  training loss:		2.166041E-05
  validation loss:		2.338627E-05

Epoch 20 of 500
  training loss:		2.008629E-05
  validation loss:		7.201778E-05

Epoch 21 of 500
  training loss:		2.831897E-05
  validation loss:		1.495472E-05

Epoch 22 of 500
  training loss:		1.897153E-05
  validation loss:		1.022858E-05

Epoch 23 of 500
  training loss:		2.290499E-05
  validation loss:		4.734654E-05

Epoch 24 of 500
  training loss:		1.350149E-05
  validation loss:		4.043727E-05

Epoch 25 of 500
  training loss:		2.231457E-05
  validation loss:		1.147664E-05

Epoch 26 of 500
  training loss:		1.890017E-05
  validation loss:		1.034753E-05

Epoch 27 of 500
  training loss:		1.430878E-05
  validation loss:		5.798681E-06

Epoch 28 of 500
  training loss:		1.916724E-05
  validation loss:		5.940242E-05

Epoch 29 of 500
  training loss:		2.173552E-05
  validation loss:		8.370214E-06

Epoch 30 of 500
  training loss:		1.525107E-05
  validation loss:		3.981200E-06

Epoch 31 of 500
  training loss:		1.521861E-05
  validation loss:		1.263570E-05

Epoch 32 of 500
  training loss:		1.646628E-05
  validation loss:		1.390160E-05

Epoch 33 of 500
  training loss:		1.671471E-05
  validation loss:		2.817543E-06

Epoch 34 of 500
  training loss:		1.692599E-05
  validation loss:		2.781326E-06

Epoch 35 of 500
  training loss:		1.386792E-05
  validation loss:		2.556922E-06

Epoch 36 of 500
  training loss:		2.084473E-05
  validation loss:		1.494861E-05

Epoch 37 of 500
  training loss:		1.620353E-05
  validation loss:		2.566048E-06

Epoch 38 of 500
  training loss:		1.452375E-05
  validation loss:		1.870879E-05

Epoch 39 of 500
  training loss:		1.261492E-05
  validation loss:		4.026053E-06

Epoch 40 of 500
  training loss:		1.371096E-05
  validation loss:		3.649198E-05

Epoch 41 of 500
  training loss:		1.514939E-05
  validation loss:		1.828271E-06

Epoch 42 of 500
  training loss:		1.335718E-05
  validation loss:		5.654005E-06

Epoch 43 of 500
  training loss:		3.071490E-05
  validation loss:		2.020749E-06

Epoch 44 of 500
  training loss:		1.494332E-05
  validation loss:		2.133946E-06

Epoch 45 of 500
  training loss:		5.921494E-06
  validation loss:		1.568026E-05

Epoch 46 of 500
  training loss:		1.874828E-05
  validation loss:		5.342166E-06

Epoch 47 of 500
  training loss:		2.831398E-05
  validation loss:		2.111494E-06

Epoch 48 of 500
  training loss:		5.598020E-06
  validation loss:		1.416927E-06

Epoch 49 of 500
  training loss:		1.139960E-05
  validation loss:		1.903653E-06

Epoch 50 of 500
  training loss:		1.277606E-05
  validation loss:		1.593849E-06

Epoch 51 of 500
  training loss:		1.886424E-05
  validation loss:		1.037003E-04

Epoch 52 of 500
  training loss:		9.866858E-06
  validation loss:		9.472028E-06

Epoch 53 of 500
  training loss:		1.209959E-05
  validation loss:		3.856636E-06

Epoch 54 of 500
  training loss:		1.390103E-05
  validation loss:		3.720572E-05

Epoch 55 of 500
  training loss:		1.833102E-05
  validation loss:		5.957669E-06

Epoch 56 of 500
  training loss:		1.018742E-05
  validation loss:		1.042915E-06

Epoch 57 of 500
  training loss:		1.047552E-05
  validation loss:		9.960497E-07

Epoch 58 of 500
  training loss:		1.072281E-05
  validation loss:		2.285655E-06

Epoch 59 of 500
  training loss:		1.362808E-05
  validation loss:		1.251093E-06

Epoch 60 of 500
  training loss:		9.851911E-06
  validation loss:		1.446218E-05

Early stopping, val-loss increased over the last 15 epochs from 0.00162802111874 to 0.0022598203832
Training RMSE: 1.06887954108e-09
Validation RMSE: 1.09427028629e-09
