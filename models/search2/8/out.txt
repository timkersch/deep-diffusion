Epoch 1 of 500
  training loss:		4.728332E-02
  validation loss:		4.275655E-03

Epoch 2 of 500
  training loss:		2.990197E-03
  validation loss:		2.094742E-03

Epoch 3 of 500
  training loss:		1.747189E-03
  validation loss:		1.464568E-03

Epoch 4 of 500
  training loss:		1.340040E-03
  validation loss:		1.212953E-03

Epoch 5 of 500
  training loss:		1.190599E-03
  validation loss:		1.125811E-03

Epoch 6 of 500
  training loss:		1.113285E-03
  validation loss:		1.068264E-03

Epoch 7 of 500
  training loss:		1.060060E-03
  validation loss:		1.015842E-03

Epoch 8 of 500
  training loss:		1.024537E-03
  validation loss:		9.854326E-04

Epoch 9 of 500
  training loss:		9.906058E-04
  validation loss:		9.755887E-04

Epoch 10 of 500
  training loss:		9.591448E-04
  validation loss:		9.312505E-04

Epoch 11 of 500
  training loss:		9.372903E-04
  validation loss:		9.183943E-04

Epoch 12 of 500
  training loss:		9.084958E-04
  validation loss:		8.815917E-04

Epoch 13 of 500
  training loss:		8.917313E-04
  validation loss:		8.605790E-04

Epoch 14 of 500
  training loss:		8.705987E-04
  validation loss:		8.466515E-04

Epoch 15 of 500
  training loss:		8.518913E-04
  validation loss:		8.268115E-04

Epoch 16 of 500
  training loss:		8.020383E-04
  validation loss:		7.324478E-04

Epoch 17 of 500
  training loss:		6.333497E-04
  validation loss:		4.992720E-04

Epoch 18 of 500
  training loss:		3.900570E-04
  validation loss:		2.994981E-04

Epoch 19 of 500
  training loss:		2.351386E-04
  validation loss:		1.881858E-04

Epoch 20 of 500
  training loss:		1.603202E-04
  validation loss:		1.366171E-04

Epoch 21 of 500
  training loss:		1.206562E-04
  validation loss:		1.060558E-04

Epoch 22 of 500
  training loss:		9.601787E-05
  validation loss:		8.942162E-05

Epoch 23 of 500
  training loss:		7.950858E-05
  validation loss:		7.688556E-05

Epoch 24 of 500
  training loss:		6.531452E-05
  validation loss:		6.712359E-05

Epoch 25 of 500
  training loss:		5.693287E-05
  validation loss:		6.310389E-05

Epoch 26 of 500
  training loss:		4.907037E-05
  validation loss:		4.529467E-05

Epoch 27 of 500
  training loss:		4.305374E-05
  validation loss:		3.999891E-05

Epoch 28 of 500
  training loss:		3.916669E-05
  validation loss:		3.766284E-05

Epoch 29 of 500
  training loss:		3.539358E-05
  validation loss:		3.438165E-05

Epoch 30 of 500
  training loss:		3.290620E-05
  validation loss:		3.166721E-05

Epoch 31 of 500
  training loss:		2.986942E-05
  validation loss:		3.120079E-05

Epoch 32 of 500
  training loss:		2.696532E-05
  validation loss:		2.573972E-05

Epoch 33 of 500
  training loss:		2.457099E-05
  validation loss:		2.339073E-05

Epoch 34 of 500
  training loss:		2.413107E-05
  validation loss:		2.266995E-05

Epoch 35 of 500
  training loss:		2.153494E-05
  validation loss:		2.037645E-05

Epoch 36 of 500
  training loss:		2.046328E-05
  validation loss:		1.883118E-05

Epoch 37 of 500
  training loss:		1.903224E-05
  validation loss:		1.992362E-05

Epoch 38 of 500
  training loss:		1.795212E-05
  validation loss:		1.860618E-05

Epoch 39 of 500
  training loss:		1.693794E-05
  validation loss:		1.629964E-05

Epoch 40 of 500
  training loss:		1.606339E-05
  validation loss:		1.470576E-05

Epoch 41 of 500
  training loss:		1.499142E-05
  validation loss:		1.396164E-05

Epoch 42 of 500
  training loss:		1.402031E-05
  validation loss:		1.326618E-05

Epoch 43 of 500
  training loss:		1.349719E-05
  validation loss:		2.261174E-05

Epoch 44 of 500
  training loss:		1.274699E-05
  validation loss:		1.169619E-05

Epoch 45 of 500
  training loss:		1.214580E-05
  validation loss:		1.267137E-05

Epoch 46 of 500
  training loss:		1.173034E-05
  validation loss:		1.070725E-05

Epoch 47 of 500
  training loss:		1.107952E-05
  validation loss:		1.052121E-05

Epoch 48 of 500
  training loss:		1.087486E-05
  validation loss:		9.826498E-06

Epoch 49 of 500
  training loss:		9.771793E-06
  validation loss:		9.322259E-06

Epoch 50 of 500
  training loss:		9.662926E-06
  validation loss:		8.978365E-06

Epoch 51 of 500
  training loss:		9.305538E-06
  validation loss:		8.398081E-06

Epoch 52 of 500
  training loss:		8.832779E-06
  validation loss:		8.374025E-06

Epoch 53 of 500
  training loss:		8.830788E-06
  validation loss:		8.261467E-06

Epoch 54 of 500
  training loss:		8.178381E-06
  validation loss:		7.780007E-06

Epoch 55 of 500
  training loss:		7.827282E-06
  validation loss:		6.967187E-06

Epoch 56 of 500
  training loss:		7.669719E-06
  validation loss:		7.533838E-06

Epoch 57 of 500
  training loss:		7.267923E-06
  validation loss:		6.561848E-06

Epoch 58 of 500
  training loss:		6.906249E-06
  validation loss:		7.106592E-06

Epoch 59 of 500
  training loss:		6.490587E-06
  validation loss:		5.902857E-06

Epoch 60 of 500
  training loss:		6.379953E-06
  validation loss:		7.459284E-06

Epoch 61 of 500
  training loss:		6.014490E-06
  validation loss:		5.471351E-06

Epoch 62 of 500
  training loss:		5.939054E-06
  validation loss:		5.350240E-06

Epoch 63 of 500
  training loss:		5.762459E-06
  validation loss:		5.508110E-06

Epoch 64 of 500
  training loss:		5.714251E-06
  validation loss:		7.091268E-06

Epoch 65 of 500
  training loss:		5.137998E-06
  validation loss:		4.734168E-06

Epoch 66 of 500
  training loss:		5.102535E-06
  validation loss:		8.829491E-06

Epoch 67 of 500
  training loss:		4.843538E-06
  validation loss:		4.165359E-06

Epoch 68 of 500
  training loss:		4.663678E-06
  validation loss:		4.045906E-06

Epoch 69 of 500
  training loss:		4.490255E-06
  validation loss:		4.225303E-06

Epoch 70 of 500
  training loss:		4.372489E-06
  validation loss:		3.985239E-06

Epoch 71 of 500
  training loss:		4.246906E-06
  validation loss:		3.977418E-06

Epoch 72 of 500
  training loss:		3.935767E-06
  validation loss:		4.703902E-06

Epoch 73 of 500
  training loss:		4.002536E-06
  validation loss:		6.993553E-06

Epoch 74 of 500
  training loss:		4.004402E-06
  validation loss:		3.445122E-06

Epoch 75 of 500
  training loss:		3.532130E-06
  validation loss:		3.411292E-06

Epoch 76 of 500
  training loss:		3.810166E-06
  validation loss:		4.279835E-06

Epoch 77 of 500
  training loss:		3.359563E-06
  validation loss:		3.704266E-06

Epoch 78 of 500
  training loss:		3.516455E-06
  validation loss:		2.699581E-06

Epoch 79 of 500
  training loss:		3.268220E-06
  validation loss:		2.702288E-06

Epoch 80 of 500
  training loss:		3.105096E-06
  validation loss:		3.275302E-06

Epoch 81 of 500
  training loss:		3.035207E-06
  validation loss:		2.815715E-06

Epoch 82 of 500
  training loss:		3.024002E-06
  validation loss:		2.683837E-06

Epoch 83 of 500
  training loss:		2.940711E-06
  validation loss:		2.666128E-06

Epoch 84 of 500
  training loss:		2.774731E-06
  validation loss:		2.223320E-06

Epoch 85 of 500
  training loss:		2.863339E-06
  validation loss:		2.256878E-06

Epoch 86 of 500
  training loss:		2.812636E-06
  validation loss:		2.263742E-06

Epoch 87 of 500
  training loss:		2.667921E-06
  validation loss:		2.502995E-06

Epoch 88 of 500
  training loss:		2.552607E-06
  validation loss:		3.292717E-06

Epoch 89 of 500
  training loss:		2.534279E-06
  validation loss:		2.010347E-06

Epoch 90 of 500
  training loss:		2.299161E-06
  validation loss:		2.767072E-06

Epoch 91 of 500
  training loss:		2.457445E-06
  validation loss:		3.570761E-06

Epoch 92 of 500
  training loss:		2.211221E-06
  validation loss:		3.508921E-06

Epoch 93 of 500
  training loss:		2.249759E-06
  validation loss:		2.286884E-06

Epoch 94 of 500
  training loss:		2.302996E-06
  validation loss:		3.199642E-06

Epoch 95 of 500
  training loss:		2.129859E-06
  validation loss:		1.823875E-06

Epoch 96 of 500
  training loss:		2.064722E-06
  validation loss:		3.113285E-06

Epoch 97 of 500
  training loss:		2.100351E-06
  validation loss:		1.508027E-06

Epoch 98 of 500
  training loss:		2.064347E-06
  validation loss:		2.450287E-06

Epoch 99 of 500
  training loss:		2.061740E-06
  validation loss:		1.446474E-06

Epoch 100 of 500
  training loss:		1.846501E-06
  validation loss:		1.789714E-06

Epoch 101 of 500
  training loss:		1.845217E-06
  validation loss:		1.790186E-06

Epoch 102 of 500
  training loss:		1.996222E-06
  validation loss:		1.434722E-06

Epoch 103 of 500
  training loss:		1.875303E-06
  validation loss:		2.815987E-06

Epoch 104 of 500
  training loss:		1.888995E-06
  validation loss:		1.279474E-06

Epoch 105 of 500
  training loss:		1.708683E-06
  validation loss:		1.782431E-06

Epoch 106 of 500
  training loss:		1.680305E-06
  validation loss:		1.249401E-06

Epoch 107 of 500
  training loss:		1.698063E-06
  validation loss:		1.708617E-06

Epoch 108 of 500
  training loss:		1.823260E-06
  validation loss:		1.225165E-06

Epoch 109 of 500
  training loss:		1.622723E-06
  validation loss:		1.177583E-06

Epoch 110 of 500
  training loss:		1.557462E-06
  validation loss:		1.162231E-06

Epoch 111 of 500
  training loss:		1.549071E-06
  validation loss:		1.114781E-06

Epoch 112 of 500
  training loss:		1.559623E-06
  validation loss:		1.194908E-06

Epoch 113 of 500
  training loss:		1.580343E-06
  validation loss:		1.985893E-06

Epoch 114 of 500
  training loss:		1.508096E-06
  validation loss:		1.221720E-06

Epoch 115 of 500
  training loss:		1.481146E-06
  validation loss:		1.103271E-06

Epoch 116 of 500
  training loss:		1.467291E-06
  validation loss:		9.488538E-07

Epoch 117 of 500
  training loss:		1.352387E-06
  validation loss:		1.622999E-06

Epoch 118 of 500
  training loss:		1.417706E-06
  validation loss:		9.115386E-07

Epoch 119 of 500
  training loss:		1.422243E-06
  validation loss:		9.033977E-07

Epoch 120 of 500
  training loss:		1.375526E-06
  validation loss:		9.272005E-07

Epoch 121 of 500
  training loss:		1.321333E-06
  validation loss:		2.692023E-06

Epoch 122 of 500
  training loss:		1.324418E-06
  validation loss:		1.348601E-06

Epoch 123 of 500
  training loss:		1.369019E-06
  validation loss:		1.847315E-06

Epoch 124 of 500
  training loss:		1.364520E-06
  validation loss:		1.047049E-06

Epoch 125 of 500
  training loss:		1.280053E-06
  validation loss:		9.753786E-07

Epoch 126 of 500
  training loss:		1.189218E-06
  validation loss:		9.076772E-07

Epoch 127 of 500
  training loss:		1.215487E-06
  validation loss:		8.432063E-07

Epoch 128 of 500
  training loss:		1.259377E-06
  validation loss:		1.093754E-06

Epoch 129 of 500
  training loss:		1.196404E-06
  validation loss:		1.932876E-06

Epoch 130 of 500
  training loss:		1.157028E-06
  validation loss:		1.188628E-06

Early stopping, val-loss increased over the last 10 epochs from 0.000210048315024 to 0.000244226559207
Training RMSE: 1.36538429972e-09
Validation RMSE: 1.36225470932e-09
