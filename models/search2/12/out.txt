Training network with 45280 training samples and 8490 validation samples
Epoch 1 of 100
  training loss:		1.070494E-02
  validation loss:		1.069253E-05
Epoch took 10.716s

Epoch 2 of 100
  training loss:		3.360212E-06
  validation loss:		8.708476E-07
Epoch took 8.972s

Epoch 3 of 100
  training loss:		1.592593E-07
  validation loss:		1.317722E-08
Epoch took 10.099s

Epoch 4 of 100
  training loss:		3.857790E-08
  validation loss:		3.315002E-09
Epoch took 11.023s

Epoch 5 of 100
  training loss:		2.503945E-08
  validation loss:		6.414307E-08
Epoch took 10.778s

Epoch 6 of 100
  training loss:		1.492034E-05
  validation loss:		7.371392E-05
Epoch took 10.449s

Epoch 7 of 100
  training loss:		1.853685E-04
  validation loss:		2.330444E-07
Epoch took 10.148s

Epoch 8 of 100
  training loss:		8.942645E-05
  validation loss:		2.487722E-04
Epoch took 11.243s

Epoch 9 of 100
  training loss:		1.886625E-04
  validation loss:		5.733269E-06
Epoch took 10.423s

Epoch 10 of 100
  training loss:		1.727135E-04
  validation loss:		1.686448E-05
Epoch took 11.053s

Epoch 11 of 100
  training loss:		4.785477E-04
  validation loss:		2.003180E-07
Epoch took 10.413s

Epoch 12 of 100
  training loss:		2.284175E-07
  validation loss:		6.169291E-08
Epoch took 9.755s

Epoch 13 of 100
  training loss:		1.367334E-04
  validation loss:		1.157696E-05
Epoch took 10.103s

Epoch 14 of 100
  training loss:		5.924523E-06
  validation loss:		3.276822E-06
Epoch took 9.801s

Epoch 15 of 100
  training loss:		1.571246E-04
  validation loss:		1.709942E-05
Epoch took 11.519s

Epoch 16 of 100
  training loss:		1.816284E-05
  validation loss:		1.541888E-06
Epoch took 8.759s

Epoch 17 of 100
  training loss:		3.535466E-04
  validation loss:		3.076637E-07
Epoch took 10.919s

Epoch 18 of 100
  training loss:		6.264436E-08
  validation loss:		2.881830E-07
Epoch took 10.447s

Epoch 19 of 100
  training loss:		2.892692E-05
  validation loss:		6.177194E-06
Epoch took 11.197s

Epoch 20 of 100
  training loss:		1.138322E-04
  validation loss:		1.548271E-05
Epoch took 9.625s

Epoch 21 of 100
  training loss:		1.055275E-05
  validation loss:		1.465302E-05
Epoch took 9.831s

Epoch 22 of 100
  training loss:		1.487464E-05
  validation loss:		2.420881E-05
Epoch took 9.802s

Epoch 23 of 100
  training loss:		1.653208E-04
  validation loss:		1.142365E-07
Epoch took 10.356s

Epoch 24 of 100
  training loss:		3.752441E-07
  validation loss:		1.302406E-06
Epoch took 10.437s

Epoch 25 of 100
  training loss:		7.190560E-05
  validation loss:		1.220425E-05
Epoch took 10.197s

Epoch 26 of 100
  training loss:		8.225057E-06
  validation loss:		1.690536E-05
Epoch took 10.727s

Epoch 27 of 100
  training loss:		8.045104E-05
  validation loss:		7.837257E-05
Epoch took 10.918s

Epoch 28 of 100
  training loss:		1.495405E-05
  validation loss:		7.811592E-07
Epoch took 9.857s

Epoch 29 of 100
  training loss:		5.807831E-05
  validation loss:		2.964241E-05
Epoch took 11.423s

Epoch 30 of 100
  training loss:		1.974785E-05
  validation loss:		1.193006E-07
Epoch took 9.731s

Early stopping, val-loss increased over the last 10 epochs from 5.60128504772e-06 to 1.78303522247e-05
Training RMSE: 0.00544686350163
Validation RMSE: 0.00545282141081
