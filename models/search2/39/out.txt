Epoch 1 of 500
  training loss:		8.833696E-02
  validation loss:		7.028710E-02

Epoch 2 of 500
  training loss:		3.998627E-02
  validation loss:		1.443083E-02

Epoch 3 of 500
  training loss:		7.562098E-03
  validation loss:		4.746538E-03

Epoch 4 of 500
  training loss:		4.507472E-03
  validation loss:		4.186839E-03

Epoch 5 of 500
  training loss:		4.066223E-03
  validation loss:		3.708490E-03

Epoch 6 of 500
  training loss:		3.604714E-03
  validation loss:		3.297286E-03

Epoch 7 of 500
  training loss:		3.208372E-03
  validation loss:		2.957956E-03

Epoch 8 of 500
  training loss:		2.895578E-03
  validation loss:		2.678568E-03

Epoch 9 of 500
  training loss:		2.640067E-03
  validation loss:		2.478552E-03

Epoch 10 of 500
  training loss:		2.451888E-03
  validation loss:		2.321102E-03

Epoch 11 of 500
  training loss:		2.319887E-03
  validation loss:		2.206976E-03

Epoch 12 of 500
  training loss:		2.190980E-03
  validation loss:		2.106598E-03

Epoch 13 of 500
  training loss:		1.996527E-03
  validation loss:		1.822316E-03

Epoch 14 of 500
  training loss:		1.619942E-03
  validation loss:		1.327477E-03

Epoch 15 of 500
  training loss:		1.115262E-03
  validation loss:		9.279423E-04

Epoch 16 of 500
  training loss:		7.866114E-04
  validation loss:		6.750931E-04

Epoch 17 of 500
  training loss:		5.886290E-04
  validation loss:		5.203905E-04

Epoch 18 of 500
  training loss:		4.785699E-04
  validation loss:		4.375080E-04

Epoch 19 of 500
  training loss:		4.060485E-04
  validation loss:		3.816037E-04

Epoch 20 of 500
  training loss:		3.593737E-04
  validation loss:		3.468832E-04

Epoch 21 of 500
  training loss:		3.170096E-04
  validation loss:		2.978916E-04

Epoch 22 of 500
  training loss:		2.852933E-04
  validation loss:		2.732809E-04

Epoch 23 of 500
  training loss:		2.583919E-04
  validation loss:		2.445818E-04

Epoch 24 of 500
  training loss:		2.351332E-04
  validation loss:		2.239700E-04

Epoch 25 of 500
  training loss:		2.184559E-04
  validation loss:		2.018543E-04

Epoch 26 of 500
  training loss:		1.987488E-04
  validation loss:		1.864546E-04

Epoch 27 of 500
  training loss:		1.852964E-04
  validation loss:		1.865986E-04

Epoch 28 of 500
  training loss:		1.693789E-04
  validation loss:		1.651637E-04

Epoch 29 of 500
  training loss:		1.594990E-04
  validation loss:		1.594959E-04

Epoch 30 of 500
  training loss:		1.511049E-04
  validation loss:		1.438315E-04

Epoch 31 of 500
  training loss:		1.433606E-04
  validation loss:		1.339367E-04

Epoch 32 of 500
  training loss:		1.297993E-04
  validation loss:		1.337853E-04

Epoch 33 of 500
  training loss:		1.250888E-04
  validation loss:		1.210847E-04

Epoch 34 of 500
  training loss:		1.199149E-04
  validation loss:		1.148175E-04

Epoch 35 of 500
  training loss:		1.146875E-04
  validation loss:		1.183468E-04

Epoch 36 of 500
  training loss:		1.087666E-04
  validation loss:		1.017002E-04

Epoch 37 of 500
  training loss:		1.061294E-04
  validation loss:		9.729348E-05

Epoch 38 of 500
  training loss:		9.786471E-05
  validation loss:		9.746767E-05

Epoch 39 of 500
  training loss:		9.519684E-05
  validation loss:		9.330793E-05

Epoch 40 of 500
  training loss:		9.078992E-05
  validation loss:		8.615089E-05

Epoch 41 of 500
  training loss:		8.796773E-05
  validation loss:		1.067006E-04

Epoch 42 of 500
  training loss:		8.330993E-05
  validation loss:		7.987592E-05

Epoch 43 of 500
  training loss:		8.083816E-05
  validation loss:		8.099080E-05

Epoch 44 of 500
  training loss:		7.692775E-05
  validation loss:		9.261909E-05

Epoch 45 of 500
  training loss:		7.522331E-05
  validation loss:		7.077906E-05

Epoch 46 of 500
  training loss:		7.597281E-05
  validation loss:		6.978142E-05

Epoch 47 of 500
  training loss:		7.188443E-05
  validation loss:		6.775434E-05

Epoch 48 of 500
  training loss:		6.581371E-05
  validation loss:		6.472023E-05

Epoch 49 of 500
  training loss:		6.531965E-05
  validation loss:		6.257616E-05

Epoch 50 of 500
  training loss:		6.228021E-05
  validation loss:		6.006316E-05

Epoch 51 of 500
  training loss:		6.181499E-05
  validation loss:		6.141901E-05

Epoch 52 of 500
  training loss:		6.136207E-05
  validation loss:		5.827213E-05

Epoch 53 of 500
  training loss:		5.588317E-05
  validation loss:		6.199420E-05

Epoch 54 of 500
  training loss:		5.347583E-05
  validation loss:		5.676243E-05

Epoch 55 of 500
  training loss:		5.197233E-05
  validation loss:		4.891884E-05

Epoch 56 of 500
  training loss:		5.003000E-05
  validation loss:		6.595507E-05

Epoch 57 of 500
  training loss:		5.066560E-05
  validation loss:		4.534780E-05

Epoch 58 of 500
  training loss:		4.982189E-05
  validation loss:		4.464228E-05

Epoch 59 of 500
  training loss:		4.690528E-05
  validation loss:		4.874321E-05

Epoch 60 of 500
  training loss:		4.431822E-05
  validation loss:		4.102379E-05

Epoch 61 of 500
  training loss:		4.432857E-05
  validation loss:		5.695210E-05

Epoch 62 of 500
  training loss:		4.157367E-05
  validation loss:		4.144346E-05

Epoch 63 of 500
  training loss:		3.872505E-05
  validation loss:		3.638856E-05

Epoch 64 of 500
  training loss:		3.825733E-05
  validation loss:		3.526586E-05

Epoch 65 of 500
  training loss:		3.594452E-05
  validation loss:		3.398802E-05

Epoch 66 of 500
  training loss:		3.440366E-05
  validation loss:		3.324238E-05

Epoch 67 of 500
  training loss:		3.521425E-05
  validation loss:		3.149757E-05

Epoch 68 of 500
  training loss:		3.336444E-05
  validation loss:		3.051448E-05

Epoch 69 of 500
  training loss:		3.199773E-05
  validation loss:		3.122024E-05

Epoch 70 of 500
  training loss:		3.227602E-05
  validation loss:		3.154633E-05

Epoch 71 of 500
  training loss:		3.103177E-05
  validation loss:		2.781356E-05

Epoch 72 of 500
  training loss:		2.867392E-05
  validation loss:		2.843937E-05

Epoch 73 of 500
  training loss:		2.986909E-05
  validation loss:		2.617320E-05

Epoch 74 of 500
  training loss:		2.659936E-05
  validation loss:		3.188983E-05

Epoch 75 of 500
  training loss:		2.658884E-05
  validation loss:		2.517803E-05

Epoch 76 of 500
  training loss:		2.549292E-05
  validation loss:		2.308430E-05

Epoch 77 of 500
  training loss:		2.363168E-05
  validation loss:		2.290166E-05

Epoch 78 of 500
  training loss:		2.457971E-05
  validation loss:		2.323633E-05

Epoch 79 of 500
  training loss:		2.454845E-05
  validation loss:		2.499144E-05

Epoch 80 of 500
  training loss:		2.326394E-05
  validation loss:		2.553543E-05

Epoch 81 of 500
  training loss:		2.229771E-05
  validation loss:		2.017486E-05

Epoch 82 of 500
  training loss:		2.392249E-05
  validation loss:		2.523232E-05

Epoch 83 of 500
  training loss:		2.214168E-05
  validation loss:		2.018689E-05

Epoch 84 of 500
  training loss:		2.022319E-05
  validation loss:		2.149074E-05

Epoch 85 of 500
  training loss:		2.042611E-05
  validation loss:		1.843134E-05

Epoch 86 of 500
  training loss:		1.920533E-05
  validation loss:		2.712984E-05

Epoch 87 of 500
  training loss:		2.006980E-05
  validation loss:		1.848541E-05

Epoch 88 of 500
  training loss:		2.016611E-05
  validation loss:		1.679749E-05

Epoch 89 of 500
  training loss:		1.792697E-05
  validation loss:		1.745677E-05

Epoch 90 of 500
  training loss:		1.758828E-05
  validation loss:		1.552633E-05

Epoch 91 of 500
  training loss:		1.766198E-05
  validation loss:		3.028538E-05

Epoch 92 of 500
  training loss:		1.750873E-05
  validation loss:		4.813397E-05

Epoch 93 of 500
  training loss:		1.663493E-05
  validation loss:		2.627351E-05

Epoch 94 of 500
  training loss:		1.541919E-05
  validation loss:		3.043380E-05

Epoch 95 of 500
  training loss:		1.945649E-05
  validation loss:		1.918223E-05

Epoch 96 of 500
  training loss:		1.619235E-05
  validation loss:		1.346097E-05

Epoch 97 of 500
  training loss:		1.656571E-05
  validation loss:		1.588322E-05

Epoch 98 of 500
  training loss:		1.529957E-05
  validation loss:		1.943337E-05

Epoch 99 of 500
  training loss:		1.470005E-05
  validation loss:		1.280633E-05

Epoch 100 of 500
  training loss:		1.383042E-05
  validation loss:		1.683024E-05

Epoch 101 of 500
  training loss:		1.508071E-05
  validation loss:		1.554633E-05

Epoch 102 of 500
  training loss:		1.598974E-05
  validation loss:		1.240350E-05

Epoch 103 of 500
  training loss:		1.327348E-05
  validation loss:		1.556676E-05

Epoch 104 of 500
  training loss:		1.439668E-05
  validation loss:		2.607751E-05

Epoch 105 of 500
  training loss:		1.260164E-05
  validation loss:		2.033953E-05

Early stopping, val-loss increased over the last 15 epochs from 0.000940606040235 to 0.000946459511894
Training RMSE: 4.95179845386e-09
Validation RMSE: 5.00264398984e-09
