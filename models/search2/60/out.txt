Epoch 1 of 500
  training loss:		7.897938E-02
  validation loss:		1.357005E-02

Epoch 2 of 500
  training loss:		6.014585E-03
  validation loss:		4.197101E-03

Epoch 3 of 500
  training loss:		3.830481E-03
  validation loss:		3.305340E-03

Epoch 4 of 500
  training loss:		2.986790E-03
  validation loss:		2.676626E-03

Epoch 5 of 500
  training loss:		2.495358E-03
  validation loss:		2.365001E-03

Epoch 6 of 500
  training loss:		1.913532E-03
  validation loss:		1.318997E-03

Epoch 7 of 500
  training loss:		9.279030E-04
  validation loss:		6.320021E-04

Epoch 8 of 500
  training loss:		5.162562E-04
  validation loss:		4.240586E-04

Epoch 9 of 500
  training loss:		3.554744E-04
  validation loss:		3.011644E-04

Epoch 10 of 500
  training loss:		2.670014E-04
  validation loss:		2.317275E-04

Epoch 11 of 500
  training loss:		2.156019E-04
  validation loss:		1.942449E-04

Epoch 12 of 500
  training loss:		1.794592E-04
  validation loss:		1.640995E-04

Epoch 13 of 500
  training loss:		1.545851E-04
  validation loss:		2.012226E-04

Epoch 14 of 500
  training loss:		1.427678E-04
  validation loss:		1.309597E-04

Epoch 15 of 500
  training loss:		1.282149E-04
  validation loss:		1.216501E-04

Epoch 16 of 500
  training loss:		1.192183E-04
  validation loss:		1.118446E-04

Epoch 17 of 500
  training loss:		1.290811E-04
  validation loss:		1.061764E-04

Epoch 18 of 500
  training loss:		1.081673E-04
  validation loss:		9.673843E-05

Epoch 19 of 500
  training loss:		9.577918E-05
  validation loss:		8.516755E-05

Epoch 20 of 500
  training loss:		9.174221E-05
  validation loss:		8.041935E-05

Epoch 21 of 500
  training loss:		9.045753E-05
  validation loss:		7.851727E-05

Epoch 22 of 500
  training loss:		8.860117E-05
  validation loss:		7.040999E-05

Epoch 23 of 500
  training loss:		8.695661E-05
  validation loss:		1.251627E-04

Epoch 24 of 500
  training loss:		1.056547E-04
  validation loss:		1.032909E-04

Epoch 25 of 500
  training loss:		9.429614E-05
  validation loss:		6.173023E-05

Epoch 26 of 500
  training loss:		1.051020E-04
  validation loss:		7.416810E-05

Epoch 27 of 500
  training loss:		7.083459E-05
  validation loss:		5.217252E-05

Epoch 28 of 500
  training loss:		9.104259E-05
  validation loss:		5.666901E-05

Epoch 29 of 500
  training loss:		8.668144E-05
  validation loss:		5.035444E-05

Epoch 30 of 500
  training loss:		7.069483E-05
  validation loss:		1.738124E-04

Epoch 31 of 500
  training loss:		9.564965E-05
  validation loss:		1.168218E-04

Epoch 32 of 500
  training loss:		8.243656E-05
  validation loss:		4.381836E-05

Epoch 33 of 500
  training loss:		7.501456E-05
  validation loss:		3.698644E-05

Epoch 34 of 500
  training loss:		7.577411E-05
  validation loss:		3.560172E-05

Epoch 35 of 500
  training loss:		7.553833E-05
  validation loss:		1.370777E-04

Epoch 36 of 500
  training loss:		7.440567E-05
  validation loss:		5.975653E-05

Epoch 37 of 500
  training loss:		7.031234E-05
  validation loss:		4.708752E-05

Epoch 38 of 500
  training loss:		6.545075E-05
  validation loss:		6.097682E-05

Epoch 39 of 500
  training loss:		6.392463E-05
  validation loss:		1.601271E-04

Epoch 40 of 500
  training loss:		9.107244E-05
  validation loss:		4.746920E-05

Epoch 41 of 500
  training loss:		6.445212E-05
  validation loss:		9.797149E-05

Epoch 42 of 500
  training loss:		7.725795E-05
  validation loss:		4.451822E-05

Epoch 43 of 500
  training loss:		5.821180E-05
  validation loss:		2.710855E-05

Epoch 44 of 500
  training loss:		7.090045E-05
  validation loss:		2.376399E-05

Epoch 45 of 500
  training loss:		4.906473E-05
  validation loss:		2.234365E-05

Epoch 46 of 500
  training loss:		8.022118E-05
  validation loss:		1.146968E-04

Epoch 47 of 500
  training loss:		5.467656E-05
  validation loss:		2.701793E-05

Epoch 48 of 500
  training loss:		5.759110E-05
  validation loss:		2.161400E-05

Epoch 49 of 500
  training loss:		4.963604E-05
  validation loss:		1.986009E-05

Epoch 50 of 500
  training loss:		8.037776E-05
  validation loss:		7.751392E-05

Epoch 51 of 500
  training loss:		5.095151E-05
  validation loss:		2.059636E-05

Epoch 52 of 500
  training loss:		5.035608E-05
  validation loss:		5.007630E-05

Epoch 53 of 500
  training loss:		5.277087E-05
  validation loss:		2.365004E-05

Epoch 54 of 500
  training loss:		7.708330E-05
  validation loss:		1.668560E-05

Epoch 55 of 500
  training loss:		5.596600E-05
  validation loss:		1.010538E-04

Epoch 56 of 500
  training loss:		3.811807E-05
  validation loss:		5.103700E-05

Epoch 57 of 500
  training loss:		5.785749E-05
  validation loss:		4.747575E-05

Epoch 58 of 500
  training loss:		5.508028E-05
  validation loss:		1.777266E-05

Epoch 59 of 500
  training loss:		5.578308E-05
  validation loss:		5.232346E-05

Epoch 60 of 500
  training loss:		5.486835E-05
  validation loss:		1.758637E-05

Epoch 61 of 500
  training loss:		3.190423E-05
  validation loss:		6.114063E-05

Epoch 62 of 500
  training loss:		7.781109E-05
  validation loss:		2.690308E-04

Epoch 63 of 500
  training loss:		4.672909E-05
  validation loss:		6.280522E-05

Epoch 64 of 500
  training loss:		4.980340E-05
  validation loss:		1.641721E-05

Epoch 65 of 500
  training loss:		4.900275E-05
  validation loss:		3.794759E-05

Epoch 66 of 500
  training loss:		6.159431E-05
  validation loss:		7.817340E-05

Epoch 67 of 500
  training loss:		4.056527E-05
  validation loss:		3.098526E-05

Epoch 68 of 500
  training loss:		4.857016E-05
  validation loss:		2.064455E-05

Epoch 69 of 500
  training loss:		5.015346E-05
  validation loss:		1.377905E-05

Epoch 70 of 500
  training loss:		6.171056E-05
  validation loss:		1.112728E-05

Epoch 71 of 500
  training loss:		4.302984E-05
  validation loss:		3.733110E-05

Epoch 72 of 500
  training loss:		5.037661E-05
  validation loss:		2.043876E-05

Epoch 73 of 500
  training loss:		4.840323E-05
  validation loss:		2.268593E-05

Epoch 74 of 500
  training loss:		4.804066E-05
  validation loss:		9.316731E-05

Epoch 75 of 500
  training loss:		5.206852E-05
  validation loss:		2.554546E-05

Epoch 76 of 500
  training loss:		4.819125E-05
  validation loss:		4.487390E-05

Epoch 77 of 500
  training loss:		4.291962E-05
  validation loss:		1.493916E-04

Epoch 78 of 500
  training loss:		3.852858E-05
  validation loss:		1.455832E-04

Epoch 79 of 500
  training loss:		5.049242E-05
  validation loss:		9.645465E-06

Epoch 80 of 500
  training loss:		5.780433E-05
  validation loss:		1.388569E-04

Early stopping, val-loss increased over the last 20 epochs from 0.00192426525956 to 0.00283705537606
Training RMSE: 2.98382497162e-09
Validation RMSE: 3.0512257522e-09
