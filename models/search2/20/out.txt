Epoch 1 of 500
  training loss:		1.014849E-01
  validation loss:		1.689391E-02

Epoch 2 of 500
  training loss:		6.700024E-03
  validation loss:		3.858837E-03

Epoch 3 of 500
  training loss:		3.605804E-03
  validation loss:		3.196109E-03

Epoch 4 of 500
  training loss:		3.035891E-03
  validation loss:		2.772887E-03

Epoch 5 of 500
  training loss:		2.644414E-03
  validation loss:		2.474422E-03

Epoch 6 of 500
  training loss:		2.387010E-03
  validation loss:		2.235443E-03

Epoch 7 of 500
  training loss:		2.197355E-03
  validation loss:		2.071071E-03

Epoch 8 of 500
  training loss:		1.945904E-03
  validation loss:		1.647634E-03

Epoch 9 of 500
  training loss:		1.222255E-03
  validation loss:		8.618335E-04

Epoch 10 of 500
  training loss:		6.872012E-04
  validation loss:		5.630815E-04

Epoch 11 of 500
  training loss:		4.528153E-04
  validation loss:		3.904041E-04

Epoch 12 of 500
  training loss:		3.395903E-04
  validation loss:		2.847819E-04

Epoch 13 of 500
  training loss:		2.625464E-04
  validation loss:		2.343082E-04

Epoch 14 of 500
  training loss:		2.147957E-04
  validation loss:		1.964527E-04

Epoch 15 of 500
  training loss:		1.808892E-04
  validation loss:		1.796055E-04

Epoch 16 of 500
  training loss:		1.590803E-04
  validation loss:		1.571665E-04

Epoch 17 of 500
  training loss:		1.447523E-04
  validation loss:		1.294036E-04

Epoch 18 of 500
  training loss:		1.309526E-04
  validation loss:		1.528193E-04

Epoch 19 of 500
  training loss:		1.210065E-04
  validation loss:		1.534934E-04

Epoch 20 of 500
  training loss:		1.129229E-04
  validation loss:		1.769247E-04

Epoch 21 of 500
  training loss:		1.158785E-04
  validation loss:		1.204610E-04

Epoch 22 of 500
  training loss:		1.058800E-04
  validation loss:		1.086374E-04

Epoch 23 of 500
  training loss:		9.435006E-05
  validation loss:		8.556156E-05

Epoch 24 of 500
  training loss:		9.347863E-05
  validation loss:		8.214675E-05

Epoch 25 of 500
  training loss:		9.441244E-05
  validation loss:		1.045175E-04

Epoch 26 of 500
  training loss:		8.974790E-05
  validation loss:		1.060258E-04

Epoch 27 of 500
  training loss:		1.042470E-04
  validation loss:		7.289638E-05

Epoch 28 of 500
  training loss:		1.257315E-04
  validation loss:		1.061367E-04

Epoch 29 of 500
  training loss:		8.501453E-05
  validation loss:		6.990183E-05

Epoch 30 of 500
  training loss:		1.058429E-04
  validation loss:		9.817126E-05

Epoch 31 of 500
  training loss:		9.349139E-05
  validation loss:		7.786189E-05

Epoch 32 of 500
  training loss:		1.000972E-04
  validation loss:		5.275672E-05

Epoch 33 of 500
  training loss:		1.092140E-04
  validation loss:		7.208038E-05

Epoch 34 of 500
  training loss:		1.069596E-04
  validation loss:		1.369695E-04

Epoch 35 of 500
  training loss:		9.074257E-05
  validation loss:		4.864418E-05

Epoch 36 of 500
  training loss:		8.479256E-05
  validation loss:		1.449966E-04

Epoch 37 of 500
  training loss:		1.002337E-04
  validation loss:		6.455711E-05

Epoch 38 of 500
  training loss:		9.134593E-05
  validation loss:		4.911854E-05

Epoch 39 of 500
  training loss:		9.451897E-05
  validation loss:		5.177487E-05

Epoch 40 of 500
  training loss:		9.525204E-05
  validation loss:		8.313071E-05

Epoch 41 of 500
  training loss:		7.396299E-05
  validation loss:		4.218077E-05

Epoch 42 of 500
  training loss:		8.733443E-05
  validation loss:		5.141364E-05

Epoch 43 of 500
  training loss:		8.272413E-05
  validation loss:		5.424172E-05

Epoch 44 of 500
  training loss:		7.334493E-05
  validation loss:		1.139366E-04

Epoch 45 of 500
  training loss:		1.001868E-04
  validation loss:		5.714865E-05

Epoch 46 of 500
  training loss:		7.222510E-05
  validation loss:		2.785215E-05

Epoch 47 of 500
  training loss:		9.291555E-05
  validation loss:		4.371749E-05

Epoch 48 of 500
  training loss:		7.257852E-05
  validation loss:		7.580857E-05

Epoch 49 of 500
  training loss:		6.865835E-05
  validation loss:		3.129755E-05

Epoch 50 of 500
  training loss:		6.856898E-05
  validation loss:		3.670997E-05

Epoch 51 of 500
  training loss:		7.097232E-05
  validation loss:		7.064985E-05

Epoch 52 of 500
  training loss:		8.057118E-05
  validation loss:		2.533395E-04

Epoch 53 of 500
  training loss:		7.820332E-05
  validation loss:		3.999612E-05

Epoch 54 of 500
  training loss:		5.136447E-05
  validation loss:		4.673960E-05

Epoch 55 of 500
  training loss:		9.804112E-05
  validation loss:		6.562392E-05

Epoch 56 of 500
  training loss:		5.110044E-05
  validation loss:		1.787313E-04

Epoch 57 of 500
  training loss:		7.976976E-05
  validation loss:		3.357828E-05

Epoch 58 of 500
  training loss:		9.156717E-05
  validation loss:		7.463152E-05

Epoch 59 of 500
  training loss:		5.980871E-05
  validation loss:		3.391281E-05

Epoch 60 of 500
  training loss:		5.727218E-05
  validation loss:		2.568236E-05

Early stopping, val-loss increased over the last 10 epochs from 0.00235095122391 to 0.00362069498472
Training RMSE: 5.67034634218e-09
Validation RMSE: 5.71195891513e-09
